# -*- coding: utf-8 -*-
"""Copia de Representación de características y representación de documentos. PySpark

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12oisiL3NG2zjvanJyPTcH51K41adotTn
"""

#
#Trabajo 2	– Preparación	de	texto,	representación	de	características	y	representación	de	documentos.
# 
# Erica Yubiana Herrera Sepulveda
# Juan Camilo Gómez Betancur
# Natalia Andrea Gaviria Angulo 
# 
# Universidad EAFIT 
# 2021-2
# 
# librerías en nltk, spacy, scikit-learn o gensim y 2) en pyspark, diferentes opciones de representación de características como one-bit, TF,TF-IDF, word2vec, doc2vec.

#configuración en google colab
from google.colab import drive
drive.mount('/content/gdrive')

# verificar que tengan instalado la librería 'pyspark'
!pip install pyspark

#Instalacion de nltk y de pyspark
!pip install nltk
!pip install pyspark

from pyspark.sql import SparkSession

#forma 1 de crear la sesión y el contexto Spark:
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = spark.sparkContext

#Se realiza la importacion de las dependencias a utilizar
#El	proyecto	lo	deberá	realizar	con: Python y	(nltk,	scikit	y/o	gensim) y PySpark,	con	Spark y	las	diferentes	librerías	de	nlp
import nltk
import gensim
import pandas as pd
import numpy as np
import re
import codecs
import matplotlib.pyplot as plt

# directorios (path) de entrada y salida:
# Para saber donde estan mis datos de entrada, donde los voy a procesar y donde los voy a sacar, donde path_in es la ruta de donde se encuentran los datos de entrada
# path_out corresponde a la ruta de los datos de salida, filenametxt en este coloco el nombre del archivo el cual voy a procesar 
#Remoción	 de	 caracteres	 especiales	 y	 tokens	 que	 considere	 irrelevantes	 para	 una	consulta.
#
path_in="gdrive/MyDrive/st1800_20212/datasets/papers_sample_pdf/"

archivo = !ls "gdrive/MyDrive/st1800_20212/datasets/papers_sample_pdf" | grep ".txt"

archivo

import pandas as pd
import numpy as np

df = pd.DataFrame(list(zip(archivo,archivos)), columns = ['archivo','contenido'])
print(df)

#columns of dataframe
df

#check number of columns
len(df.columns)

#number of records in dataframe
df.columns

# forma de conjunto de datos
print((df.count(),len(df.columns)))

sparkDF=spark.createDataFrame(df)

from pyspark.ml.feature import Tokenizer
tokenization=Tokenizer(inputCol='contenido',outputCol='tokens')
tokenized_df = tokenization.transform(sparkDF)
tokenized_df.show(5)

from pyspark.ml.feature import StopWordsRemover
stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')
refined_df=stopword_removal.transform(tokenized_df)
refined_df.select(['tokens','refined_tokens']).show(5)

refined_df.columns

from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import *

len_udf = udf(lambda s: len(s), IntegerType())

refined_count_df = refined_df.withColumn("token_count", len_udf(col('refined_tokens')))

refined_count_df.orderBy(rand()).show(10)

# Count Vectorizer
from pyspark.ml.feature import CountVectorizer
count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')
cv_df=count_vec.fit(refined_df).transform(refined_df)
cv_df.select(['refined_tokens','features']).show(4,False)
bow = count_vec.fit(refined_df).vocabulary
print(bow)

# TF with HashingTF
from pyspark.ml.feature import HashingTF
# podria utilizar numFeatures como el tamaño del Bag of Words:
l = len(bow)
hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=l)
#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features',numFeatures=11)
# compare la salida e interprete con y sin numFeatures:
#hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features')

hashing_df=hashing_vec.transform(refined_df)
hashing_df.show(4)

from pyspark.ml.feature import IDF
tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')
tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)
tf_idf_df.show(4)