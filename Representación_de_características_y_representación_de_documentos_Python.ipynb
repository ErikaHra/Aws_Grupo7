{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Representación de características y representación de documentos. Python",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLv2LUpdiJw1"
      },
      "source": [
        "#\n",
        "#Trabajo 2\t– Preparación\tde\ttexto,\trepresentación\tde\tcaracterísticas\ty\trepresentación\tde\tdocumentos.\n",
        "# \n",
        "# Erica Yubiana Herrera Sepulveda\n",
        "# Juan Camilo Gómez Betancur\n",
        "# Natalia Andrea Gaviria Angulo \n",
        "# \n",
        "# Universidad EAFIT \n",
        "# 2021-2\n",
        "# \n",
        "# librerías en nltk, spacy, scikit-learn o gensim y 2) en pyspark, diferentes opciones de representación de características como one-bit, TF,TF-IDF, word2vec, doc2vec.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG-9HNZglaZx",
        "outputId": "2bded6c5-38f2-4317-a057-2535fd78b687"
      },
      "source": [
        "#configuración en google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z70kP8BXgoK"
      },
      "source": [
        "# cargar las librerias necesarias\n",
        "## 1. nltk para 'procesamiento natural del lenguaje'\n",
        "# Natural language toolkit (NLTK) es la biblioteca más popular para el procesamiento del lenguaje natural (NLP) usada para construir programas para análisis de texto\n",
        "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
        "# Pandas es un paquete de Python que proporciona estructuras de datos similares a los dataframes de R, muy utilizada sobre todo dentro del ámbito de Data Science y Machine Learning\n",
        "## 3. re - expresiones regulares\n",
        "## 4. numpy, codecs, etc - otraas\n",
        "# 5. Matplotlib es una biblioteca para la generación de gráficos a partir de datos contenidos en listas o arrays en el lenguaje de programación Python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB9g16FSXoSQ",
        "outputId": "565d6e53-03fa-4604-d3ea-b48a70ac9838"
      },
      "source": [
        "#Instalacion de nltk y de pyspark\n",
        "!pip install nltk\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 65 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 65.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=00aa5e7ae2476178a8378516690706cc60ae6eaf18b11f8d6baf0baceb91a472\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZxizpeNdgTO"
      },
      "source": [
        "#Se realiza la importacion de las dependencias a utilizar\n",
        "#El\tproyecto\tlo\tdeberá\trealizar\tcon: Python y\t(nltk,\tscikit\ty/o\tgensim) y PySpark,\tcon\tSpark y\tlas\tdiferentes\tlibrerías\tde\tnlp\n",
        "import nltk\n",
        "import gensim\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir,chdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsVo4xaljEQ4"
      },
      "source": [
        "# directorios (path) de entrada y salida:\n",
        "# Para saber donde estan mis datos de entrada, donde los voy a procesar y donde los voy a sacar, donde path_in es la ruta de donde se encuentran los datos de entrada\n",
        "# path_out corresponde a la ruta de los datos de salida, filenametxt en este coloco el nombre del archivo el cual voy a procesar \n",
        "#Remoción\t de\t caracteres\t especiales\t y\t tokens\t que\t considere\t irrelevantes\t para\t una\tconsulta.\n",
        "#\n",
        "path_in=\"gdrive/MyDrive/st1800_20212/datasets/papers_sample_pdf/\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3apkioqCj9v_"
      },
      "source": [
        "archivo = !ls \"gdrive/MyDrive/st1800_20212/datasets/papers_sample_pdf\" | grep \".txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rDSXCvpk0jY",
        "outputId": "250fb006-2ef7-4a1d-94e6-961bb3a045d6"
      },
      "source": [
        "archivo "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0704.3504.txt',\n",
              " '0706.1402.txt',\n",
              " '0710.0736.txt',\n",
              " '0803.2570.txt',\n",
              " '0808.0084.txt',\n",
              " '0811.1254.txt',\n",
              " '0811.2853.txt',\n",
              " '0812.2709.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGeX0h_SlLLv",
        "outputId": "dac564af-8c10-470f-e304-ca27515742f4"
      },
      "source": [
        "# leer un archivo de ejemplo en .txt\n",
        "# input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
        "archivos = []\n",
        "for t in archivo:\n",
        "  archivos.append(open(path_in+t,\"r\").read())\n",
        "print(archivos)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Smooth Rényi Entropy of Ergodic Quantum\\nInformation Sources\\nBerry Schoenmakers\\n\\nJilles Tjoelker\\n\\narXiv:0704.3504v1 [quant-ph] 26 Apr 2007\\n\\nDept. of Mathematics and Computer Science\\nTechnical University Eindhoven\\nThe Netherlands\\nberry@win.tue.nl\\n\\nj.tjoelker@student.tue.nl\\n\\nPim Tuyls\\n\\nEvgeny Verbitskiy\\n\\nInformation Security Systems\\nPhilips Research Eindhoven\\nThe Netherlands\\n\\nDigital Signal Processing\\nPhilips Research Eindhoven\\nThe Netherlands\\n\\npim.tuyls@philips.com\\n\\nevgeny.verbitskiy@philips.com\\n\\nAbstract— We investigate the recently introduced notion of\\nsmooth Rényi entropy for the case of ergodic information sources,\\nthereby generalizing previous work which concentrated mainly\\non i.i.d. information sources. We will actually consider ergodic\\nquantum information sources, of which ergodic classical information sources are a special case. We prove that the average\\nsmooth Rényi entropy rate will approach the entropy rate of a\\nstationary, ergodic source, which is equal to the Shannon entropy\\nrate for a classical source and the von Neumann entropy rate\\nfor a quantum source.\\n\\nI. I NTRODUCTION\\nThe elegant notion of smooth Rényi entropy was introduced\\nrecently by Renner and Wolf in [6] for classical information\\nsources, and the natural extension to quantum information\\nsources was defined by Renner and König in [5]. In these two\\npapers and further work by Renner and Wolf [7], [4], many\\nproperties of smooth Rényi entropy—and smooth min-entropy\\nand smooth max-entropy in particular—have been studied in\\ndetail.\\nA central property of smooth Rényi entropy proved in these\\nworks is that for memoryless (i.i.d.) information sources, the\\naverage smooth Rényi entropy rate will approach the entropy\\nrate of the source, which is equal to the Shannon entropy for a\\nclassical source and the von Neumann entropy for a quantum\\nsource. Whereas, in general, the average (conventional) Rényi\\nentropy rate of a memoryless source does not converge to the\\nsource’s entropy rate.\\nIn this paper we extend the study of smooth Rényi entropy\\nto the more general class of stationary, ergodic sources rather\\nthan memoryless sources. We will prove that for both the\\nclassical and the quantum case that the average smooth Rényi\\nentropy rate will approach the Shannon and the von Neumann\\nentropy rate, respectively. We will do so by first treating the\\nclassical case and then reducing the quantum case to the\\nclassical one without losing generality.\\nIn general, smooth Rényi entropy of order α > 1, and α =\\n∞ (min-entropy) in particular, is of cryptographic relevance\\n(e.g., for randomness-extraction), and smooth Rényi entropy\\nof order α < 1, and α = 0 (max-entropy) in particular, are\\nrelevant to data compression (minimum encoding length). In\\nthese contexts, the importance of smooth Rényi entropy is that\\nits rate is basically equal to the Shannon/von Neumann entropy\\n\\nrate for an i.i.d. source (and for ergodic sources as well, as we\\nshow in this paper). This is not the case for conventional Rényi\\nentropy. More generally, as shown in the papers by Renner et\\nal. mentioned above, smooth Rényi entropy behaves much as\\nShannon/von Neumann entropy does.\\nIn this paper we focus on the unconditional case, whereas\\nmuch of the abovementioned work by Renner et al. treats the\\nmore general conditional case. We leave the extension to the\\nconditional case for future work. However, we do consider\\ntwo notions of ǫ-closeness, one based on trace distance (also\\nknown as variational or statistical distance) and one based on\\nnon-normalized density matrices (or probability distributions),\\nwhere the latter is more suitable to handle the conditional\\ncase.1 Thus, we believe that our results can be extended to the\\nconditional case as well.\\nWe also note that Renner [4] presents a different kind of\\ngeneralization of i.i.d. quantum sources, namely by analyzing\\nthe smooth min-entropy of symmetric (permutation-invariant)\\nquantum states. Or, more precisely, states in a symmetric\\nsubspace of H⊗n are considered, for n ∈ N. See [4, Chapter 4]\\nfor details, which also covers the conditional case.\\nII. P RELIMINARIES\\nThroughout this paper we use P and Q to denote probability\\ndistributions with over the same finite or countably infinite\\nrange Z. Similarly, we use ρ and σ to denote density matrices\\non the same Hilbert space of a finite or countably infinite dimension. These probability distributions\\nPand density matrices\\nare not necessarily normalized (e.g.,\\nz P(z) < 1 if P is\\nnon-normalized and tr(ρ) < 1 if ρ is non-normalized).\\nFor ease of comparison we state all the preliminaries\\nexplicitly for the classical case as well as for the quantum\\ncase.\\nDefinition 1 (Classical Rényi entropy): The Rényi entropy\\nof order α ∈ [0, ∞] of probability distribution P is\\nX\\n1\\nlog\\nP(z)α ,\\nHα (P) =\\n1−α\\nz∈Z\\n\\n1 The trace distance was originally used in [6], [5]. The use of nonnormalized probability distributions was also shown in the full version of\\n[6] and used in [7]. In this paper, we extend this to the use of non-normalized\\ndensity matrices in the quantum case.\\n\\n\\x0cfor 0 < α < ∞, α 6= 1, and Hα (P) = limβ→α Hβ (P)\\notherwise.\\nHence, H0 (P) = log |{z ∈ Z : P(z) > 0}|, H1 (P) = H (P)\\n(Shannon entropy) and H∞ (P) = − log maxz∈Z P(z).\\nFor a random variable Z we use Hα (Z) as a shorthand for\\nHα (PZ ), where PZ is the probability distribution of Z.\\nSmooth Rényi entropy was introduced in [6] for the classical\\ncase. For ǫ ≥ 0, let B ǫ(P) denote either the set of probability\\ndistributions which are ǫ-close to P, B ǫ(P) = {Q : δ(P, Q) ≤\\nǫ}, or the set of non-normalized P\\nprobability distributions which\\nare ǫ-close to P, B ǫ(P) = {Q : z∈Z Q(z) ≥ 1 − ǫ, ∀z∈Z 0 ≤\\nQ(z) ≤ P(z)}. The first notionPof ǫ-closeness, based on the\\nstatistical distance δ(P, Q) = 12 z∈Z |P(z)−Q(z)|, was used\\nin [6]. The second notion was mentioned in the full version\\nof [6], and used in [7].\\nDefinition 2 (Classical smooth Rényi entropy, [6]): The ǫsmooth Rényi entropy of order α ∈ [0, 1) ∪ (1, ∞] of a\\nprobability distribution P is\\n\\x1a\\ninf Q∈Bǫ (P) Hα (Q), 0 ≤ α < 1,\\nǫ\\nHα (P) =\\nsupQ∈Bǫ (P) Hα (Q), 1 < α ≤ ∞.\\nAt the end of this paper, we point out that Hαǫ (P) will\\nactually vary, depending on which notion of ǫ-closeness is\\nα\\nlog(1 − ǫ).\\nused, leading to a maximum difference of α−1\\nFor a probability distribution P on, e.g., Z = {0, 1}N, we\\ndefine Pn as the probability distribution corresponding to the\\nrestriction of the “infinite volume” distribution P to the finite\\nvolume {0, . . . , n − 1}.\\nDefinition 3 (Entropy rate of a classical source): For\\na\\nstationary source given by its probability measure P, we\\ndefine\\n1\\nh(P) = lim H (Pn ),\\nn→∞ n\\n1\\nhǫα (P) = lim Hαǫ (Pn ).\\nn→∞ n\\nWe will actually prove that hǫα (P) = h(P) as ǫ → 0.\\nWe use the standard notion of typical sequences and typical sets, which are defined for any information source (not\\nnecessarily i.i.d.). See, for instance, [2] or [3].\\nDefinition 4 (Typical sequences, typical set): A sequence\\nz n ∈ {0, 1}n, n ∈ N, is called ǫ-typical if\\ne−n(h(P)+ǫ) ≤ P(z n ) ≤ e−n(h(P)−ǫ) .\\n(n)\\n\\nThe typical set Tǫ is the set of all ǫ-typical sequences from\\n{0, 1}n.\\nIn this paper we need the following consequence of the AEP,\\nwhere we refer to [2, Section 16.8] for the AEP for ergodic\\nsources (known as the Shannon-McMillan-Breiman theorem).\\n\\nDefinition 5 (Quantum Rényi entropy): The Rényi entropy\\nof order α ∈ [0, ∞] of a density matrix ρ is\\nSα (ρ) =\\n\\n1\\nlog tr(ρα )\\n1−α\\n\\nfor 0 < α < ∞, α 6= 1, and Sα (ρ) = limβ→α Sβ (ρ)\\notherwise.\\nHence, S0 (ρ) = log rank(ρ), S1 (ρ) = S (ρ) = − tr(ρ log ρ)\\n(von Neumann entropy) and S∞ (ρ) = − log λmax (ρ).\\nAnalogous to the classical case, smooth Rényi entropy is\\ndefined in the quantum case (see [5]). We use either the set\\nof density matrices which are ǫ-close to ρ, B ǫ (ρ) = {σ :\\nδ(ρ, σ) ≤ ǫ} or the set of non-normalized density matrices\\nwhich are ǫ-close to ρ, B ǫ (ρ) = {σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤\\nρ}. The first notion of ǫ-closeness, based on the trace distance\\nδ(ρ, σ) = 12 tr(|ρ − σ|), was used in [5]. The second notion is\\nintroduced here, and will actually be used in the next section.\\nDefinition 6 (Quantum smooth Rényi entropy, [5]): The ǫsmooth Rényi entropy of order α ∈ [0, 1)∪(1, ∞] of a density\\nmatrix ρ is\\n\\x1a\\ninf σ∈Bǫ (ρ) Sα (σ), 0 ≤ α < 1,\\nǫ\\nSα (ρ) =\\nsupσ∈Bǫ (ρ) Sα (σ), 1 < α ≤ ∞.\\nDefinition 7 (Entropy rates of a quantum source): For a\\nstationary quantum source ρ, given by its local densities\\nρ(n) = ρ0,...,n−1 , for n ∈ N, we define:\\n1\\nS (ρ(n) ),\\nn\\n1\\nsǫα (ρ) = lim Sαǫ (ρ(n) ).\\nn→∞ n\\nWe use the following notion of typical states and typical\\nsubspaces, as can be found in [1] (also see [3]).\\nDefinition 8 (Typical state, typical subspace): A pure state\\n(n)\\n(n)\\n|ei i, where ei is an eigenvector of ρ(n) is called ǫ-typical\\n(n)\\nif the corresponding eigenvalue λi satisfies\\ns(ρ)\\n\\n=\\n\\nlim\\n\\nn→∞\\n\\n(n)\\n\\ne−n(s(ρ)+ǫ) ≤ λi\\n\\n≤ e−n(s(ρ)−ǫ) .\\n\\n(n)\\n\\nThe typical subspace Tǫ is the subspace spanned by all ǫtypical states.\\nWe will need the following consequences of the quantum\\nAEP for ergodic sources, which has been studied in [1] (see\\n[3] for the quantum AEP for i.i.d. sources).\\nTheorem 2 (Quantum AEP bounds): Let ρ be a stationary,\\nergodic quantum source with local densities ρ(n) . Let ǫ > 0.\\nThen, for sufficiently large n,\\ntr(ρ(n) PT (n) ) ≥ 1 − ǫ,\\nǫ\\n\\n(n)\\n\\nTheorem 1 (Classical AEP bounds): Let P be a stationary,\\nergodic probability distribution on Z = {0, 1}N. Let ǫ > 0.\\nThen, for sufficiently large n,\\nP(Tǫ(n) ) ≥ 1 − ǫ,\\nand\\n|Tǫ(n) | ≤ en(h(P)+ǫ) .\\n\\nwhere PT (n) is the projector onto the subspace Tǫ . Furtherǫ\\nmore,\\ntr(PT (n) ) ≤ en(s(ρ)+ǫ) .\\nǫ\\nClearly, the quantum AEP for ergodic sources implies the\\nclassical AEP for ergodic sources.\\nThe following theorem by Renner and Wolf states that\\nsmooth Rényi entropy approaches Shannon entropy in the case\\nof a classical i.i.d. source.\\n\\n\\x0cTheorem 3 ([7, Lemma I.2]): Let Z n denote an n-tuple of\\ni.i.d. random variables with probability distribution PZ . Then,\\nlim lim\\n\\nǫ→0 n→∞\\n\\n1 ǫ n\\nH (Z ) = H (Z),\\nn α\\n\\n(n)\\n\\nz n ∈Tǫ\\n\\nfor any α ∈ [0, ∞].\\nThe analogous theorem by Renner and König for a quantum\\ni.i.d. source is as follows.\\nTheorem 4 ([5, Lemma 3]): Let ρ be a density matrix.\\nThen,\\n1\\nlim lim Sαǫ (ρ⊗n ) = S (ρ),\\nǫ→0 n→∞ n\\nfor any α ∈ [0, ∞].\\nIII. M AIN R ESULT\\nWe extend the results by Renner and Wolf (Theorem 3\\nabove) and by Renner and König (Theorem 4 above) to the\\ncase of ergodic sources. Throughout this section, we use\\nthe notion of ǫ-closeness based on non-normalized probability\\ndistributions and density matrices, so B ǫ (P) = {Q :\\nP\\nǫ\\nz∈Z Q(z) ≥ 1 − ǫ, ∀z∈Z 0 ≤ Q(z) ≤ P(z)} and B (ρ) =\\n{σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}, respectively. In the next\\nsection, we will argue that the results are independent on which\\nnotion of ǫ-closeness is used.\\nA. Classical Case\\nWe start with our main result for the classical case. The\\nknown result for an i.i.d. source is by Renner and Wolf,\\nTheorem 3 above. We will extend this to a stationary, ergodic\\nsource in Theorem 5 below.\\nLemma 1: Let P be a stationary, ergodic information source\\ngiven by its probability measure and let 0 < ǫ < 1/2. Then\\nwe have,\\nh(P) − ǫ ≤ hǫ∞ (P) ≤ h(P) + 2ǫ.\\nProof: Let 0 < ǫ < 1/2. To prove the lower bound, we\\nǫ\\n(Pn ) ≥ n(h(P) − ǫ).\\nshow that, for sufficiently large n, H∞\\nDefine non-normalized probability distribution Q for all z n ∈\\n{0, 1}n by\\n(\\n(n)\\nP(z n ), if z n ∈ Tǫ\\n(1)\\nQ(z n ) =\\n(n)\\n0,\\nif z n ∈\\n/ Tǫ .\\n(n)\\n\\nClearly, 0 ≤ Q(z n ) ≤ P(z n ) and, by the AEP, Q(Tǫ ) =\\n(n)\\nP(Tǫ ) ≥ 1 − ǫ for sufficiently large n. So, Q ∈\\n(n)\\nB ǫ (Pn ). Furthermore, for z n ∈ Tǫ , we have that\\n− log P(z n ) ≥ n(h(P) − ǫ), and hence that for any z n that\\n− log Q(z n ) ≥ n(h(P) − ǫ). This implies that H∞ (Q) =\\n− log maxzn Q(z n ) ≥ n(h(P) − ǫ) and the lower bound\\nfollows.\\nNext, to prove the upper bound, we show that, for sufficiently large n, one has that for all Q ∈ B ǫ (Pn ),\\nH∞ (Q) = − log max\\nQ(z n ) ≤ n(h(P) + 2ǫ).\\nn\\nz\\n\\nThis follows from maxzn ∈T (n) Q(z n ) ≥ e−n(h(P)+2ǫ) , which\\nǫ\\nP\\n(n)\\nin turn follows from zn ∈T (n) Q(z n ) ≥ |Tǫ |e−n(h(P)+2ǫ).\\nǫ\\n\\n(n)\\n\\nFrom the AEP we get |Tǫ | ≤ en(h(P)+ǫ) , hence it suffices\\nto prove that, for sufficiently large n,\\nX\\nQ(z n ) ≥ e−nǫ .\\n(2)\\nQ(z n ) ≥ 1 − ǫ, for Q ∈ B ǫ (Pn ), and also\\nn\\n≤ ǫ (because Q(z n ) ≤ P(z n ) and\\n(n) Q(z )\\nz n ∈T\\n/\\n\\nAs\\nP\\n\\nP\\n\\nzn\\nǫ\\n\\n(n)\\n\\nP (Tǫ\\nthat\\n\\n) ≥ 1 − ǫ from the AEP), we only need to observe\\n1 − 2ǫ > e−nǫ\\n\\nholds for sufficiently large n, using that ǫ < 1/2.\\nWe now state an analogous lemma for the max-entropy.\\nLemma 2: Let P be a stationary, ergodic information source\\ngiven by its probability measure and let 0 < ǫ < 1/2. Then\\nwe have,\\nh(P) − 2ǫ ≤ hǫ0 (P) ≤ h(P) + ǫ.\\nProof: Let 0 < ǫ < 1/2. To prove the upper bound, we\\nshow that, for sufficiently large n, H0ǫ (Pn ) ≤ n(h(P)+ ǫ). We\\ndo so by showing that H0 (Q) = log |{z n : Q(z n ) > 0}| ≤\\nn(h(P)+ ǫ) for the non-normalized probability distribution Q,\\ndefined by (1) in the proof of Lemma 1. As\\n|{z n : Q(z n ) > 0}| = |{z n ∈ Tǫ(n) : Q(z n ) > 0}| ≤ |Tǫ(n) |,\\nthe result follows directly from the AEP.\\nNext, to prove the lower bound, we show that, for sufficiently large n, one has that for all Q ∈ B ǫ (Pn ),\\nH0 (Q) = log |{z n : Q(z n ) > 0}| ≥ n(h(P) − 2ǫ).\\n(n)\\n\\nThis is implied by |{z n ∈ Tǫ : Q(z n ) > 0}| ≥ en(h(P)−2ǫ),\\nwhich is in turn implied by\\nX\\nQ(z n ) ≥\\nmax\\nQ(z n )en(h(P)−2ǫ) .\\n(n)\\n\\nz n ∈Tǫ\\n\\n(n)\\n\\nz n ∈Tǫ\\n\\n,Q(z n )>0\\n\\nUsing inequality (2) from the proof of Lemma 1, it suffices\\nto show that\\nQ(z n ) ≤ e−nǫ e−n(h(P)−2ǫ) = e−n(h(P)−ǫ).\\n\\nmax\\n\\n(n)\\n\\nz n ∈Tǫ\\n\\n,Q(z n )>0\\n\\nThis is a direct consequence of the definition of ǫ-typical\\nsequences, as Q(z n ) ≤ P(z n ) for Q ∈ B ǫ (Pn ), using that\\n(n)\\nQ(z n ) > 0 holds for at least one z n ∈ Tǫ\\non account of\\ninequality (2).\\nTheorem 5: For α ∈ [0, ∞], the ǫ-smooth entropy of a\\nstationary, ergodic information source P given by its probability measure on Z = {0, 1}N is close to the mean Shannon\\nentropy:\\nlim hǫα (P) = h(P).\\nǫ→0\\nProof: For α < 1, the monotonicity of smooth Rényi\\nentropy (see, e.g., [7, Lemma 1]) yields Hαǫ (Pn ) ≤ H0ǫ (Pn ),\\nand hence hǫα (P) ≤ h(P) + ǫ by Lemma 2.\\nTo get a lower bound for hǫα (P), we note that\\nHαǫ (Pn ) ≥ H02ǫ (Pn ) −\\n\\nlog(1/ǫ)\\n,\\n1−α\\n\\n\\x0cusing [7, Lemma 2]. So, hǫα (P) ≥ h2ǫ\\n0 (P) as the constant term\\non the right-hand side vanishes for n → ∞. Using Lemma 2,\\nwe thus get hǫα (P) ≥ h(P) − 4ǫ.\\nThis proves that limǫ→0 hǫα (P) = h(P). The proof for α > 1\\nis completely symmetrical, hence omitted.\\nNote that the term 2ǫ in the upper and lower bounds of\\nLemmas 1 and 2, respectively, can be improved to (1 + δ)ǫ\\nfor any constant δ > 0. Similarly, the term 4ǫ in the proof of\\nTheorem 5 for the lower bound for hǫα can be improved to\\n(1 + δ)ǫ for any constant δ > 0.\\nB. Quantum Case\\nAlthough it is possible to prove the quantum case directly,\\nalong the same lines as in the classical case, we treat the\\nquantum case indirectly, by reducing it to the classical case.\\nThis leads to a more compact proof. To this end, we will\\nfirst prove Lemma 3 below, which captures the correspondence between B ǫ (ρ(n) ) and B ǫ(λ(n) ). We only consider the\\ncase of ǫ-closeness for non-normalized density matrices and\\nprobability distributions (but the lemma also holds for the case\\nof ǫ-closeness based on trace distance).\\nTo prove our lemma, we need Weyl’s monotonicity principle\\nwhich we recall first.\\nTheorem 6 (Weyl monotonicity): If A, B are m by m Hermitian matrices and B is positive, then λi (A) ≤ λi (A + B)\\nfor all i = 1, . . . , m, where λi (M ) is the i-th eigenvalue of\\nM (ordered from largest to smallest).\\nLemma 3: Let ρ be a density matrix with eigenvalues λ1 ≥\\nλ2 ≥ . . . ≥ λm .\\n1) For any density matrix σ with eigenvalues µ1 ≥ µ2 ≥\\n. . . ≥ µm ,\\nσ ∈ B ǫ (ρ) ⇒ µ ∈ B ǫ (λ).\\n2) Given real numbers µ1 , . . . , µm such that µ ∈ B ǫ(λ),\\nthere exists a matrix σ with eigenvalues µ1 , . . . , µm such\\nthat σ ∈ B ǫ (ρ).\\nProof: We prove the result for\\nX\\nµi ≥ 1 − ǫ, ∀i 0 ≤ µi ≤ λi },\\nB ǫ (λ) = {µ :\\ni\\n\\nB ǫ (ρ) =\\n\\n{σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}.\\n\\nFor the first part, let σ be a (possibly non-normalized) density\\nmatrix with eigenvalues µ1 ≥ µ2 ≥ . . . ≥ µm and suppose\\nσ ∈ B ǫ (ρ). Since σ is positive we have µi ≥ 0 for all i.\\nAnd since σ ≤ ρ, we have that ρ − σ is positive as well,\\nso λi ≥ µi for all i (using Weyl’s monotonicity principle,\\nTheorem\\n6 above). Finally, note that tr(σ) ≥ 1−ǫ is equivalent\\nP\\nto i µi ≥ 1 − ǫ, so we conclude that µ ∈ B ǫ (λ).\\nFor the second part, let µ ∈ B ǫ (λ) be given. We write the\\nHermitian matrix ρ in diagonal form,\\nX\\nλi |vi ihvi |.\\nρ=\\ni\\n\\nfor eigenvectors vi (i = 1, . . . , m), and we show that the\\nHermitian matrix σ, defined by\\nX\\nµi |vi ihvi |,\\nσ=\\ni\\n\\nis in B ǫ (ρ).\\nSince µ ∈ B ǫ (λ), we have that 0 ≤ µi ≤ λi , and because ρ\\nand σ commute (eigenvalues\\nof ρ − σ are λi − µi ), we have\\nP\\n0 ≤ σ ≤ ρ. Clearly, i µi ≥ 1 − ǫ so tr(σ) ≥ 1 − ǫ as well,\\nand therefore σ ∈ B ǫ (ρ).\\nWe now proceed to prove the main result for the quantum\\ncase.\\nTheorem 7: For α ∈ [0, ∞], the ǫ-smooth entropy of\\na stationary, ergodic quantum source ρ given by its local\\ndensities ρ(n) , for n ∈ N, is close to the mean von Neumann\\nentropy:\\nlim sǫα (ρ) = s(ρ).\\nǫ→0\\nProof: We will apply Theorem 5 as follows.\\nFirst note that for the local densities ρ(n) for a quantum\\ninformation source ρ, we have that S (ρ(n) ) = H (λ(n) ), where\\nλ(n) denotes the probability distribution corresponding to the\\neigenvalues of ρ(n) . Consequently, s(ρ) = h(λ) as well, where\\nλ denotes the probability distribution corresponding to the\\neigenvalues of ρ.\\nNext, we recall the definitions of smooth Rényi entropy in\\nthe classical and quantum case, resp.:\\n\\x1a\\ninf Q∈Bǫ (P) Hα (Q), 0 ≤ α < 1,\\nǫ\\nHα (P) =\\nsupQ∈Bǫ (P) Hα (Q), 1 < α ≤ ∞.\\n\\x1a\\ninf σ∈Bǫ (ρ) Sα (σ), 0 ≤ α < 1,\\nSαǫ (ρ) =\\nsupσ∈Bǫ (ρ) Sα (σ), 1 < α ≤ ∞.\\nWe only consider the case α < 1, as the other case follows\\nby symmetry. We have that\\nSαǫ (ρ(n) ) =\\n=\\n=\\n\\ninf\\n\\nSα (σ)\\n\\ninf\\n\\nHα (µ)\\n\\nσ∈Bǫ (ρ(n) )\\nµ∈Bǫ (λ(n) )\\n\\nHαǫ (λ(n) ),\\n\\nusing that Lemma 3 implies that the infimum over B ǫ (ρ(n) )\\nis equal to the infimum over B ǫ (λ(n) ).\\nAs a consequence, we have that sǫα (ρ) = hǫα (λ) and the\\nresult follows from Theorem 5. Here, we use the fact that\\nquantum AEP implies classical AEP.\\nWe note that the actual convergence rate (as a function\\nof ǫ) is the same as in the classical case, which follows by\\nconsidering the analogons of Lemmas 1 and 2.\\nIV. N OTIONS\\n\\nOF\\n\\nǫ-C LOSENESS\\n\\nAs mentioned in the introduction, two notions of ǫ-closeness\\nwere originally introduced by Renner and Wolf [6], [7], which\\ncan both be used in the definition of classical smooth Rényi\\nentropy. For the quantum case, the paper by Renner and König\\n[5] only considers the notion of ǫ-closeness based on the trace\\ndistance. As the natural quantum analogon of the notion of\\nǫ-closeness based on non-normalized probability distributions,\\nwe have used the set of non-normalized density matrices which\\nare ǫ-close to a given density matrix ρ:\\nB ǫ (ρ) = {σ : tr(σ) ≥ 1 − ǫ, 0 ≤ σ ≤ ρ}.\\n\\n\\x0cThe entropy rates (Definitions 3 and 7), and consequently\\nthe results for these entropy rates (Theorems 3, 4, 5, and 7) do\\nnot depend on which of these notions of ǫ-closeness is used.\\nFurthermore, if the corresponding notions of ǫ-closeness are\\nused, the quantum case and the classical case are in general\\nconnected as follows:\\nSαǫ (ρ) =\\n\\ninf\\n\\nσ∈Bǫ (ρ)\\n\\nSα (σ) =\\n\\ninf\\n\\nµ∈Bǫ (λ)\\n\\nHα (µ) = Hαǫ (λ),\\n\\nwhere λ denotes the probability distribution corresponding to\\nthe eigenvalues of ρ.\\nWe note, however, that the smooth Rényi entropy Hαǫ may\\ndepend on which notion of ǫ-closeness is used, contrary to\\nwhat was stated before (see, e.g., Section 3.3 of the full version\\nof [6]). In general, one can show that\\nα\\nHα (Q) ≤\\n0 ≤ inf Hα (Q)− P inf\\nlog(1−ǫ),\\nα−1\\nδ(P,Q)≤ǫ\\nz Q(z)≥1−ǫ\\n∀z 0≤Q(z)≤P(z)\\n\\nfor 0 ≤ α < 1, and that\\nα\\nlog(1−ǫ) ≤ sup Hα (Q)−\\nα−1\\nδ(P,Q)≤ǫ\\n\\nsup\\n\\nHα (Q) ≤ 0,\\n\\nP\\nz Q(z)≥1−ǫ\\n∀z 0≤Q(z)≤P(z)\\n\\nfor 1 < α ≤ ∞. So, only for α = 0 either notion of ǫcloseness yields the same value for the smooth Rényi entropy\\nHαǫ . But for all other values of α, the difference may be as\\nα\\nlog(1 − ǫ). The maximum difference is attained\\nlarge as α−1\\nfor the uniform distribution P(z) = 1/m on a finite range Z\\nof size m, assuming that ǫ is sufficiently small (i.e., ǫ < 1/m).\\nACKNOWLEDGMENT\\nBoris Škorić is gratefully acknowledged for discussions in\\nthe early stage of this work.\\nR EFERENCES\\n[1] I. Bjelakovic and A. Szkola, ”The Data Compression Theorem\\nfor Ergodic Quantum Information Sources”, 2003. Available as\\nquant-ph/0301043.\\n[2] T. M. Cover and J.A. Thomas, ”Elements of Information Theory”, 2nd\\nedition, Wiley-Interscience, 2006.\\n[3] M. A. Nielsen and I. L. Chuang, ”Quantum Computation and Quantum\\nInformation”, Cambridge University Press, 2000.\\n[4] R. Renner, ”Security of Quantum Key Distribution”, Diss. ETH\\nNo. 16242, PhD Thesis, ETH Zürich, September 2005. Also available\\nas quant-ph/0512258.\\n[5] R. Renner and R. König, LNCS 3378, TCC 2005, pp. 407-425.\\n[6] R. Renner and S. Wolf, ”Smooth Rényi Entropy and\\nApplications, ISIT 2004, p. 233. Full version available as\\nhttp://qi.ethz.ch/pub/publications/smooth.ps.\\n[7] R. Renner and S. Wolf, ”Simple and Tight Bounds for Information\\nReconciliation and Privacy Amplification”, LNCS 3788, Asiacrypt 2005,\\npp. 199–216.\\n\\n\\x0c', 'Analyzing Design Process and Experiments on the\\nAnITA Generic Tutoring System\\nMatthias R. BRUST\\nFaculty of Sciences, Technology and Communication, University of Luxembourg\\nL-1359 Luxembourg, Luxembourg\\nand\\nSteffen ROTHKUGEL\\nFaculty of Sciences, Technology and Communication, University of Luxembourg\\nL-1359 Luxembourg, Luxembourg\\n\\nABSTRACT\\nIn the field of tutoring systems, investigations have\\nshown that there are many tutoring systems specific to a\\nspecific domain that, because of their static architecture,\\ncannot be adapted to other domains. As consequence,\\noften neither methods nor knowledge can be reused. In\\naddition, the knowledge engineer must have\\nprogramming skills in order to enhance and evaluate the\\nsystem. One particular challenge is to tackle these\\nproblems with the development of a generic tutoring\\nsystem. AnITA, as a stand-alone application, has been\\ndeveloped and implemented particularly for this purpose.\\nHowever, in the testing phase, we discovered that this\\narchitecture did not fully match the user’s intuitive\\nunderstanding of the use of a learning tool. Therefore,\\nAnITA has been redesigned to exclusively work as a\\nclient/server application and renamed to AnITA2. This\\npaper discusses the evolvements made on the AnITA\\ntutoring system, the goal of which is to use generic\\nprinciples for system re-use in any domain. Two\\nexperiments were conducted, and the results are\\npresented in this paper.\\nKeyword: Tutoring System, AnITA, Component-Based\\nWeb Application and Learning System.\\n\\n1. INTRODUCTION\\nResearch on tutoring systems in order to provide methods\\nfor more efficient and intense learning in ComputerBased Training (CBT) has attracted a lot of attention.\\nHowever, investigations have shown that there are many\\ndomain-specific tutoring systems that cannot be adapted\\neasily to other domains, because of their static\\narchitecture and domain-driven Graphical-User-Interface\\n(GUI). [5] pointed out that traditional tutoring systems\\nare extremely domain-dependant and neither methods nor\\nknowledge can be reused. Evidently, more and more\\ntutoring systems have been developed to match the\\nrequirements of different disciplines. As a consequence\\nof this traditional architecture, this work requires several\\npersons to continue maintaining and developing the\\nsystem, i.e. a knowledge engineer with programming\\nskills.\\n\\nThe general solution for these problems is to reuse the\\narchitecture of a tutoring system, and substitute the\\ndomain-specific parts with generic components. Our\\napproach has been to first examine existing tutoring\\nsystems to extract common functionality. After that, these\\nresults have been described in terms of components. This\\napproach, however, also faced problems, because in some\\ncases it has been very difficult to discover and recognize\\ndependencies between functionalities. Sometimes, we\\nhave not even been able to classify components because\\nof an extremely interwoven architecture [10]. Based upon\\nthese considerations, AnITA, a generic tutoring system,\\nhas been developed and implemented. In the generic\\ntutoring system concept proposed, the domain has been\\nseparated as far as possible from the architecture of the\\ntutoring system itself.\\nThe first draft of AnITA intended to serve as an\\nenvironment for both training and testing. We received\\npositive feedback from the use of AnITA, showing that it\\ntackles the problems aforementioned. However, other\\nproblems of a different scope have been revealed\\nthroughout its use. Therefore, we decided to partly redesign AnITA to deal with these problems as well.\\nThis work reports on the evolvement of the AnITA\\ntutoring system, which aims to use generic principles for\\narchitecture re-use in any domain. In section 2, the\\ndevelopment and design process of AnITA is described,\\npointing out new obstacles and problems that appeared.\\nSection 3 introduces AnITA2 as a method to tackle this\\nsituation. Experiments were setup and accomplished with\\nAnITA2. Their results are shown in Section 4. Section 5\\ngives an overview of the new project that integrates\\nAnITA2 with CALM [1] in order to work in a mobile\\nenvironment based on ad-hoc networks. Section 6\\nsummarizes the experiences and results we obtained\\nduring our work with the AnITA tutoring system.\\n\\n2. AnITA: TRAINING AND TESTING\\nConcept\\nAnITA is intended to serve as environment for both\\ntraining and testing. In the training mode, students are\\nable to “play” with the questions, see what happens when\\nchoosing the wrong answers, and recognizing major\\nmisconceptions. In the test mode, students demonstrate\\ntheir understanding in an “official” test that can be\\n\\n\\x0cevaluated by a human tutor on a remote computer. To\\nmotivate learning, AnITA provides a variety of question\\ntypes: multiple-choice, calculation-driven, and fill-in-theblank questions with two different fill-in types (text and\\ncombo-box). As already mentioned, the domain has been\\nseparated from the architecture of the tutoring system\\nitself. One benefit of such a separation is the possibility to\\nuse XML for defining tests and, thus, a step towards\\nestablishing well-known standards [8]. Based on such a\\nlanguage, a professor with little or no programming skills\\nis still able to model tests. Furthermore, systemindependent domain design will be possible. There will\\nno longer be a need for a knowledge engineer. Usually,\\ntutoring systems are not able to adapt to student’s\\nknowledge. It has been a challenge to investigate\\npossibilities of finding generic principles of adaptation.\\nTo do that, test paradigms have been examined and\\nrealized as described in the subsequent section [4].\\nTest-Paradigms\\nXML is used as technology to describe test paradigms.\\nAnITA realizes four test paradigms (cf. Figure 2.1). Free\\nselection displays questions in the order they appear in\\nthe XML file. Causal Links Selection is sensitive to right\\nand wrong answers.\\n\\nImplementation and Architecture\\nAnITA is a pure Java stand-alone application based on\\nthe Swing GUI that requires the Java Runtime\\nEnvironment to run. With this application, students can\\napply their knowledge in the available questionnaires.\\nInteractions are evaluated, but neither stored on the\\nsystem’s internal state nor on a local database. In the\\ntesting mode AnITA can establish a connection to a\\nremote database through a Servlet that runs on the remote\\nhost. Because the Servlet is written in Java and the\\nimplemented database has a rational design, a\\nJDBC/ODBC-driver is used to set up the communication\\nbetween the Servlet and the database. The tests are stored\\non the client (training mode) or on the server (testing\\nmode) as illustrated in figure 2.2.\\nServer\\n\\nClient\\n\\n(JDSK 2.1)\\n\\nAnITA\\n(Java)\\n\\nHTTP\\n\\nAnITA\\nServlet\\n\\nJDBC\\n\\nXML\\n\\nXML\\n\\n(Training)\\n\\n(Test)\\n\\nQuestion 1\\n\\n:\\n\\nQuestion 1\\nRight\\nQuestion 2\\n\\nAnITA\\nDB\\n\\nWrong\\n\\nBalanced\\nOrdering\\nConstraint\\n\\nQuestion 3\\n\\nQuestion n\\n\\n:\\nQuestion nx\\n\\nBrowser\\n\\nServer\\n(Apache Tomecat 5.0)\\n\\n:\\nQuestion m\\n\\nHTML\\n\\nHTTP\\nSession\\n\\nAnITA JSP\\n\\nQuestion 1\\nDynamic Ordering\\nConstraint\\nQuestion 2\\n\\nQuestion 3\\n\\nJDBC\\n\\nQuestion 4\\n\\nXML\\n(Test)\\nQuestion 5\\n\\nAnITA\\nDB\\n\\nQuestion 6\\n\\nOrdering\\nConstraint\\nQuestion 7\\n\\nFigure 2.1: Causal Links, (Dynamic) Ordering Constraints and\\nBalanced Ordering Constraints as Test-Paradigms in AnITA.\\n\\n(Dynamic) Ordering Constrain Selection follows a predetermined order for selecting questions. A forced\\nordering constraint question can only be called upon\\nother question as a reference. This paradigm takes into\\naccount the existence of a question that is a subpart of\\nother questions and that according to the context cannot\\nappear alone. A dynamic ordering constraint selection\\nenables the system to choose randomly from different\\nconstraints. This feature is aimed to create different tests\\nout of just one XML-file. Balanced Constraint Selection\\nis more basic from a conceptual point of view. The\\nattribute balance is introduced with its values n and p.\\nThe n value implies an arithmetic average a for the last n\\nselected questions. If a is greater than p the system may\\ncontinue selecting the next question and following the\\nordering constraints, causal links or free selection.\\nOtherwise, the selection repeats the last n questions.\\n\\nFigure 2.2: Overview of the AnITA architecture (left) and AnITA2\\narchitecture (right)\\n\\nExperiences\\nAs reported in [3], first experiments with AnITA have\\nshown that students expressed mostly positive comments.\\nAt this time, we were able to apply four domains in\\nAnITA: Operating Systems, Statistics, Architecture, and\\nInformation Systems. We understood this as indication\\nthat we were on the promising way on creating a generic\\ntutoring system successfully. Although these experiments\\nwere seen as encouraging for AnITA, nowadays, we have\\nto re-evaluate AnITA from a wider perspective and\\nconcede that important aspects had not been evaluated.\\nTo install AnITA on the university’s computer, we\\nneeded a computer expert who knows how Java programs\\nwork and how to install the Java Runtime Environment.\\nAnITA had to be installed on all 28 computers in the lab.\\nAlthough these steps may appear very simple, we must\\nnot forget to design applications as simple as possible –\\nincluding their installation procedures.\\nFurthermore, because of the overall design (Swing,\\nXML parser, complex data structures etc.), AnITA runs\\nvery slowly on older computers that can be found quite\\noften in public labs. It took between fifteen seconds and\\n\\n\\x0cby designing the system in a component-based way with\\nrespect to the question types. Now, it is easy to create\\nnew questions types in AnITA2, because it is not\\nnecessary to change the application code. Additionally, a\\nperformance meter was added. This enables students to\\ncompare visually his/her performance in a bar chart that\\nshows the performance of the last 20 training units of one\\nspecific test (Figure 3.1).\\nImplementation & Architecture\\nAnITA2 is designed on client/server principles as a Webapplication using Java Server Pages (JSP). Therefore,\\nAnITA2 does not have to be installed, because it can be\\naccessed using a standard Internet browser. Tests are\\nexclusively stored on the server-side and are transmitted\\nto the client on demand (on a question-by-question basis).\\nAll data is stored in a rational database on server side.\\nAnITA2 uses component-based techniques for\\nextensibility with respect to question types. Internally,\\nintrospection is used to implement this feature.\\nIntrospection allows Java code to discover information\\nabout the fields, methods and constructors of arbitrary\\nclasses and to dynamically invoke them.\\n\\nFigure 3.1: Login page and a fill-in question with its correction\\n\\none minute to launch AnITA on these computers,\\nobviously an undesirable situation.\\nFinally, another design flaw became apparent that we\\nfound unsatisfactory from a pedagogical point of view:\\nTo evaluate the system and the student, it would be best\\nto have as much information as possible about both [8].\\nHowever, in AnITA, the student could act autonomously\\nin the training mode without ever publishing his/her\\nresults and behavior. The pedagogical value of AnITA\\ndecreased because it was almost impossible to receive\\ndata about the student’s performance since the last tests.\\nEven though it was satisfying having reached our goal, it\\nwas disappointing to recognize that AnITA caused such\\nserious problems. As we observed these problems, new\\ntechnologies became available and we decided to redesign AnITA. As a result, AnITA2 has been\\nimplemented.\\n\\n3. AnITA2: EVALUATION-BASED DESIGN\\nConcept\\nThe basic concept of AnITA did not change, but it was\\nextended in some areas. As described in the previous\\nsection, AnITA provides a variety of question types.\\nOne concrete additional demand has been identified\\ndue to the following case: a physician analyzed AnITA\\nand expressed the desire to interact with x-ray pictures to\\nlocate illness areas. In AnITA2, we took this into account\\n\\npublic String setTest() {\\n// Choose next question\\n...\\n// Initialize the class loader\\nMultiClassLoader loader = null;\\nloader =\\nnew FileClassLoader(GlobalValues.CLASSES_PATH + File.separator);\\n...\\ntestClass = loader.loadClass(\"anita/\"+n.getNodeName());\\n...\\ntestObject = testClass.newInstance();\\n...\\nString testInHTML = \"\";\\n...\\nClass[] classPara = new Class[1];\\nclassPara[0] = org.w3c.dom.Node.class;\\nMethod testMethod = testObject.getClass().\\ngetMethod (\"set\"+n.getNodeName(), classPara);\\ntestInHTML += (String) testMethod.invoke(testObject, new Object[] { n });\\n...\\nreturn testInHTML;\\n}\\n\\nFigure 3.2: AnITA2 code that invokes component-based question types.\\n\\nWhile reading the XML-based questions, AnITA2 needs\\nto execute the appropriate code related to that question\\ntype. Questions types are not hard coded directly in the\\nAnITA2 application. Rather, question types can be added\\ndynamically by simply changing the DTD specification\\nand adding a new component to the system. Figure 3.2\\nshows the code that invokes the question types as\\ndescribed above.\\n\\n4. EXPERIENCES WITH AnITA2\\nSince AnITA2 is a generic approach, it is important to\\nvalidate the results with a variety of experiments. In order\\nto do this, we set up an experiment in a language school\\nas soon as AnITA2 became stable. Here, AnITA2\\nprovided basic questions to teach German to Brazilians.\\nAnITA2 was improved by using the results of this\\nexperiment and another one was set up that focused on\\nmore in-depth questions regarding the course “Operating\\nSystems” at a European university.\\n\\n\\x0cLanguage Test\\nOne experiment was done in a language school in Brazil.\\nAll nine students were Brazilians and between 18 and 24\\nyears old. They had attended their initial German course\\nfor eight weeks having two hours of lectures per week.\\nAll students were well experienced with computers. The\\nquestions were simple and covered a wide range.\\nThe objective of our test was to find out, if students\\nwho paid to learn English would use a free onlinelearning tool. We also prepared AnITA2 to find answers\\nto the following questions.\\n•\\n•\\n•\\n\\nAre there indicators that students would use this\\nfree tool on their own?\\nIn which way would they use AnITA2? (When?\\nHow long? How many times? Progress?)\\nWhich influence does the use of AnITA2 have\\nin the classroom?\\n\\nThey could take the test by using all of the teaching\\nmaterial and could use as much time they wanted. We did\\nnot explain that it was a test to show if they really would\\nuse AnITA2 and we did not mention that any data was\\nstored in a database.\\nResult: Most of the students (six students) discontinued\\nthe test between the eighth and thirteenth question.\\nConclusion: We wonder, if students prefer a certain\\nnumber of questions for being more motivated than with\\na different number and in which way the time they spend\\non the exercises is influencing on this number. Next\\nexperiments have to show this correlation.\\nResult: Two students commented that it was problematic\\nnot to know how many questions remained.\\nConclusion: The DTD-Specification of AnITA2 and\\nAnITA2-code were modified to switch on the option to\\nshow the remaining number of questions on the left side\\nof the AnITA page. Modifying the specification, it has to\\nbe observed that the number of remaining question\\nchanges dynamically when using Causal Links,\\n(Dynamic) Ordering Constraints and Balanced Ordering\\nConstraints. For this, it will be necessary to readapt the\\nnew feature to realize test paradigms.\\nResult: There was also the desire to present all question\\non one page.\\nConclusion: Here we recognized the desire to change\\nAnITA2’s format to a traditional questionnaire.\\nFurthermore, after we asked more specific questions, we\\ndiscovered that the students hoped to find the answer to a\\nquestion by reading other questions (hints, etc.). But we\\nargued that this situation would complicate the creation\\nof questionnaires because one question cannot be used to\\nanswer another question and this would restrict AnITA2.\\nFinal Conclusion: Most students had a job aside from\\ndoing their studies. Hence, they sometimes could not\\nattend the classes. These students in particular used\\nAnITA2 in their free time to try to make up for what they\\nmissed. Therefore, there is a need for an extra option such\\nas this. During the lectures, students started discussing\\nabout the AnITA2 system. This seemed to encourage\\nother students that did not use AnITA2 before to start\\nusing it, probably because they wanted to be able to\\nfollow the discussion. Comparing the test results\\ncollected, we discovered that a lot of students kept on\\nusing AnITA2 until they had answered all the questions\\n\\ncorrectly. Therefore, we conclude that AnITA2 can be\\nseen as a method to encourage learning.\\nIn-depth Test for Operating Systems\\nOn the basis of the modifications, which resulted from\\nthe first experiment, we conducted a second experiment.\\nOur domain was the discipline of synchronization as\\nsubpart of operating systems in an in-depth level. The\\nparticipants were three students between the ages of 24\\nand 28 that have a few days to their final diploma exams.\\nThe main questions were:\\n•\\n•\\n•\\n\\nDoes AnITA2 have the capacity to design an indepth test?\\nWould the results of the test match the\\nprofessor’s evaluation for the same student?\\nWhat is the student’s opinion about using\\nAnITA2 for testing or training?\\n\\nThey could not use the teaching material, but could take\\nas much time as they wanted. We did not explain that it\\nwas a test to show if they really would use AnITA2 and\\nwe did not mention that any data would be stored in a\\ndatabase.\\nResult: All students confirmed that testing their\\nknowledge on AnITA2 was very satisfying, because they\\nincreased their self-confidence for the “real” test. They\\nmentioned that they prefer AnITA2 to other multiplechoice-based tutoring systems, because of the variety of\\nquestion types. Interestingly, one of them started\\nexplaining that he felt more difficulties with pure text\\ninputs than with multiple-choice questions. Other\\nstudents agreed with his opinion on this issue.\\nConclusion: Our decision to offer a component-based\\nsystem for question types is supported by some students’\\nopinions. The classification of question types we did\\nbefore the design of AnITA was understood intuitively by\\nthe students [2].\\nResult: The desire was expressed that the tool could\\nadapt to the student’s knowledge.\\nConclusion: In intelligent tutoring systems, adaptation is\\nan important role, but we see it as a challenge to continue\\nworking on our generic adaptation methods, although we\\nfocus on a tutoring system.\\nResult: One student pointed out that AnITA2 is\\nsurprisingly capable to testing algorithms and fragments\\nof code in an interesting way, but he also mentioned that\\nthese questions took a lot of time.\\nConclusion: We conclude that it was unavoidable to add\\nspecific functionality to a generic tutoring concept in\\norder to widen the scope. This could also be a hint of the\\nlimits of the generic concept used in AnITA and AnITA2.\\nFinal Conclusion: It is difficult to compare the student’s\\nperformance on AnITA2 with the professor’s evaluation,\\nbecause the system offers an in-depth written test while\\nthe final diploma exam is an oral test that covers the\\nentire discipline. Comparing AnITA’s evaluation to the\\nprofessor’s, the student tended to receive a higher grade\\nfrom the professor. We were not sure about the\\nsignificance of this result, so we decided to prepare\\nanother experiment to investigate this point more\\nthoroughly. Finally, the graphical performance meter was\\nseen as very helpful for self-evaluation.\\n\\n\\x0c5. FUTURE WORK\\n\\nquestions, and annotations, thereby increasing the scope\\nand usefulness of the teaching material.\\n\\nMore and more students use palmtop and handheld\\ncomputers at home and at university. With larger memory\\ncapacities, a variety of data input devices, and the ability\\nto link into wireless networks, applications from different\\ndomains must be adapted to run in mobile environments.\\nIn the future, we will work on this challenge by\\ndeveloping CARLA, a learning system for mobile ad-hoc\\nnetworks. In contrast to the AnITA system, the purpose\\nof CARLA is to support cooperative learning for two\\nreasons. First, because there are extreme constraints on\\ninformation dissemination in ad-hoc networks [6], a\\ncooperative concept has a better chance of being\\nsuccessfully implemented. On the other side, we see this\\nrestriction as an opportunity to gain experience with postmodern learning theories, i.e. cooperative and\\ncollaborative learning in a computer-based mobile\\nenvironment.\\nScenario\\nFor example, students might join forces to prepare for\\nexams. Teaching materials like lecture notes, slides, and\\nbasic questions, are distributed to students at specific\\nlocations like the lecture room or professor’s office. This\\nis done by a professor’s mobile device or by a stationary\\nnode. Initially, all students start using the same teaching\\nmaterial. During a lecture, students can write annotations\\non the slides [1]. Later they may want to test their\\nknowledge by answering questions. Through this process,\\nthey may discover a correlation between sections of the\\nteaching material and their annotations and questions.\\nThey can express their findings by adding links. The\\nresulting personalized material enables students to gain a\\ndeeper understanding of the subject.\\nAs a cooperative environment, the CARLA system\\naims to disseminate all additional material to the students.\\nCARLA enables students to analyze received material by\\nusing evaluation mechanisms. The resulting evaluations\\nwill be shown on annotations, links, questions and\\nteaching material, to represent the “usefulness” of an\\nelement. Students are able to recognize misleading or\\nfalsified content more easily. Figure 5.1 gives an example\\nof teaching material combined with additional elements.\\nTeaching Material\\n\\nQuestion\\n\\n6. CONCLUSION\\nIn this paper, the design and evolvement of the AnITA\\ngeneric tutoring system is described. The concept\\nproposed intentionally separates the domain from the\\ntutoring system. The main benefit of this system is that it\\ncreates the possibility to apply an XML-based language\\nfor tests and to establish standard directives for system\\ndesign [8]. Based on the resulting language, even\\nprofessors with little or no programming skills are\\nenabled to model tests, and both the domain and tutoring\\nsystem can be designed and maintained independently.\\nExperiments with AnITA showed that it was\\nnecessary to re-evaluate AnITA from a broader\\nperspective, and concede that important aspects had not\\nbeen considered in both the evaluation and the design\\nprocess.\\nWhile adapting AnITA to AnITA2, the code became\\nabout 25 % shorter and more concise. We concluded that\\nthis was the result of using a Web browser as user\\ninterface. The browser realizes the GUI through based\\nupon HTML code that is effectively plain text.\\nSeveral years ago, researchers were faced with the\\nchallenge of realizing learning environments as an\\nInternet-based client/server application. Since the use of\\nmobile systems is increasing, the focus has shifted to the\\ndevelopment of self-organized learning platforms for\\nsuch systems. CARLA was proposed as a way to deal\\nwith this new aspect. We hope that CARLA can continue\\nthe work that the AnITA generic tutoring system has\\nbegun. Finally, the results of our experiments indicate\\nthat AnITA2 seems to be a catalyst for increasing the\\nefficiency and intensity of the learning process.\\n\\nACKNOWLEDGMENTS\\nThis research is supported in parts by the Luxembourg\\nMinistère de la Culture, de l\\'Enseignement Supérieur et\\nde la Recherche.\\n\\nStatistics\\n\\nREFERENCES\\nAnnotation\\n\\nEvaluation\\n\\nLink\\n\\nAnnotation\\n\\n[1] C. M. Adriano, Inquiring the Course Paradigm\\nwith CALM. International Conference on Engineering\\nand Computer Education (ICECE), 1999, 265 – 269.\\n\\nEvaluation\\n\\nFigure 5.1: Illustrating the correlation between questions, annotations,\\nlinks and evaluations\\n\\nCARLA is designed to run with short periods of\\ninteraction between the devices in the ad-hoc network.\\nThis design is important in order to take into account the\\nrestrictions of a mobile ad-hoc network.\\nThe objective of CARLA is not only for professors to\\nbe able to evaluate students’ cooperative work, but also to\\nsteer the learning process in the right direction with\\nadditional links and/or annotations [7]. Another benefit of\\nCARLA is that the professor will be able to redesign the\\ninitial teaching material based on the students’ links,\\n\\n[2] M. R. Brust, AnITA: Eine allgemeine XMLbasierte Architektur fuer Intelligente Tutorielle\\nSysteme, Matthias Rudolf Brust, Dissertation for Master\\nof Science, University Trier, Germany, 2002.\\n[3] M. R. Brust, AnITA: Implementing E-Training and\\nE-Tests Environments by using a Generic\\nArchitecture based on XML, XXX Brazilian Congress\\nof Education in Engineering, COBENGE 2002, 22 - 25\\nSeptember 2002, Piracicaba, Sao Paulo, Brazil\\n[4] M. R. Brust, Applying Test-Paradigms in a Generic\\nTutoring System Concept for Web-based Learning,\\n\\n\\x0cInternational Conference on Information and Resource\\nManagement, Philadelphia, USA, 2003.\\n[5] V. Devedzic, D. Radovic, On the Notion of\\nComponents for Intelligent Tutoring Systems, L, In:\\n4th International Conference on Intelligent Tutoring\\nSystems (ITS’98) Proceedings, San Antonio, Texas EUA,\\n1998.\\n[6] Ch. Hutter, M. R. Brust, S. Rothkugel, ADS—\\nDirectory Services for Mobile Ad-Hoc Networks\\nBased on an Information Market Model, International\\nWorkshop on Ubiquitous Computing (IWUC 2004),\\nPorto, Portugal, 2004.\\n[7] M. R. Brust, S. Rothkugel, On Anomalies in\\nAnnotation Systems, The Third Advanced International\\nConference on Telecommunications (AICT 2007),\\nMauritius, 2007.\\n[8] W. D. Leiva, Ch. M. Adriano, P. C. Masiero,\\nComputer Supported Authoring of Questionaires,\\nICECE’2000, São Paulo, Brazil.\\n[9] H. H. Remmers, N. L. Gage, J. Francis Rummel, A\\npractical Introduction to Measurement and\\nEvaluation, Harper & Brothers Publishers, New York,\\n1959.\\n[9] K. VanLehn, Andes: A coached problem solving\\nenvironment for physics, Intelligent Tutoring Systems:\\n5th International Conference, Montreal, Canada.Gauthier,\\nFrasson, VanLehn (eds), Springer (Lecture Notes in\\nComputer Science, Vol. 1839), pp. 133-142\\n\\n\\x0c', '1\\n\\nColour Image Segmentation by the\\nVector-valued Allen-Cahn Phase-field Model: a\\nMultigrid Solution\\narXiv:0710.0736v1 [cs.CV] 3 Oct 2007\\n\\nDavid A Kay\\n\\n∗\\n\\nAlessandro Tomasi\\n\\nAbstract\\nWe propose a new method for the numerical solution of a PDE-driven model for colour image\\nsegmentation and give numerical examples of the results. The method combines the vector-valued\\nAllen-Cahn phase field equation with initial data fitting terms. This method is known to be\\nclosely related to the Mumford-Shah problem and the level set segmentation by Chan and Vese.\\nOur numerical solution is performed using a multigrid splitting of a finite element space, thereby\\nproducing an efficient and robust method for the segmentation of large images.\\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\\n\\nI. Introduction\\nThe Mumford-Shah functional was first proposed in [24] as a general way to pose the problem\\nof image segmentation. Reviews can be found for example in Petitot [26], and Fusco [12]. The\\ngenerality of its statement has led to several methods of solution; in particular, the model sometimes\\nreferred to as the reduced Mumford-Shah was solved by the level set method by Chan and Vese [8],\\nbased on a previous paper on the motion of multiphase junctions tracked by the level set method\\nby Zhao, Chan, Merriman and Osher [33], and subsequently extended in Chan and Vese [6], [7],\\n[9], Vese [29], Chan, Shen and Vese [5]. Due to the extent of their work, it is also often referred\\nto as the Chan-Vese model. The level set is used to track the boundaries of objects and should\\nconverge to the set of contours in the image.\\nEsedoḡlu and Tsai [11] proposed the Allen-Cahn equation, also known as the phase field model,\\nas a method of solution to the reduced Mumford-Shah problem when used in conjunction with the\\nChan-Vese fitting terms. The Allen-Cahn equation has been used in the context of image processing\\nby Beneš, Chalupecký and Mikula [3], who first convolve the image with a Gaussian smoothing\\nkernel to eliminate noise, and then use it as an anisotropic gradient filter based on the proposals\\nby Perona and Malik [20]. Our approach differs significantly in that we use no smoothing kernel\\nand we modify the energy functional by adding fitting terms; moreover, we use the vector-valued\\nformulation of the Allen-Cahn equation due to Garcke, Nestler and Stoth [13].\\nA different phase transition model (Modica-Mortola) was also recently used by Jung, Kang and\\nShen [14], and although the model and numerical method of solution therein differs from this work,\\nit is in many ways similar in the fundamental approach of adapting a phase transition model to\\nsolve the Mumford-Shah problem, and in the results obtained.\\nOxford\\nComputing\\nLaboratory,\\nWolfson\\nBuilding,\\nParks\\nRoad,\\nOxford,\\nOX1 3QD,\\nUK.\\nDavid.Kay@comlab.ox.ac.uk Tel: 0044 (0)1865 610814\\nDepartment of Mathematics, University of Sussex, Falmer, Brighton, BN1 9RF, UK. keug1@sussex.ac.uk Tel: 0044\\n(0)1273 873450. Corresponding author.\\n\\nDRAFT\\n\\n\\x0c2\\n\\nOur computations are solved by a multigrid algorithm which falls into the category of Successive\\nSubspace Corrections (see Xu [30], [31]). This was successfully applied to the vector-valued AllenCahn equation in Kornhuber, [17], Kornhuber and Krause [18], [19] with a small variation (see\\nKornhuber and Krause [15]).\\nIn section II we briefly introduce and summarise previous directly relevant work leading up to\\nsection II-C, in which we formally introduce our own formulation and show how the minimisation\\nof our functional leads to the desired system of PDEs; in section III we discretise the system and\\nintroduce the numerical method of solution, and in section IV we present a few practical aspects\\nof implementation together with examples.\\nII. Image segmentation by the Allen-Cahn equation\\nA. Relation to the Mumford-Shah functional\\nGiven an image I ∈ Ω ⊂ R2 , the Mumford-Shah method seeks to partition the domain Ω into\\nseveral subdomains Ωi separated by a set K of boundaries, also known as edges or discontinuities.\\nThis segmentation takes the form of piecewise smooth functions u ∈ Ω − K which are discontinuous\\non K; the method of selection from all possible functions u is minimisation of an energy functional\\nZ\\nZ\\nZ\\n2\\nMS(u) =\\n|∇u| dx + µ\\ndσ + λ (u − I)2 dx\\n(1)\\nΩ−K\\n\\nK\\n\\nΩ\\n\\nwhere µ, λ are positive constants.\\nThe first term minimises the variation of u and promotes its smoothness, the second term\\nminimises the length of interfaces and determines the boundaries between Ωi , and the third term,\\nsometimes referred to as the fidelity or fitting term, minimises the variation between u and I.\\nAs noted in Petitot [26], the coefficients µ and λ define several scales of the problem: low µ leads\\nto fine-grain segmentation, high µ to coarse-grain results. Sensitivity to contrast is measured by\\n(4λ2 µ)1/4 and robustness to noise depends on λµ.\\nMany variations on this theme have been proposed since its first formulation. Mumford and Shah\\nthemselves pointed out that a reduced form of the problem, referred to as the minimal partition\\nproblem, is the restriction of u to piecewise constant functions, i.e. u = ci with each ci a constant\\non each connected region Ωi . The minimising values are then clearly the averages of I across each\\nregion.\\nIn the level set case, using by way of example a single function φ to segment an image containing\\nonly one object against a background, the Chan-Vese functional introduced in [8] replaces the\\nfidelity term in MS(u) by two fitting terms,\\nZ\\nZ\\n2\\nF1 (φ) + F2 (φ) =\\n|I − c1 | x +\\n|I − c2 |2 dx\\n(2)\\ninside(φ=0)\\n\\noutside(φ=0)\\n\\nwhere c1 , c2 are the average of I inside and outside φ = 0, respectively. Considering for a moment the\\nideal situation in which the image contains only one object, i.e. is split into two regions of roughly\\nconstant value clearly separated by a gradient boundary, these two terms are clearly minimised\\nwhen the set φ = 0 coincides with the contour of the object, i.e. the set K.\\nB. A phase-field formulation\\nThe Allen-Cahn PDE was introduced in [1] to model the domain coarsening occurring after a\\nphase transition. It follows the evolution of a function u(x) known as the order parameter, which\\nsmoothly varies between the values of 0 and 1 across an interface1 to represent which parts of the\\n1\\n\\nThe original model was defined on the interval [−1, 1], but it is convenient to consider [0, 1] for our purposes,\\nwithout loss of generality.\\nDRAFT\\n\\n\\x0c3\\n\\nmaterial are in one phase or another. It is obtained by minimising the following energy functional:\\nZ\\n1\\n(3)\\nAC(u) =\\nǫ|∇u|2 + Ψ(u) dx\\n4ǫ\\nΩ\\nThe function Ψ(u) represents a potential that attains minimal values at the two extreme values of\\nu. This is not in general a convex function, nor is it necessarily smooth. Esedoḡlu and Tsai [11],\\nfor example, chose the quartic double-well form of the potential, Ψ(u) = u2 (1 − u)2 .\\nComparing the first two terms in (1) and (3), the first term is identical up to a scaling constant,\\nwhile the role of the second term is quite similar since the parameter ǫ is directly related to the\\nwidth of interfaces between phases; it is well known that minimising Ψ(u) as above reduces both\\nthe width and length of boundaries. In this paper, we examine the results of extending (3) to its\\nvector-valued formulation and combining it with fitting terms such as those in (2).\\nIt is reasonable to suggest that the results obtained by a level set method and a phase-field\\nmethod should be closely comparable because both are known to be equivalent to curve motion by\\nmean curvature; for an overview, see for example [10].\\nThe method of solution described in [11] follows the MBO thresholding scheme by Merriman,\\nBence and Osher [21], [22], which assigns to the order parameter either one or the other extremal\\nvalue at each step; we propose to use the formulation known as the double obstacle instead, in\\nwhich, Ψ takes the form\\nΨ(u) = Φ(u) + Q(u),\\nwhere\\n\\nΦ(u) =\\n\\n(\\n\\n0\\nu ∈ [0, 1]\\n+∞ u < [0, 1]\\n\\nis known as the indicator function on the set [0, 1], and Q(u) is the concave quadratic\\n\\nQ(u) = u(1 − u).\\nC. A modified vector-valued Allen-Cahn equation\\nOur primary objective is to achieve a fast, robust image segmentation method. Given a domain\\nΩ ⊂ R2 and an image I: Ω → Rc with c data channels or colours, we follow the motion of a function\\nu with several (N) components that we want to adapt to the significant features of I. We propose\\nto do this by solving a system of PDEs on a finite-element space by a multrigrid method, and we\\nderive this system by minimising an energy functional of the form\\nZ\\n|∇u|2 u(1 − u)\\nE=\\nǫ\\n+\\n+ Φ(u) + λu · F(c, I) dx,\\n(4)\\n2\\nǫ\\nΩ\\nwhere\\n\\nR\\n\\nF(c, I) = (I − c)2 , c = RΩ\\n\\nuI\\n\\nu\\nΩ\\n\\n,\\n\\n(5)\\n\\nthe quantity c representing the average of I in u, in other words being a measure of the oscillation\\nof the data over the support of u.\\nIn order to achieve a simultaneous segmentation of I into arbitrarily many pieces, we refer to the\\nvector-valued formulation of the Allen-Cahn system was introduced in Garcke, Nestler and Stoth\\n\\nDRAFT\\n\\n\\x0c4\\n\\n[13], which allows one to consider an arbitrary number of components to the order parameter, now\\ndescribed by a single vector-valued function u ∈ VN in the function set defined as\\n\\uf8f1\\n\\uf8fc\\nN\\n\\uf8f4\\n\\uf8f4\\n\\x10\\n\\x11N X\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8fd\\n2\\nVN ≔ \\uf8f4\\n:\\nv\\n∈\\nH\\n(Ω)\\nv\\n(x)\\n=\\n1\\na.e.\\nin\\nΩ,\\nv\\n≥\\n0\\na.e.\\nin\\nΩ\\n,\\n(6)\\n\\uf8f4\\ni\\ni\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n\\uf8fe\\ni=1\\n\\nIn other words, the vector-valued function u must pointwise remain on the N-dimensional Gibbs\\nSimplex\\n\\uf8e6\\n\\uf8f1\\n\\uf8fc\\n\\uf8e6\\nN\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\nX\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8f2\\n\\uf8fd\\nN\\uf8e6\\n\\uf8e6\\nx\\n∈\\nR\\nx\\n=\\n1,\\n0\\n≤\\nx\\nGN : = \\uf8f4\\n,\\n(7)\\n\\uf8f4\\n\\uf8e6\\ni\\ni\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8f3\\n\\uf8fe\\n\\uf8e6\\n\\uf8e6\\ni=1\\n\\nwhich is itself an (N − 1)-dimensional subset of the hyperplane\\n\\uf8e6\\n\\uf8fc\\n\\uf8f1\\n\\uf8e6\\nN\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\nX\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8fd\\n\\uf8f2\\nN\\uf8e6\\n\\uf8e6\\nx\\n=\\n1\\nΣN : = \\uf8f4\\n.\\nx\\n∈\\nR\\n\\uf8f4\\n\\uf8e6\\ni\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8fe\\n\\uf8f3\\n\\uf8e6\\n\\uf8e6\\ni=1\\n\\nN\\n\\nTrivially, xi ≤ 1 ∀x ∈ G (though not so for all x ∈ ΣN ).\\nAn N-dimensional extension to the potential function Ψ(u) is now required, with N minima given\\nby u j = 1, ui,j = 0; the form\\nN\\nX\\nQ(u) =\\nui (1 − ui )\\ni=1\\n\\nhas been used in the following, with Φ(u) now the indicator function on GN and Ψ(u) = Φ(u)+Q(u)\\nas for the standard double-obstacle Allen-Cahn.\\nFollowing standard procedure, we wish to minimise the energy functional (4) to derive a pde to\\nwhich we can introduce a pseudo-time stepping to find a global minimum. To find the variational\\nderivative of (4) in a direction v, care must be taken to remain in GN ; in other words, we do not\\nwant our function\\nto leave the allowed\\nset. If considering E(u + αv) for some α ∈ R, it must be that\\nP\\nP\\nP\\ni u + αv =\\ni u = 1, and hence\\ni αv = 0. To that end, Garcke, Nestler and Stoth [13] introduce\\nthe hyperplane\\n\\uf8e6\\n\\uf8f1\\n\\uf8fc\\n\\uf8e6\\nN\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\nX\\n\\uf8e6\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8f2\\n\\uf8fd\\n\\uf8e6\\nN\\uf8e6\\nTΣN : = \\uf8f4\\nu\\n∈\\nR\\nu\\n=\\n0\\n\\uf8f4\\n\\uf8e6\\ni\\n\\uf8f4\\n\\uf8f4\\n\\uf8e6\\n\\uf8f3\\n\\uf8fe\\n\\uf8e6\\n\\uf8e6 i=1\\n\\nwhich is at all points tangent to ΣN (and hence to GN ), together with the projection operator T\\ndefined by\\n1\\n(8)\\nTx: = x − (x · 1N ) · 1N\\nN\\nas acting on a vector x ∈ RN , where 1N : = (1, 1 . . . 1) ∈ RN . Geometrically, the two hyperplanes ΣN\\nand TΣN are parallel; TΣN passes through the origin; the vector\\n1N\\nN̂: =\\n∈ GN\\nN\\nis normal to TΣN and represents the shortest distance between ΣN and TΣN . As N grows larger,\\nthis distance grows smaller. By construction, u − v ∈ TΣN ∀u, v ∈ ΣN .\\nWe now turn to the question of minimising (4). As is usual for the Allen-Cahn functional (3), we\\nuse a gradient descent method, i.e. we find the directional derivative of (4), and obtain a variational\\ninequality to be solved numerically. Using the notation h·, ·i to indicate the standard inner product,\\nand using the the N-subgradient\\nn\\no\\n∂Φ(u): = ξ ∈ RN | Φ(v) − Φ(u) ≥ ξ(v − u) ,\\nDRAFT\\n\\n\\x0c5\\n\\nsince we have\\n\\nhξ, u − vi ≥ 0\\n\\n∀ u, v ∈ dom Φ, ∀ξ ∈ ∂Φ(u).\\n\\n(9)\\n\\nit is easy to see that a simple minimisation with respect to the order parameter u, which subsequently determines the constants ci as well (as seen in [8]), leads to\\n\\nh\\n\\n2\\n1\\n∂\\nE(u), Tvi ≥ h−ǫ△u − u + λF(c, I) + ξ, Tvi\\n∂u\\nǫ\\nǫ\\n\\nand hence, by introducing a pseudo-time parametrisation, we have the inclusion\\n\\x12\\n\\x13\\n2\\nhut − ǫ△u + T − u + λF(c, I) , v − ui ∋ h−∂Φ(u), v − ui\\nǫ\\n≥ 0\\n\\n(10)\\n(11)\\n\\n∀u, v ∈ VN . An approximation to this variational inequality can naturally be sought in terms of a\\nfinite element method, as described below.\\nIII. Discretisation and Numerical Solution\\nIt has been shown that iterative solvers such as the Jacobi, Gauss-Seidel, Successive OverRelaxation and multigrid methods can be reduced to a class known as Subspace Correction methods;\\nthese first appeared in Xu [30], see also Xu [31], Kornhuber [16]. Convergence of subspace correction\\nmethods has been examined for example in Tai and Xu [27] for convex optimisation problems and\\nin Neuss [25].\\nIn essence, it is shown that a viable alternative to constantly projecting the solution from one\\nlevel to another is to once and for all project all basis functions from all multigrid levels onto\\nthe coarsest one, solving the problem by iterating through all basis functions. An example of this\\nprojection method is shown in figure 2. This method was successfully applied to the vector-valued\\nAllen-Cahn equation by Kornhuber [17], Kornhuber and Krause [18], [19].\\nIn subsection III-A we introduce the finite element method to establish the notation; building\\nupon that basis, subsection III-B shows the multigrid discretisation used, while subsection III-C\\ndetails a few aspects of the numerical implementation due to the Gibbs Space constraint and the\\ndouble-obstacle method.\\nA. Finite Element Notation\\nThe continuous domain Ω is split into a set of subdomains T , referred to as a triangulation,\\ngiven by the set of triangles τ such that\\n[\\nΩ=\\nτ\\nτ∈T\\n\\nThe natural length scale associated with each triangulation is\\n\\nh: = max diam(τ).\\nτ∈T\\n\\nThe vertices of all triangles form a set of n points or nodes, and for the purposes of this work, each\\ntriangulation is further required not to have any hanging nodes, i.e. nodes that are not corners of\\na triangle. Each node xi ∈ Ω, i = 1 . . . n is assigned a function ηi (x) such that\\n(\\n1 if x = xi ;\\nηi : =\\n0 if x , xi .\\nDRAFT\\n\\n\\x0c6\\n\\nAlthough these functions can be arbitrarily elaborate while still satisfying the specified requirements, continuous piecewise linear functions are more than adequate for second-order problems\\nsuch as ours. The set of all such functions, which can also be written as\\nn\\no\\nS: = η ∈ C(Ω): η|τ is linear ∀τ ∈ T ,\\n(12)\\n\\nforms a basis for the finite element space\\n\\nVh = span {ηi }ni=1\\nsuch that all functions uh ∈ Vh can be represented as\\nn\\nX\\nh\\nu (x) =\\nuhi ηi (x)\\n\\n(13)\\n\\n(14)\\n\\ni=1\\n\\nand such that continuous functions u on the original domain can be represented on the triangulation\\nby their piecewise linear interpolant π: C(Ω) → Vh .\\nWe use the lumped mass and stiffness matrices\\nn\\nX\\nM̂: =\\nM = hηi , 1i, A: = h∇ηi , ∇η j i ∀ηi , η j ∈ Vh .\\nj=1\\n\\nTo discretise the inequality in (11) by the finite element method, we pass to the the weak\\nformulation\\n\\x13\\n\\x12\\n2\\n∀vh ∈ Vh\\n(15)\\nhuht + T − uh + λF(c, I) , vh − uh i − ǫh∇uh , ∇(vh − uh )i ≥ 0\\nǫ\\nso that the discrete solution uh is only required to have H1 regularity. We can then consider\\nthe above problem as a series of sub-problems over each basis function. This is known to be\\nequivalent to an iterative solver, such as a Gauss-Seidel method; the precise sequence in which\\nthese are considered can be altered for instance to follow a red-black Gauss-Seidel pattern. By\\nsimple application of the properties of the projection operator T, using the discretisation method\\noutlined above, we have n problems of the form\\n\\x12\\n\\x12\\n\\x13\\x13\\n2\\nut + T − u + λF(c, I) M̂(v − u) − ǫuA(v − u) ≥ 0.\\n(16)\\nǫ\\nwith some appropriate time discretisation to follow. The inequality is due to the multi-valued nature\\nof the subgradient at the boundaries of GN ; each iteration in the numerical method is performed\\nas though (16) were a strict equality, and if the result lies outside the acceptable space, then it is\\nprojected appropriately, as described in section III-C.\\nB. Multigrid Solution\\nThe rate of convergence of Gauss-Seidel methods requires O(n) iterations per time-step, and\\nO(n2 ) computations overall to converge to a solution, which is less than optimal. Multigrid methods\\nare known to be more efficient, in most cases being of only O(n) complexity overall; they rely on\\nconstructing several nested finite element spaces, usually by refining a coarse or macro triangulation\\nT1 L − 1 times, eventually giving the fine triangulation TL , where\\n\\nTl ⊂ Tl+1 .\\nFigure 1 shows a simple example of one such refinement. Each triangulation is associated with\\nits own finite element function space. Using projection and restriction operators, the solution is\\nDRAFT\\n\\n\\x0c7\\n\\n(a)\\n\\n(b)\\n1\\n\\n1\\n\\n0.9\\n\\n0.9\\n\\n0.8\\n\\n0.8\\n\\n0.7\\n\\n0.7\\n\\n0.6\\n\\n0.6\\n\\n0.5\\n\\n0.5\\n\\n0.4\\n\\n0.4\\n\\n0.3\\n\\n0.3\\n\\n0.2\\n\\n0.2\\n\\n0.1\\n0\\n\\nFig. 1.\\n\\n0.1\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n0\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\na) A coarse grid; b) the same grid, after one refinement.\\n\\ncomputed on one level, usually by a Gauss-Seidel solver or equivalent, and then passed to another\\nlevel to be corrected. The theoretical basis of the multigrid method lies in showing that the error\\nat each iteration can be considered to have several components of varying frequency, and that\\neach time the problem is solved on a grid of natural spacing h, the error components that are of\\nfrequency h or higher are all reduced significantly, while those with a lower frequency are barely\\naffected. The multigrid method significantly improves convergence by using several levels to deal\\nwith many components of the error in every iteration.\\nA recent development in multigrid methods has been to consider iterative solvers as part of the\\nSuccesive Subspace Correction framework, which simply considers a minimisation over a sequence\\nof function spaces such as (13), each defined as the set spanned by the basis functions defined on a\\ndifferent grid. In the context of finite elements, such a hierarchy of function sets is readily provided\\nby the basis functions at each level of refinement. To be more precise, given the problem Pi,j : find\\nu such that\\n\\x12\\n\\x13\\n2\\nhuht + T − uh + λF(c, I) , ηi − uh i − ǫh∇uh , ∇(ηi − uh )i ≥ 0\\nηi ∈ VN,j ,\\nǫ\\nan SSC method applied to this case consists of simply solving all such Pi,j by looping through all\\nnodes i and levels j and updating the solution at each step, in whatever appropriate sequence the\\nspecific method demands.\\nConsider the finite element discretisation in (16); we firstly discretise in time using a backward\\nEuler scheme to obtain the fully discrete problem. At each time step, j, we use the SSC method\\nto efficiently update the approximation uk as follows: starting at the coarsest level and moving to\\nthe finest in a standard multigrid w pattern, for every basis function ηiL , i = 1, 2, ..., nL on level L\\nwe update uk+1 = uk + αiL where αiL satisfies the inequality\\n\\uf8eb k\\n\\x12\\n\\x13\\uf8f6\\uf8f7\\n\\uf8ec\\uf8ec u + αiL η − u j\\n2\\n\\uf8f7\\n+ T − u j + λF(c j , I) \\uf8f7\\uf8f7\\uf8f8 − ǫA(uk + αη) ≥ 0\\nM̂ \\uf8ec\\uf8ec\\uf8ed\\nδt\\nǫ\\nOnce all basis functions and levels have been looped over, the iteration is complete; if the solution\\nsatisfies some prescribed error tolerance, it is accepted and becomes the new iterate u j+1 ; otherwise,\\nthe iteration is repeated using the computed iteration as the new starting point.\\nAll basis functions η of all multigrid levels are projected onto the finest grid. An example of this\\nprojection is given in figure 2. Thus each iteration is of the form\\n\\nDRAFT\\n\\n\\x0c8\\n\\n(a)\\n\\n(b)\\n1\\n\\n1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\n0.4\\n\\n0.4\\n\\n0.2\\n\\n0.2\\n\\n0\\n0\\n\\n0\\n0\\n0.5\\n1\\n\\nFig. 2.\\n\\n1\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0\\n\\n0.5\\n1\\n\\n1\\n\\n0.8\\n\\n0.6\\n\\n0.4\\n\\n0.2\\n\\n0\\n\\na) A basis function defined on a coarse grid; b) the same basis function projected onto a finer grid\\n\\n\\x12\\n\\x13!\\nuk + αη − ut\\n2 t\\nt\\nM̂\\n+ T − u + λF(c , I) − ǫA(uk + αη) = 0\\nδt\\nǫ\\nbefore projection into the required function space. It is enough to multiply both sides by ηT and\\nre-arrange the above by simple algebra to obtain an equation for α:\\n\\x1a \\x14\\n\\x12\\n\\x13\\x15\\n\\x1b\\n2\\nαηT (M̂ − ǫδtA)η = ηT M̂ ut − uk + δtT ut − λF(ct , I) + ǫAuk .\\n(17)\\nǫ\\nC. Gibbs Space Constraint Pseudocode\\nIn the vector-valued case, since N functions need to be updated simultaneously for each basis\\nfunction, it is clear that α is also vector-valued and of size N. In order to preserve the constraint\\nN\\nX\\n\\nui = 1.\\n\\nN\\nX\\n\\nαi = 0.\\n\\ni=1\\n\\nit must be that for each update\\n\\ni=1\\n\\nThis is actually taken care of by the choice of minimisation, i.e. by involving the projection operator\\nT and only allowing search directions u + αTv.\\nSecondly, for the functions to remain in the Gibbs space GN at all times, at each iteration of the\\nnumerical scheme we must ensure that the result still lies in GN , i.e. in addition to the restriction\\non the sum of α above, all components of u must be positive. This requirement may be less than\\nstraightforward to enforce on coarse grids, where several nodes are affected by the change in α;\\nthis is also mentioned in Kornhuber [17] and is due to the large support of the coarse grid basis\\nfunctions after projection onto the finest grid; for that reason, a truncated multigrid algorithm may\\nbe more efficient than a full v-cycle.\\nThe pseudo-code for a general subspace correction is given below; it is assumed that for each\\nbasis function, a list of affected nodes has been drawn so that the least amount of work possible is\\nbeing done.\\n1) Evaluate α as per (17); then, calculate a trial version of u.\\n2) Check for each function ui whether any of its entries have become negative. If not, no further\\nwork is required.\\nDRAFT\\n\\n\\x0c9\\n\\n3) Otherwise, we have a split of the functions ui into two sets, Q being the set of ui with negative\\n(unacceptable) values, and P being the remainder.\\nFor each function in Q, we need to determine a new αi such that ui remains between 0 and 1,\\ni.e. such that the lowest\\nP value is zero. This is easily done and we refer to this correction as β.\\nSince we know that α = 0 must be satisfied, if we decrease αi then we need to increase all\\nother α j , j , i by a corresponding amount. However, we can exclude those functions that are\\nalready negative. Therefore, if there are k functions in P and we decrease αi by an amount\\nβ, then we need to decrease all αk by β/k.\\nDo this for each function in Q.\\n4) All functions in Q are now guaranteed to lie between 0 and 1. However, the successive\\ncorrections β applied to the functions in P may have caused some of those to become negative;\\nhence, repeat from step 2, but only considering a restricted set of functions. Repeat this\\nprocess until there are no more negative functions.\\nSince at least one function is being removed from the set of all available functions at each\\nrepetition of this process, it is a procedure of maximum order N complexity.\\nIV. Implementation\\nA. Determination of Natural Scales\\nIn principle, it is necessary to consider the possibility that the interface between two areas of\\ninterest be only one pixel wide; in other words, that two neighbouring pixels belong to different\\nobjects in the image. It is well known that the interface width of the double obstacle Allen-Cahn\\nequation is of O(ǫ) - see for example [10] and references therein. This immediately suggests a natural\\nlength scale for the problem: an ǫ smaller than the size of a pixel in the given data would not make\\nsense, and a larger ǫ would only blur edges and corners. All our computations have been performed\\nusing this natural length scale.\\nThe value of ǫ also suggests an estimate for a plausible λ; if the value σ = λǫ were much less than\\n1, the effects of the fidelity terms would be negligible; conversely, if it were too much larger than 1,\\nthe interface width would be reduced to such a slim margin that it could no longer be considered\\na diffuse interface. It was found in our computations that a value of roughly σ ∈ [10, 100] gave the\\nmost interesting results.\\nHowever, it must be noted that the numerical discretisation of the Allen-Cahn equation can\\nonly give interface motion results of a satisfactory level of accuracy when there are several nodes\\nto represent each interface, ideally 8 but in practice at least 4. In the context of a multigrid\\nsimulation, this can be achieved quite naturally by taking the given image’s pixel structure as the\\nreference grid and refining it an appropriate number of times - assuming uniform refinement to be\\nthe simplest case, where the number of nodes roughly doubles each time, this implies at least three\\nrefinements. It is also possible to coarsen the regular pixel grid a few times, depending on the size\\nof the image. Thus the coarsest grid in the final setup will probably not correspond to the grid on\\nwhich the data was defined.\\nAnother consequence of the need for a refinement process is the need to represent the image\\ndata on a finer grid than the one it was defined on. In the present model, the only link between\\nthe evolution of the function u to the image to be processed are the fidelity terms of the form\\nZ\\n(I − c)2 dx\\nΩ\\n\\nwith the constants\\n\\nR\\n\\nc = RΩ\\n\\nΩ\\n\\nIu\\nu\\nDRAFT\\n\\n\\x0c10\\n\\napproximating the average of I on supp u. The method of implementation of these terms is therefore\\ncritical to obtaining a good result. Moreover, a decision needs to be made as to what function space\\nthe image I is assumed to belong to. There are several candidates, such as C∞ (Ω) smooth functions,\\nwhich make the mathematical analysis much easier and are generally derived by convolving the\\nimage data with a Gaussian; Lipschitz continuous functions, which include piecewise linear interpolants; and functions of bounded variation. This distinction can be of great practical importance\\nto a finite element method because depending on the chosen approach to node placement and\\nfidelity term implementation it may be necessary to compare the values of u and I at points that\\ndo not correspond to given pixel values, or to points that lie exactly on the boundary between two\\nor more pixels (e.g. corners). Which space the image belongs to ultimately depends on what kind\\nof data one is examining, but in this case we have chosen to consider the image as a set of piecewise\\nconstant values over each pixel; this is done in order not to artificially blur any edges before any\\ncomputations have even taken place.\\nTogether with mesh refinement, it is necessary to project the values of the original pixels to\\nthe fine grid. This is not done by interpolating the data values in any way, in order not to create\\nspurious gradients. This is necessary because the fitting terms in our method rely on calculating\\nthe average value of I in each region; introducing new gradients also introduces small areas of\\ndifferent average value, which may be erroneously identified as new objects. Therefore, every newly\\ncreated node is assigned exactly the same value as one of its neighbours. This preserves sharp\\ndiscontinuities; it also leads to some staircasing of the data, but only on a relatively small scale\\nthat should be locked out by fixing ǫ on the coarse grid.\\nB. Data Projection\\nIf it is indeed necessary to project the values of the original pixels to the fine grid, this should\\nnot be by interpolating the data values in any way, in order not to create spurious gradients. This\\nis necessary because the fitting terms rely on calculating the average value of I in each region;\\nintroducing new gradients also introduces small areas of different average value, which may be\\nerroneously identified as new objects in their own right. Therefore, every newly created node is\\nassigned exactly the same value as one of its neighbours, which preserves sharp discontinuities.\\nIn other words, the image data is taken to represent an underlying function I ∈ BV of bounded\\nvariation.\\nThere are at least two distinct options for the implementation of these terms using a finite element\\nmodel. The first is direct projection of the image data, node by node. The second is to note that\\nthe image function I is only weakly represented in the FE system, in the sense that only the value\\nof its integral over a region is contained in the fidelity terms, and therefore in principle only the\\nvalue of its integral on each element is required; projection can therefore be carried out triangle by\\ntriangle. This makes perfect sense as long as the triangulation is nested. In other words, the fidelity\\nterms can be computed by either of the two following methods: either one uses the standard mass\\nmatrix,\\n\\nMij : = hηi , η j i,\\n!2\\nN\\nX\\nM·I·u\\nMin · In − P\\nF=\\nM·u\\nn=1\\n\\nDRAFT\\n\\n\\x0c11\\n\\nProjection by node\\n\\nProjection by simplex\\n\\nFig. 3.\\n\\nA clarification of two different data projection schemes\\n\\nor one computes the integral of I from first principles:\\n\\nMI =\\n\\nτn\\nN X\\nX\\n\\nI|t hηi |t , ηn |t i;\\n\\nn=1 t=1\\n\\nMI · u\\nF = MI − P\\nM·u\\n\\n!2\\n\\nEach method is associated with its own advantages, disadvantages, and computational costs. It\\nis worth noting that the errors associated with each one decrease with each mesh refinement. The\\nformer can be thought of as projection by node and the latter as projection by simplex ; examples\\nare shown in figure 3.\\nC. Post-processing\\nFig. 4a shows an example of the post-processed solution. The green and blue functions indicate\\nsegmented regions as identified by the program; using these, the average of the data, osc I, is\\nused in each to obtain the denoised data (red). This process is easily achieved by multiplying said\\naverage by the component itself, then summing all the resulting components; this is referred to as\\nthe composite. Recall that the measure of osc I on the support of each component ui is already\\nknown and used actively in the fitting terms driving the evolution (see (5)). Further, because each\\ncomponent has values not identical to 0 or 1, notably at each interface, it is useful to round all\\nvalues to either extremum, in such a way that only one component is equal to 1 and all others\\nare 0 at any given point; in this way, segmented regions are defined more precisely. This naturally\\nleads to the rounded composite, the advantage of which is shown in 4b, a comparison of the errors\\ngiven by the composite and rounded composite.\\n\\nDRAFT\\n\\n\\x0c12\\n\\n(a)\\n\\n(b)\\n\\n1.2\\n\\n1.2\\n\\n1\\n\\n1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\n0.4\\n\\n0.4\\n\\n0.2\\n\\n0.2\\n\\n0\\n\\n−0.2\\n\\n0\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n−0.2\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\nFig. 4. a)A comparison of noisy data (black), segmented regions (green and blue), and resulting denoised signal\\n(red); b) errors for the composite (blue) and rounded composite (green) segmentation.\\n\\n(a)\\n\\n(b)\\n\\n1\\n\\n0.05\\n\\n0.9\\n\\n0.04\\n\\n0.8\\n\\n0.03\\n\\n0.7\\n\\n0.02\\n\\n0.6\\n\\n0.01\\n\\n0.5\\n\\n0\\n\\n0.4\\n\\n−0.01\\n\\n0.3\\n\\n−0.02\\n\\n0.2\\n\\n−0.03\\n\\n0.1\\n0\\n\\nFig. 5.\\n\\n−0.04\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n−0.05\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\na) noisy data (black) and rounded composite (red); b)original noise (blue) vs. recovered noise (green).\\n\\nFigure 5a shows the noisy data (black) and the resulting rounded composite (red); for comparison,\\nFig. 5b shows the original noise (blue) and the recovered noise (green) obtained by subtracting the\\nrounded composite from the initial data.\\nD. Two Dimensional Examples\\nFigure 6 shows the results of our method as applied to an image of concentric circles, with some\\nnoise. Figure 7 shows a more interesting case depicting what appears to be several overlapping\\ngeometric solids superimposed on a chequered background, for comparison with the results in\\nJung, Kang and Shen [14].\\nWe also wish to examine the case of multi-channel data, the most obvious example of which\\nis that of colour images. Although there is no reason why the proposed method could not be\\nextended to images acquired at different wavelengths, for example by combining a visible nighttime photograph with an infrared scan of the same area, the following arguments will concentrate\\non the case of colour images captured in the visible spectrum.\\nIt may seem natural to perform the processing on each channel individually. However, this does\\nnot take into account the fact that the information gathered from each channel is not disjoint but\\nhas a certain, possibly stronger, correlation across channels rather than within the space of each\\nchannel - in other words, the same pixel on each channel most likely corresponds to the same object\\nbeing photographed, whereas there’s no such guarantee for any two adjacent pixels in the same\\n\\nDRAFT\\n\\n\\x0c13\\n\\nImage data\\n\\nRounded composite\\n\\nVectorial constituents\\n\\nRemainder\\n\\nFig. 6. Segmentation of four concentric circles at [.25 .95 .55 .75] over a background at level .1, with added random\\nnoise of amplitude .05. The remainder shows a combination of error and noise, scaled to show all detail\\n\\nchannel.2 Also, the problem remains of how to combine the resulting information, and this is not\\na straightforward course of action in the case of segmentation.\\nSeveral methods to non-trivially process a colour image have previously been examined. The\\nstructure-texture decomposition method suggested by Meyer [23] was recently extended to colour\\nimages by Aujol and Kang [2] applying the G-norm to the RGB space. Another interesting approach\\nis presented for example in Tang, Sapiro and Caselles [28], where the chromaticity values of each\\npixel are mapped onto the unit sphere and then processed using a diffusion-based filter; the authors\\nconsider both isotropic and anisotropic types. A rather similar approach seems to have been used in\\nYu and Bajaj [32], developed seemingly independently. It was argued in Kang and Shen [4] that a\\nchromaticity-brightness (CB) filter outperforms a hue-saturation-value (HSV) filter in combination\\nwith a total variation method.\\nWe propose to use multi-channel information in our model by adapting the fitting terms in (5)\\nto use the information from each of the colour channels I j :\\nR\\nui I j\\nci,j (ui , I j ) = RΩ\\nu\\nΩ i\\n2\\nThis depends on the field of interest. For example, in astronomy it may well be the case that a channel of\\ninformation corresponds to a specific wavelength of light, in which case it is not only true that any one object is\\nhighly unlikely to appear across all wavelenghts, but the same absorption spectrum across different wavelenghts\\nbeing captured at the same pixel most likely indicates two similar objects (e.g. gas clouds), one behind the other.\\n\\nDRAFT\\n\\n\\x0c14\\n\\nImage data\\n\\nRounded composite\\n\\nVectorial constituents\\n\\nFig. 7.\\n\\nSegmentation of a geometric composite\\n\\nF(ci,j , I j ) =\\n\\n3\\nX\\n\\n|I j − ci,j (ui , I j )|2 dx\\n\\nj=1\\n\\nThis method can be followed regardless of how the multi-channel information is encoded, i.e. it can\\nin principle be applied to any colour space, and any number of channels. For simplicity, the RGB\\nspace has been used in computations so far. Figures 8 and 9 were obtained using the RGB space. As\\ncan clearly be seen, the use of the RGB space cannot distinguish between a difference in colour and\\none in luminosity, potentially leading to somewhat unrealistic segmentations of complex real-world\\nimages; this could possibly be remedied by using the L*a*b* space and ignoring the luminosity\\ncomponent.\\nV. Conclusion\\nThe method here presented is a flexible method for image segmentation and denoising based\\non the combination of previously consolidated work. It can either be fine-tuned to specific goals\\nDRAFT\\n\\n\\x0c15\\n\\nFig. 8.\\n\\nImage data\\n\\nRounded composite\\n\\nVectorial constituents\\n\\nRemainder\\n\\nFlowers\\n\\nImage data\\n\\nFig. 9.\\n\\nRounded composite\\n\\nLena\\n\\nor be allowed to run with almost no user input other than the initial image, which can define its\\nown natural scale. Thanks to the multigrid formulation, computational costs will not spiral out of\\ncontrol. The method can also be further extended in several directions, including adaptive mesh\\nrefinement and adaptive time-stepping to further reduce computational time; it can deal with any\\ndesired colour space and any number of input and output channels, independent of each other.\\nThe Esedoḡlu-Tsai formulation involving MBO thresholding has the advantage of dealing well\\nwith the well-known unstable equilibrium value of 1/2; however, we would suggest that there are\\nimportant dynamics motivated by the Chan-Vese fitting terms, especially at short time-scales, that\\nare lost by the thresholding mechanism. Our multigrid solution seeks to retain the computational\\nswiftness while capturing the full dynamics of the equations.\\n\\nDRAFT\\n\\n\\x0c16\\n\\nReferences\\n[1] S. M. Allen and J. W. Cahn. A macroscopic theory for antiphase boundary motion and its applications to\\nantiphase domain coarsening. Acta Metal., 27:1085–1095, 1979.\\n[2] J.-F. Aujol and S. H. Kang. Color image decomposition and restoration. Journal of Visual Communication\\nand Image Representation, 17(4):916–928, August 2006. Previously appeared as UCLA CAM Report 04-73,\\nDecember 2004.\\n[3] M. Beneš, V. Chalupecký, and K. Mikula. Geometrical image segmentation by the Allen-Cahn equation. Applied\\nNumerical Mathematics, 51(2-3):187–205, 2004.\\n[4] T. F. Chan, S.-H. Kang, and J. J. Shen. Total variation denoising and enhancement of color images based on\\nthe CB and HSV color models. Journal of Visual Communication and Image Representation, 12(4):422–435,\\n2001. Previously appeared as Tech. Report UCLA CAM 00-25, June 2001.\\n[5] T. F. Chan, J. J. Shen, and L. Vese. Variational PDE models in image processing. Notices of the American\\nMathematical Society, 50(1):14–26, September 2002. Later appeared as Tech. Report UCLA CAM 02-61,\\nDecember 2002.\\n[6] T. F. Chan and L. Vese. An active contour model without edges. Lecture Notes in Computer Science, 1682:141–\\n151, 1999.\\n[7] T. F. Chan and L. Vese. Image segmentation using level sets and the piecewise-constant Mumford-Shah model.\\nTechnical Report 00-14, UCLA CAM, April 2000. Submitted to IJCV, 2000.\\n[8] T. F. Chan and L. Vese. Active contours without edges. IEEE Transactions on Image Processing, 10(2):266–277,\\n2001. Previously appeared as Tech. Report UCLA CAM Report 98-53, December 1998.\\n[9] T. F. Chan and L. Vese. A level set algorithm for minimizing the Mumford-Shah functional in image processing.\\nIn IEEE/Computer Society Proceedings of the 1st IEEE Workshop on ”Variational and Level Set Methods in\\nComputer Vision”, pages 161–168, 2001.\\n[10] C. M. Elliott. Approximations of curvature dependent interface motion. In I. Duff and G. A. Watson., editors,\\nState of the Art Numerical Analysis, pages 407–440. Clarendon Press, Oxford, 1997. Also appeared as University\\nof Sussex, CMAIA Research Report 96/21.\\n[11] S. Esedoḡlu and Y.-H. R. Tsai. Threshold dynamics for the piecewise constant mumford-shah functional. Journal\\nof Computational Physics, 211(1):367–384, 2006.\\n[12] N. Fusco. An overview of the Mumford-Shah problem. Milan Journal of Mathematics, 71(1):95–119, September\\n2003.\\n[13] H. Garcke, B. Nestler, and B. Stoth. On anisotropic order parameter models for multi-phase systems and their\\nsharp interface limits. Physica D, 115:87–108, 1998.\\n[14] Y. M. Jung, S. H. Kang, and J. Shen. Multiphase image segmentation via Modica-Mortola phase transition.\\nTechnical Report 06-31, UCLA CAM, June 2006.\\n[15] R. Kornhuber. Monotone multigrid methods for elliptic variational inequalities I. Numerische Mathematik,\\n69:167–184, 1994.\\n[16] R. Kornhuber. Nonlinear multigrid techniques. In J. Blowey, J. Coleman, and A. Craig, editors, Theory and\\nNumerics of Differential Equations, pages 179–229. Springer Verlag, 2001.\\n[17] R. Kornhuber. On multigrid methods for vector-valued Allen-Cahn equations with obstacle potential. In\\nI. Herrera, D. E. Keyes, B. Olof, and R. Y. Widlund, editors, Proceedings of the 14th international conference on\\nDomain Decomposition Methods in Science and Engineering, pages 307–314, 2003.\\n[18] R. Kornhuber and R. Krause. On multigrid methods for vector-valued Allen-Cahn equations. In I. H. et al.,\\neditor, Domain Decomposition Methods in Science and Engineering, pages 307–314. UNAM, 2003.\\n[19] R. Kornhuber and R. Krause. Robust multigrid methods for vector-valued Allen-Cahn equations with logarithmic\\nfree energy. Computing and Visualization in Science, 9(2):103–116, 2006.\\n[20] J. Malik and P. Perona. Scale space and edge detection using anisotropic diffusion. Proc. IEEE Computer Soc.\\nWorkshop on Computer Vision, pages 6–22, 1987. Also appeared in IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, Vol. 12 no. 7 July 1990 pg 629–639.\\n[21] B. Merriman, J. K. Bence, and S. J. Osher. Diffusion generated motion by mean curvature. Technical Report\\n92-18, UCLA CAM, April 1992.\\n[22] B. Merriman, J. K. Bence, and S. J. Osher. Motion of multiple junctions: a level set approach. Journal of\\nComputational Physics, 112(2):334–363, 1994.\\n[23] Y. Meyer. Oscillating patterns in image processing and nonlinear evolution equations. In The Fifteenth Dean\\nJacqueline B Lewis Memorial Lectures, number 22 in University Lecture Series. AMS, 2001.\\n[24] D. Mumford and J. Shah. Optimal approximation by piecewise smooth functions and associated variational\\nproblems. Comm. Pure Appl. Math., 42:577–685, 1989.\\n[25] N. Neuss. V-cycle convergence with unsymmetric smoothers and application to an anisotropic model problem.\\nSIAM Journal on Numerical Analysis, 35(3):1201–1212, 1998.\\n\\nDRAFT\\n\\n\\x0c17\\n\\n[26] J. Petitot. An introduction to the Mumford-Shah segmentation model. Journal of Physiology - Paris, 97:335–342,\\n2003.\\n[27] X.-C. Tai and J. Xu. Global and uniform convergence of subspace correction methods for some convex\\noptimization problems. Mathematics of Computation, 71(237):105–124, 2001.\\n[28] B. Tang, G. Sapiro, and V. Caselles. Color image enhancement via chromaticity diffusion. IEEE Transactions\\non Image Processing, 10(5):701–707, May 2001.\\n[29] L. A. Vese. Multiphase object detection and image segmentation. Technical Report 02-36, UCLA CAM, June\\n2002.\\n[30] J. Xu. Iterative methods by space decomposition and subspace correction. SIAM Review, 34(4):581–613, 1992.\\n[31] J. Xu. The method of subspace corrections. Journal of Computational and Applied Mathematics, 128:335–362,\\n2001.\\n[32] Z. Yu and C. Bajaj. Anisotropic vector diffusion in image smoothing. In Proceedings of International Conference\\non Image Processing, pages 828–831, 2002.\\n[33] H.-K. Zhao, T. Chan, B. Merriman, and S. Osher. A variational level set approach to multiphase motion. J.\\nComput. Phys., 127(1):179–195, 1996.\\n\\nDRAFT\\n\\n\\x0c', '1\\n\\nUnequal Error Protection:\\nAn Information Theoretic Perspective\\n\\narXiv:0803.2570v4 [cs.IT] 25 Oct 2009\\n\\nShashi Borade Barış Nakiboğlu Lizhong Zheng\\nEECS, Massachusetts Institute of Technology\\n{ spb , nakib , lizhong } @mit.edu\\nAbstract\\nAn information theoretic framework for unequal error protection is developed in terms of the exponential\\nerror bounds. The fundamental difference between the bit-wise and message-wise unequal error protection (UEP)\\nis demonstrated, for fixed length block codes on DMCs without feedback. Effect of feedback is investigated via\\nvariable length block codes. It is shown that, feedback results in a significant improvement in both bit-wise and\\nmessage-wise UEP (except the single message case for missed detection). The distinction between false-alarm\\nand missed-detection formalizations for message-wise UEP is also considered. All results presented are at rates\\nclose to capacity.\\n\\nI. I NTRODUCTION\\nClassical theoretical framework for communication [35] assumes that all information is equally important.\\nIn this framework, the communication system aims to provide a uniform error protection to all messages: any\\nparticular message being mistaken as any other is viewed to be equally costly. With such uniformity assumptions,\\nreliability of a communication scheme is measured by either the average or the worst case probability of error,\\nover all possible messages to be transmitted. In information theory literature, a communication scheme is said to\\nbe reliable if this error probability can be made small. Communication schemes designed with this framework\\nturn out to be optimal in sending any source over any channel, provided that long enough codes can be employed.\\nThis homogeneous view of information motivates the universal interface of “bits” between any source and any\\nchannel [35], and is often viewed as Shannon’s most significant contribution.\\nIn many communication scenarios, such as wireless networks, interactive systems, and control applications,\\nwhere uniformly good error protection becomes a luxury; providing such a protection to the entire information\\nmight be wasteful, if not infeasible. Instead, it is more efficient here to protect a crucial part of information better\\nthan the rest. For example,\\n• In a wireless network, control signals like channel state, power control, and scheduling information are often\\nmore important than the payload data, and should be protected more carefully. Thus even though the final\\nobjective is delivering the payload data, the physical layer should provide a better protection to such protocol\\ninformation. Similarly for the Internet, packet headers are more important for delivering the packet and need\\nbetter protection to ensure that the actual data gets through.\\n• Another example is transmission of a multiple resolution source code. The coarse resolution needs a better\\nprotection than the fine resolution so that the user at least obtains some crude reconstruction after bad noise\\nrealizations.\\n• Controlling unstable plants over noisy communication link [33] and compressing unstable sources [34]\\nprovide more examples where different parts of information need different reliability.\\nIn contrast with the classical homogeneous view, these examples demonstrate the heterogeneous nature of information. Furthermore the practical need for unequal error protection (UEP) due to this heterogeneity demonstrated\\nin these examples is the reason why we need to go beyond the conventional content-blind information processing.\\nThis research is supported by DARPA ITMANET project and an AFOSR grant FA9550-06-0156. Initial part of this paper was submitted\\nto IEEE International Symposium on Information Theory, 2008.\\n\\n\\x0c2\\n\\nConsider a message set M = {1, 2, 3, . . . , 2k } for a block code. Note that members of this set, i.e. “messages”,\\ncan also be represented by length k strings of information bits, b = [b1 , b2 , . . . bk ]. A block code is composed of\\nan encoder which maps the messages, M ∈ M into channel inputs and a decoder which maps channel outputs to\\ndecoded message, M̂ ∈ M. An error event for a block code is {M̂ 6= M }. In most information theory texts, when\\nan error occurs, the entire bit sequence b is rejected. That is, errors in decoding the message and in decoding\\nthe information bits are treated similarly. We avoid this, and try to figure out what can be achieved by analyzing\\nthe errors of different subsets of bits separately.\\nIn the existing formulations of unequal error protection codes [38] in coding theory, the information bits are\\npartitioned into subsets, and the decoding errors in different subsets of bits are viewed as different kinds of errors.\\nFor example, one might want to provide a better protection to one subset of bits by ensuring that errors in these\\nbits are less probable than the other bits. We call such problems as “bit-wise UEP”. Previous examples of packet\\nheaders, multiple resolution codes, etc. belong to this category of UEP.\\nHowever, in some situations, instead of bits one might want to provide a better protection to a subset of\\nmessages. For example, one might consider embedding a special message in a normal k-bit code, i.e., transmitting\\none of 2k + 1 messages, where the extra message has a special meaning and requires a smaller error probability.\\nNote that the error event for the special message is not associated to error in any particular bit or set of bits.\\nInstead, it corresponds to a particular bit-sequence (i.e. message) being decoded as some other bit-sequence.\\nBorrowing from hypothesis testing, we can define two kinds of errors corresponding to a special message.\\n• Missed-detection of a message i occurs when transmitted message M is i and decoded message M̂ is some\\nother message j 6= i. Consider a special message indicating some system emergency which is too costly to\\nbe missed. Clearly, such special messages demand a small missed detection probability. Missed detection\\nprobability of a message is simply the conditional error probability after its transmission.\\n• False-alarm of a message i occurs when transmitted message M is some other message j 6= i and decoded\\nmessage M̂ is i. Consider the reboot message for a remote-controlled system such as a robot or a satellite\\nor the “disconnect” message to a cell-phone. Its false-alarm could cause unnecessary shutdowns and other\\nsystem troubles. Such special messages demand small false alarm probability.\\nWe call such problems as “message-wise UEP”. In conventional framework, every bit is as important as every\\nother bit and every message is as important as every other message. In short in conventional framework it is\\nassumed that all the information is “created equal”. In such a framework there is no reason to distinguish between\\nbit-wise or message wise error probabilities because message-wise error probability is larger than bit-wise error\\nprobability by an insignificant factor, in terms of exponents. However, in the UEP setting, it is necessary to\\ndifferentiate between message-errors and bit-errors. We will see that in many situations, error probability of\\nspecial bits and messages have behave very differently.\\nThe main contribution of this paper is a set of results, identifying the performance limits and optimal coding\\nstrategies, for a variety of UEP scenarios. We focus on a few simplified notions of UEP, most with immediate\\npractical applications, and try to illustrate the main insights for them. One can imagine using these UEP strategies\\nfor embedding protocol information within the actual data. By eliminating a separate control channel, this can\\nenhance the overall bandwidth and/or energy efficiency.\\nFor conceptual clarity, this article focuses exclusively on situations where the data rate is essentially equal\\nto the channel capacity. These situation can be motivated by the scenarios where data rate is a crucial system\\nresource that can not be compromised. In these situations, no positive error exponent in the conventional sense\\ncan be achieved. That is, if we aim to protect the entire information uniformly well, neither bit-wise nor messagewise error probabilities can decay exponentially fast with increasing code length. We ask the question then “can\\nwe make the error probability of a particular bit, or a particular message, decay exponentially fast with block\\nlength?”\\nWhen we break away from the conventional framework and start to provide better protection to against certain\\nkinds of errors, there is no reason to restrict ourselves by assuming that those errors are erroneous decoding of\\nsome particular bits or missed detections or false alarms associated with some particular messages. A general\\n\\n\\x0c3\\n\\nformulation of UEP could be an arbitrary combination of protection demands against some specific kinds of\\nerrors. In this general definition of UEP, bit-wise UEP and message-wise UEP are simply two particular ways\\nof specifying which kinds of errors are too costly compared to others.\\nIn the following, we start by specifying the channel model and giving some basic definitions in Section II.\\nThen in section III we discuss bit-wise UEP and message-wise UEP for block codes without feedback. Theorem\\n1 shows that for data-rates approaching capacity, even a single bit cannot achieve any positive error exponent.\\nThus in bit-wise UEP, the data-rate must back off from capacity for achieving any positive error exponent even\\nfor a single bit. On the contrary, in message-wise UEP, positive error exponents can be achieved even at capacity.\\nWe first consider the case when there is only one special message and show that, Theorem 2, optimal (misseddetection) error exponent for the special message is equal to the red-alert exponent, which is defined in section\\nIII-B. We then consider situations where an exponentially large number of messages are special and each special\\nmessage demands a positive (missed detection) error exponent. (This situation has previously been analyzed\\nbefore in [12], and a result closely related to our has been reported there.) Theorem 3 shows a surprising result\\nthat these special messages can achieve the same exponent as if all the other (non-special) messages were absent.\\nIn other words, a capacity achieving code and an error exponent-optimal code below capacity can coexist without\\nhurting each other. These results also shed some new light on the structure of capacity achieving codes.\\nInsights from the block codes without feedback becomes useful in Section IV where we investigate similar\\nproblems for variable length block codes with feedback. Feedback together with variable decoding time creates\\nsome fundamental connections between bit-wise UEP and message-wise UEP. Now even for bit-wise UEP, a\\npositive error exponent can be achieved at capacity. Theorem 5 shows that a single special bit can achieve the\\nsame exponent as a single special message—the red-alert exponent. As the number of special bits increases, the\\nachievable exponent for them decays linearly with their rate as shown in Theorem 6. Then Theorem 7 generalizes\\nthis result to the case when there are multiple levels of specialty—most special, second-most special and so on.\\nIt uses a strategy similar to onion-peeling and achieves error exponents which are successively refinable over\\nmultiple layers. For single special message case, however, Theorem 8 shows that feedback does not improve\\nthe optimal missed detection exponent. The case of exponentially many messages is resolved in Theorem 9.\\nEvidently many special messages cannot achieve an exponent higher than that of a single special message, i.e.\\nred-alert exponent. However it turns out that the special messages can reach red-alert exponent at rates below\\na certain threshold, as if all the other special messages were absent. Furthermore for the rates above the very\\nsame threshold, special messages reach the corresponding value of Burnashev’s exponent, as if all the ordinary\\nmessages were absent.\\nSection V then addresses message-wise UEP situations where special messages demand small probability of\\nfalse-alarms instead of missed-detections. It considers the case of fixed length block codes with out feedback\\nas well as variable length block codes with feedback. This discussion for false-alarms was postponed from\\nearlier sections to avoid confusion with the missed-detection results in earlier sections. Some future directions\\nare discussed briefly in Section VI.\\nAfter discussing each theorem, we will provide a brief description of the optimal strategy, but refrain from\\ndetailed technical discussions. Proofs can be found in later sections. In section VII and section VIII we will\\npresent the proofs of the results in Section III, on block codes without feedback, and Section IV, on variable\\nlength block codes with feedback, respectively. Lastly in Section IX we discuss the proofs for the false-alarm\\nresults of Section V. Before going into the presentation of our work let us give a very brief overview of the\\nprevious work on the problem, in different fields.\\nA. Previous Work and Contribution\\nThe simplest method of unequal error protection is to allocate different channels for different types of data.\\nFor example, many wireless systems allocate a separate “control channel”, often with short codes with low\\nrate and low spectral efficiency, to transmit control signals with high reliability. The well known Gray code,\\nassigning similar bit strings to close by constellation points, can be viewed as UEP: even if there is some error\\n\\n\\x0c4\\n\\nin identifying the transmitted symbol, there is a good chance that some of the bits are correctly received. But\\nclearly this approach is far from addressing the problem in any effective way.\\nThe first systematic consideration of problem in coding theory was within the frame work of linear codes. In\\n[24], Masnick and Wolf suggested techniques which protects different parts (bits) of the message against different\\nnumber of channel errors (channel symbol conversions). This frame work has extensively studied over the years\\nin [22], [16], [7], [26], [21], [27], [8] and in many others. Later issue is addressed within frame work of Low\\nDensity Parity Check (LDPC) codes too [39], [29], [30], [32], [31], and [28].\\n“Priority encoded transmission” (PET) was suggested by Albenese et.al. [2] as an alternative model of the\\nproblem, with packet erasures. In this approach guarantees are given not in terms of channel errors but packet\\nerasures. Coding and modulation issues are addressed simultaneously in [10]. For wireless channels, [15] analyzes\\nthis problem in terms of diversity-multiplexing trade-offs.\\nIn contrast with above mentioned work, we pose and address the problem within the information theoretic\\nframe work. We work with the error probabilities and refrain from making assumptions about the particular\\nblock code used while proving our converse results. This is the main difference between our approach and the\\nprevailing approach within the coding theory community.\\nIn [3], Bassalygo et. al. considered the error correcting codes whose messages are composed of two group\\nof bits, each of which requires different level of protection against channel errors and provided inner and outer\\nbounds to the achievable performance, in terms of hamming distances and rates. Unlike other works within\\ncoding theory frame work, they do not make any assumption about the code. Thus their results can indeed be\\nreinterpreted in our framework as a result for bit wise UEP, on binary symmetric channels.\\nSome of the the UEP problems have already been investigated within the framework of information theory\\ntoo. Csiszár studied message wise UEP with many messages in [12]. Moreover results in [12] are not restricted\\nto the rates close to capacity, like ours. Also messages wise UEP with single special message was dealt with in\\n[23] by Kudryashov. In [23], an UEP code with single special message is used as a subcode within a variable\\ndelay communication scheme. The scheme proposed in [23] for the single special message case is a key building\\nblock in many of the results in section IV. However the optimality of the scheme was not proved in [23]. We\\nshow that it is indeed optimal.\\nThe main contribution of the current work is the proposed frame work for UEP problems within information\\ntheory. In addition to the particular results presented on different problems and the contrasts demonstrated between\\ndifferent scenarios, we believe the proof techniques used in subsections1 VII-A, VIII-B.2 and VIII-D.2 are novel\\nand they are promising for the future work in the field.\\nII. C HANNEL M ODEL\\n\\nAND\\n\\nN OTATION\\n\\nA. DMC’s and Block Codes\\nWe consider a discrete memoryless channel (DMC) WY |X , with input alphabet X = {1, 2, . . . , |X |} and output\\nalphabet Y = {1, 2, . . . , |Y|}. The conditional distribution of output letter Y when the channel input letter X\\nequals i ∈ X is denoted by WY |X (·|i).\\nPr [Y = j| X = i] = WY |X (j|i)\\n\\n∀i ∈ X , ∀j ∈ Y.\\n\\nWe assume that all the entries of the channel transition matrix are positive, that is, every output letter is reachable\\nfrom every input letter. This assumption is indeed a crucial one. Many of the results we present in this paper\\nchange when there are zero-probability transitions.\\nA length n block code without feedback with message set M = {1, 2, . . . , |M|} is composed of two mappings,\\nencoder mapping and decoder mapping. Encoder mapping assigns a length n codeword,2\\n∆\\n\\nx̄n (k) = (x̄1 (k), x̄2 (k) · · · , x̄n (k))\\n1\\n\\n∀k ∈ M\\n\\nThe key idea in subsection VIII-B.2 is a generalization of the approach presented in [4].\\nUnless mentioned otherwise, small letters (e.g. x) denote a particular value of the corresponding random variable denoted in capital\\nletters (e.g. X).\\n2\\n\\n\\x0c5\\n\\nwhere x̄t (k) denotes the input at time t for message k. Decoder mapping, M̂ , assigns a message to each possible\\nchannel output sequence, i.e. M̂ : Y n → M.\\nAt time zero, the transmitter is given the message M , which is chosen from M according to a uniform\\ndistribution. In the following n time units, it sends the corresponding codeword. After observing Y n , receiver\\ndecodes a message. The error probability Pe and rate R of the code is given by\\nh\\ni\\nPe , Pr M̂ 6= M\\nand\\nR , ln |M|\\nn .\\nB. Different Kinds of Errors\\nWhile discussing message-wise UEP, we consider the conditional error probability for a particular message\\ni ∈ M,\\n\\x0c\\nh\\ni\\n\\x0c\\nPr M̂ 6= i\\x0c M = i .\\n\\nRecall that this is the same as the missed detection probability for message i.\\nOn the other hand when we are talking about bit-wise UEP, we consider message sets that are of the form\\nM = M1 × M2 . In such cases message M is composed of two submessages, M = (M1 , M2 ). First submessage\\nM1 corresponds to the high-priority bits while second submessage M2 corresponds to the low-priority bits. The\\nuniform choice of M from M, implies the uniform and independent choice of M1 and M2 from M1 and M2\\nrespectively. Error probability of a submessage Mj is given by\\ni\\nh\\nj = 1, 2\\nPr M̂j 6= Mj\\n\\nNote that the overall message M is decoded incorrectly when\\nh either M\\ni 1 or M2 or both are decoded incorrectly.\\nThe goal of bit-wise UEP is to achieve best possible Pr M̂1 6= M1 while ensuring a reasonably small Pe =\\nh\\ni\\nPr M̂ 6= M .\\nC. Reliable Code Sequences\\n\\nWe focus on systems where reliable communication is achieved in order to find exponentially tight bounds for\\nerror probabilities of special parts of information. We use the notion of code-sequences to simplify our discussion.\\nA sequence of codes indexed by their block-lengths is called reliable if\\nlim Pe (n) = 0\\n\\nn→∞\\n\\nFor any reliable code-sequence Q, the rate RQ is given by\\nRQ , lim inf\\nn→∞\\n\\nln |M(n) |\\nn\\n\\nThe (conventional) error exponent of a reliable sequence is then\\nEQ , lim inf\\nn→∞\\n\\n− ln Pe (n)\\nn\\n\\n.\\nThus the number of messages in Q is3 = enRQ and their average error probability decays like e−nEQ with block\\nlength. Now we can define error exponent E(R) in the conventional sense, which is equivalent to the ones given\\nin [20], [36], [13], [17], [25].\\n3\\n\\n.\\nThe = sign denotes equality in the exponential sense. For a sequence a(n) ,\\nln a(n)\\n.\\na(n) = enF ⇔ F = lim inf\\nn→∞\\nn\\n\\n\\x0c6\\n\\nDefinition 1: For any R ≤ C the error exponent E(R) is defined as\\n∆\\n\\nE(R) =\\n\\nsup EQ\\n\\nQ:RQ ≥R\\n\\nAs mentioned previously, we are interested in UEP when operating at capacity. We already know, [36], that\\nE(C) = 0, i.e. the overall error probability cannot decay exponentially at capacity. In the following sections,\\nwe show how certain parts of information can still achieve a positive exponent at capacity. In doing that, we\\nare focusing only on the reliable sequences whose rates are equal to C. We call such reliable code sequences as\\ncapacity-achieving sequences.\\nThrough out the text we denote Kullback-Leibler (KL) divergence between two distributions αX (·) and βX (·)\\nas D (αX (·)k βX (·)).\\nX\\n(i)\\nD (αX (·)k βX (·)) =\\nαX (i) ln αβXX (i)\\ni∈X\\n\\nSimilarly conditional KL divergence between VY |X (·|·) and WY |X (·|·) under PX (·) is given by\\nX\\n\\n\\x0c\\n\\x01 X\\nV\\n(j|i)\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX =\\nPX (i)\\nVY |X (j|i) ln WYY|X\\n|X (j|i)\\ni∈X\\n\\nj∈Y\\n\\nThe output distribution that achieves the capacity is denoted by PY∗ and a corresponding input distribution is\\ndenoted by PX∗ .\\nIII. UEP\\n\\nAT\\n\\nC APACITY: B LOCK C ODES\\n\\nWITHOUT\\n\\nF EEDBACK\\n\\nA. Special bit\\nWe first address the situation where one particular bit (say the first) out of the total log2 |M| bits is a special\\nbit—it needs a much better error protection than the overall information. The error probability of the special bit is\\nrequired to decay as fast as possible while ensuring reliable communication at capacity, for the overall code. The\\nsingle special bit is denoted by M1 where M1 = {0, 1} and over all message M is of the form M = (M1 , M2 )\\nwhere M = M1 × M2 . The optimal error exponent Eb for the special bit is then defined as follows4 .\\n(n)\\nDefinition 2: For a capacity-achieving sequence Q with message sets M(n) = M1 ×M2 where M1 = {0, 1},\\nthe special bit error exponent is defined as\\nEb ,Q , lim inf\\nn→∞\\n\\n− ln Pr\\n\\n(n)\\n\\n[M̂1 6=M1 ]\\n\\nn\\n\\nThen Eb is defined\\nh as Eb ,\\ni supQ Eb ,Q .\\n.\\n(n)\\nThus if Pr\\nM̂1 6= M1 = exp(−nEb ,Q ) for a reliable sequence Q, then Eb is the supremum of Eb ,Q over\\nall capacity-achieving Q’s.\\nSince E(C) = 0, it is clear that the entire information cannot achieve any positive error exponent at capacity.\\nHowever, it is not clear whether a single special bit can steal a positive error exponent Eb at capacity.\\nTheorem 1:\\nEb = 0\\nThis implies that, if we want the error probability of the messages to vanish with increasing block length and\\nthe error probability of at least one of the bits to decay with a positive exponent with block length, the rate of\\nthe code sequence should be strictly smaller than the capacity.\\nProof of the theorem is heavy in calculations, but the main idea behind is the “blowing up lemma” [13].\\nConventionally, this lemma is only used for strong converses for various capacity theorems. It is also worth\\nmentioning that the conventional converse techniques like Fano’s inequality are not sufficient to prove this result.\\n4\\nAppendix A discusses a different but equivalent type of definition and shows why it is equivalent to this one. These two types of\\ndefinitions are equivalent for all the UEP exponents discussed in this paper.\\n\\n\\x0c7\\n\\nM̂ = 0\\n\\nM̂ = 1\\n\\nFig. 1.\\n\\nSplitting the output space into 2 distant enough clusters.\\n\\nIntuitive interpretation: Let the shaded balls in Fig. 1 denote the minimal decoding regions of the messages.\\nThese decoding regions ensure reliable communication, they are essentially the typical noise-balls ([11]) around\\ncodewords. The decoding regions on the left of the thick line corresponds to M̂1 = 1 and those on the right\\ncorrespond to the same when M̂1 = 0. Each of these halves includes half of the decoding regions. Intuitively, the\\nblowing up lemma implies that if we try to add slight extra thickness to the left clusters in Figure 1, it blows up\\nto occupy almost all the output space. This strange phenomenon in high dimensional spaces leaves no room for\\nthe right cluster to fit. Infeasibility of adding even slight extra thickness implies zero error exponent the special bit.\\n\\nB. Special message\\n.\\nNow consider situations where one particular message (say M = 1) out of the = enC total messages is a special\\nmessage—it needs a superior error protection. The missed detection probability for this ‘emergency’ message\\nneeds to be minimized. The best missed detection exponent Emd is defined as follows.5\\nDefinition 3: For a capacity-achieving sequence Q, missed detection exponent is defined as\\nEmd ,Q , lim inf\\nn→∞\\n\\n− ln Pr\\n\\n(n)\\n\\n[M̂ 6=1|M =1]\\nn\\n\\n.\\n\\nThen Emd is defined as Emd , supQ Emd ,Q .\\nCompare this with the situation where we aim to protect all the messages uniformly well. If all the messages\\ndemand equally good missed detection exponent, then no positive exponent is achievable at capacity. This follows\\nfrom the earlier discussion about E(C) = 0. Below theorem shows the improvement in this exponent if we only\\ndemand it for a single message instead of all.\\nDefinition 4: The parameter C̃ is defined6 as the red-alert exponent of a channel.\\n\\x01\\nC̃ , max D PY∗ (·)k WY |X (·|i)\\ni∈X\\n\\nWe will denote the input letter achieving above maximum by xr .\\nTheorem 2:\\nEmd = C̃.\\n\\n˛\\n˛\\nh\\ni\\nh\\ni\\n˛\\n˛\\nNote that the definition obtained by replacing Pr (n) M̂ 6= 1˛ M = 1 by minj Pr (k) M̂ 6= j ˛ M = j is equivalent to the one given\\nabove, since we are taking the supremum over Q anyway. In short, the message j with smallest conditional error probability could always\\nbe relabeled as message 1.\\n6\\nAuthors would like to thank Krishnan Eswaran of UC Berkeley for suggesting this name.\\n5\\n\\n\\x0c8\\n\\nRecall that Karush-Kuhn-Tucker (KKT) conditions for achieving capacity imply the following expression for\\ncapacity, [20, Theorem 4.5.1].\\n\\n\\x01\\nC = max D WY |X (·|i)\\n PY∗ (·)\\ni∈X\\n\\nNote that simply switching the arguments of KL divergence within the maximization for C, gives us the expression\\nfor C̃. The capacity C represents the best possible data-rate over a channel, whereas red-alert exponent C̃ represents\\nthe best possible protection achievable for a message at capacity.\\nIt is worth mentioning here the “very noisy” \\x01channel in [20]. In\\n[6], the KL divergence\\n\\n this formulation\\n\\x01\\nis symmetric, which implies D PY∗ (·)k WY |X (·|i) ≈ D WY |X (·|i)\\n PY∗ (·) . Hence the red-alert exponent and\\ncapacity become roughly equal. For a symmetric channel like BSC, all\\x01inputs can be used as xr . Since the PY∗ is\\nthe uniform distribution for these channels, C̃ = D PY∗ (·)k WY |X (·|i) for any input letter i. This also happens\\nto be the sphere-packing exponent Esp (0) of this channel [36] at rate 0.\\nOptimal strategy: Codewords of a capacity achieving code are used for the ordinary messages. Codeword for\\nthe special message is a repetition sequence of the input letter xr . For all the output sequences special message\\nis decoded, except for the output sequences with empirical distribution (type) approximately equal to PY∗ . For\\nthe output sequences with empirical distribution approximately PY∗ , the decoding scheme of the original capacity\\nachieving code is used.\\nIndeed Kudryashov [23] had already suggested the encoding scheme described above, as a subcode for his nonblock variable delay coding scheme. However discussion in [23] does not make any claims about the optimality\\nof this encoding scheme.\\nIntuitive interpretation: Having a large missed detection exponent for the special message corresponds to having\\na large decoding region for the special message. This ensures that when M = 1, i.e. when the special message\\nis transmitted, probability of M̂ 6= 1 is exponentially small. In a sense Emd indicates how large the decoding\\n.\\nregion of the special message could be made, while still filling = enC typical noise balls in the remaining space.\\nThe red region in Fig. 2 denotes such a large region. Note that the actual decoding region of the special message\\nis much larger than this illustration, because it consists of all output types except the ones close to PY∗ , whereas\\nthe ordinary decoding regions only contain the output types close to PY∗ .\\n\\nFig. 2.\\n\\nAvoiding missed-detection\\n\\nUtility of this result is two folds: first, the optimality of such a simple scheme was not obvious before; second,\\nas we will see later protecting a single special message is a key building block for many other problems when\\nfeedback is available.\\n\\n\\x0c9\\n\\nC. Many special messages\\n.\\nNow consider the case when instead of a single special message, exponentially many of the total = enC\\n(n)\\nmessages are special. Let Ms ⊆ M(n) denote this set of special messages,\\nnr\\nM(n)\\ns = {1, 2, · · · , ⌈e ⌉}.\\n\\nThe best missed detection exponent, achievable simultaneously for all of the special messages, is denoted by\\nEmd (r).\\nDefinition 5: For a capacity-achieving sequence Q, the missed detection exponent achieved on sequence of\\nsubsets Ms is defined as\\n− ln max Pr (n) [M̂ 6=i|M =i]\\ni∈Ms(n)\\n.\\nEmd ,Q,Ms , lim inf\\nn\\nn→∞\\n\\nThen for a given r < C , Emd (r) is defined as, Emd (r) , supQ,Ms Emd ,Q,Ms where maximization is over\\n(n)\\nln |Ms |\\n= r.\\nMs ’s such that lim inf\\nn→∞\\nn\\nThis message wise UEP problem has already been investigated by Csiszár in his paper on joint source-channel\\ncoding [12]. His analysis allows for multiple sets of special messages each with its own rate and an overall rate\\nthat can be smaller than the capacity.7\\n.\\nEssentially, Emd (r) is the best value for which missed detection probability of every special message is =\\nexp(−nEmd (r)) or smaller. Note that if the only messages in the code are these ⌈enr ⌉ special messages (instead\\n.\\nof |M(n) | = enC total messages), their best missed detection exponent equals the classical error exponent E(r)\\ndiscussed earlier.\\nTheorem 3:\\nEmd (r) = E(r) ∀ r ∈ [0, C).\\nThus we can communicate reliably at capacity and still protect the special messages as if we are only\\ncommunicating the special messages. Note that the classical error exponent E(r) is yet unknown for the rates\\nbelow critical rate (except zero rate). Nonetheless, this theorem says that whatever E(r) can be achieved for ⌈enr ⌉\\n.\\nmessages when they are by themselves in the codebook, can still be achieved when there are = enC additional\\nordinary messages requiring reliable communication.\\nOptimal strategy: Start with an optimal code-book for ⌈enr ⌉ messages which achieves the error exponent E(r).\\nThese codewords are used for the special messages. Now the ordinary codewords are added using random coding.\\nThe ordinary codewords which land close to a special codeword may be discarded without essentially any effect\\non the rate of communication.\\nDecoder uses a two-stage decoding rule, in first stage of which it decides whether or not a special message was\\nsent. If the received sequence is close to one or more of the special codewords, receiver decides that a special\\nmessage was sent else it decides an ordinary message was sent. In the second stage, receiver employs an ML\\ndecoding either among the ordinary messages or the among the special messages depending on its decision in\\nthe first stage.\\nThe overall missed detection exponent Emd (r) is bottle-necked by the second stage errors. It is because the first\\nstage error exponent is essentially the sphere-packing exponent Esp (r), which is never smaller than the second\\nstage error exponent E(r).\\nIntuitive interpretation: This means that we can start with a code of ⌈enr ⌉ messages, where the decoding\\nregions are large enough to provide a missed detection exponent of E(r). Consider the balls around each codeword\\nwith sphere-packing radius (see Fig. 3(a)). For each message, the probability of going outside its ball decays\\nexponentially with the sphere-packing exponent. Although, these ⌈enr ⌉ balls fill up most of the output space,\\n7\\n\\nAuthors would like to thank Pulkit Grover of UC Berkeley for pointing out this closely related work, [12]\\n\\n\\x0c10\\n\\n.\\nthere are still some cavities left between them. These small cavities can still accommodate = enC typical noise\\nballs for the ordinary messages (see Fig. 3(b)), which are much smaller than the original ⌈enr ⌉ balls. This is\\nanalogous to filling sand particles in a box full of large boulders. This theorem is like saying that the number of\\nsand particles remains unaffected (in terms of the exponent) in spite of the large boulders.\\n\\n(a) Exponent optimal code\\nFig. 3.\\n\\n(b) Achieving capacity\\n\\n“There is always room for capacity”\\n\\nD. Allowing erasures\\nIn some situations, a decoder may be allowed declare an erasure when it is not sure about the transmitted\\nmessage. These erasure events are not counted as errors and are usually followed by a retransmission using a\\ndecision feedback protocol like Hybrid-ARQ. This subsection extends the earlier result for Emd (r) to the cases\\nwhen such erasures are allowed.\\nIn decoding with erasures, in addition to the message set M, the decoder can map the received sequence Y n\\nto a virtual message called “erasure”. Let Perasure denote the average erasure probability of a code.\\nh\\ni\\nPerasure = Pr M̂ = erasure\\n\\nPreviously when there was no erasures, errors were not detected. For errors and erasures decoding, erasures are\\ndetected errors, the rest of the errors are undetected errors and Pe denotes the undetected error probability. Thus\\naverage and conditional (undetected) error probabilities are given by\\n\\x0c\\nh\\ni\\nh\\ni\\n\\x0c\\nPe = Pr M̂ 6= M , M̂ 6= erasure\\nand Pe (i) = Pr M̂ 6= M , M̂ 6= erasure\\x0c M = i\\n\\nAn infinite sequence Q of block codes with errors and erasures decoding is reliable, if its average error probability\\nand average erasure probability, both vanish with n.\\nlim Pe (n) = 0\\n\\nn→∞\\n\\nand\\n\\nlim Perasure (n) = 0\\n\\nn→∞\\n\\nIf the erasure probability is small, then average number of retransmissions needed is also small. Hence this\\ncondition of vanishingly small Perasure (n) ensures that the effective data-rate of a decision feedback protocol\\nremains unchanged in spite of retransmissions. We again restrict ourselves to reliable sequences whose rate equal\\nC.\\nWe could redefine all previous exponents for decision-feedback (df) scenarios, i.e. for reliable codes with erasure\\ndecoding. But resulting exponents do not change with the provision of erasures with vanishing probability for\\nsingle bit or single message problems, i.e. decision feedback protocols such as Hybrid-ARQ does not improve\\nEb or Emd . Thus we only discuss the decision feedback version of Emd (r).\\n\\n\\x0c11\\n\\nDefinition 6: For a capacity-achieving sequence with erasures, Q, the missed detection exponent achieved on\\nsequence of subsets Ms is defined as\\ndf\\nEmd\\n,Q (r)\\n\\n, lim inf\\nn→∞\\n\\n− ln max\\n\\ni∈Ms(n)\\n\\nPr\\n\\n(n)\\n\\n[M̂ 6=i,M̂ 6=erasure|M =i]\\nn\\n\\n.\\n\\ndf\\ndf\\nThen for a given r < C , Emd\\n,Q (r) is defined as, Emd ,Q (r) , supQ,Ms Emd ,Q,Ms where maximization is over\\n(n)\\n\\nln |Ms |\\n= r.\\nMs ’s such that lim inf\\nn→∞\\nn\\nNext theorem shows allowing erasures increases the missed-detection exponent for r below critical rate, on\\nsymmetric channels.\\nTheorem 4: For symmetric channels\\ndf\\nEmd\\n(r) ≥ Esp (r) ∀ r ∈ [0, C).\\n\\nCoding strategy is similar to the no-erasure case. We first start with an erasure code for ⌈enr ⌉ messages like\\nthe one in [18]. Then add randomly generated ordinary codewords to it. Again a two-stage decoding is performed\\nwhere the first stage decides between the set of ordinary codewords and the set of special codewords using a\\nthreshold distance. If this first stage chooses special codewords, the second stage applies the decoding rule in\\n[18] amongst special codewords. Otherwise, the second stage uses the ML decoding among ordinary codewords.\\ndf\\nThe overall missed detection exponent Emd\\n(r) is bottle-necked by the first stage errors. It is because the firststage error exponent Esp (r) is smaller than the second stage error exponent Esp (r) + C − r . This is in contrast\\nwith the case without erasures.\\nIV. UEP\\n\\nAT\\n\\nC APACITY: VARIABLE L ENGTH B LOCK C ODES\\n\\nWITH\\n\\nF EEDBACK\\n\\nIn the last section, we analyzed bit wise and message wise UEP problems for fixed length block codes\\n(without feedback) operating at capacity. In this section, we will revisit the same problems for variable length\\nblock codes with perfect feedback, operating at capacity. Before going into the discussion of the problems, let\\nus recall variable length block codes with feedback briefly.\\nA variable length block code with feedback, is composed of a coding algorithm and a decoding rule. Decoding\\nrule determines the decoding time and the message that is decoded then. Possible observations of the receiver\\ncan be seen as leaves of |Y|-ary tree, as in [4]. In this tree, all nodes at length 1 from the root denote all |Y|\\npossible outputs at time t = 1. All non-leaf nodes among these split into further |Y| branches in the next time\\nt = 2 and the branching of the non-leaf nodes continue like this ever after. Each node of depth t in this tree\\ncorresponds to a particular sequence, y t , i.e. a history of outputs until time t. The parent of node y t is its prefix\\ny t−1 . Leaves of this tree form a prefix free source code, because decision to stop for decoding has to be a casual\\nevent. In other words the event {τ = t} should be measurable in the σ -field generated by Y t . In addition we\\nhave Pr [τ < ∞] = 1 thus decoding time τ is Markov stopping time with respect to receivers observation. The\\ncoding algorithm on the other hand assigns an input letter, Xt+1 (y t ; i), to each message, i ∈ M, at each non-leaf\\nnode, y t , of this tree. The encoder stops transmission of a message when a leaf is reached i.e. when the decoding\\nis complete.\\nCodes we consider are block codes in the sense that transmission of each message (packet) starts only after\\nthe transmission of the previous one ends. The error probability and rate of the code are simply given by\\nh\\ni\\nM\\nPe = Pr M̂ 6= M\\nand,\\nR = ln\\nE[τ ]\\n\\nA more thorough discussion of variable length block codes with feedback can be found in [9] and [4].\\n\\n\\x0c12\\n\\nEarlier discussion in Section II-B about different kinds of errors is still valid as is but we need to slightly\\nmodify our discussion about the reliable sequences. A reliable sequence of variable length block codes with\\nfeedback, Q, is any countably infinite collection of codes indexed by integers, such that\\nlim Pe (k) = 0\\n\\nk→∞\\n\\nIn the rate and exponent definitions for reliable sequences, we replace block-length n by the expected decoding\\ntime E [τ ]. Then a capacity achieving sequence with feedback is a reliable sequence of variable length block\\ncodes with feedback whose rate is C\\nIt is worth noting the importance of our assumption that all the entries of the transition probability matrix,\\nWY |X are positive. For any channel with a WY |X which has one or more zero probability transitions, it is possible\\nto have error free codes operating at capacity, [9]. Thus all the exponents discussed below are infinite for DMCs\\nwith one or more zero probability transitions.\\nA. Special bit\\n(k)\\n\\nLet us consider a capacity achieving sequence Q whose message sets are of the form M(k) = M1 × M2\\nwhere M1 = {0, 1}. Then the error exponent of the M1 , i.e., the initial bit, is defined as follows.\\nDefinition 7: For a capacity achieving sequence with feedback, Q, with message sets M(k) of the form\\n(k)\\nM(k) = M1 × M2 where M1 = {0, 1}, the special bit error exponent is defined as\\n− ln Pr (n) [M̂1 6=M1 ]\\nE[τ (k) ]\\nk→∞\\n\\nEbf ,Q , lim inf\\n\\nThen Ebf is defined as Ebf , supQ Ebf ,Q\\nTheorem 5:\\nEbf = C̃.\\nRecall that without feedback, even a single bit could not achieve any positive error exponent at capacity, Theorem\\n1. But feedback together with variable decoding time connects the message wise UEP and the bit wise UEP and\\nresults in a positive exponent for bit wise UEP. Below described strategy show how schemes for protecting a\\nspecial message can be used to protect a special bit.\\n√\\nOptimal strategy: We use a length (k + k) fixed length block code with errors and erasures decoding\\nas a\\n√\\nbuilding block for our code. Transmitter first transmits M1 using a short repetition code of length k. If the\\ntentative decision about M1 , M̃1, is correct after this repetition code, transmitter sends M2 with a length k capacity\\nachieving code. If M̃1 is incorrect after the repetition code, transmitter sends the symbol xr for k time units where\\n√\\n\\x01\\nxr is the input letter i maximizing the D PY∗ (·)k WY |X (·|i) . If the output sequence in the second phase, Y√ k+k ,\\nk+1\\nis not a typical sequence of PY∗ , an erasure is declared for the block. And the same message is retransmitted by\\nrepeating the same strategy afresh. Else receiver uses an ML decoder to chose M̂2 and M̂ = (M̃1 , M̂2 ).\\nThe erasure probability is vanishingly small, as a result the undetected error probability of Mi in fixed length\\nerasure code is approximately\\nequal to the error probability of Mi in the variable length block code. Furthermore\\n√\\nE [τ ] is roughly (k + k) despite the retransmissions. A decoding error for M1 happens only when M̃1 6= M1\\nand the empirical distribution of the output sequence in the second phase is close to PY∗ . Note that latter event\\n.\\nhappens with probability = e−C̃E[τ ] .\\n\\nB. Many special bits\\nWe now analyze the situation where instead of a single special bit, there are approximately E [τ ] r/ ln 2 special\\nbits out of the total E [τ ] C/ ln 2 (approx.) bits. Hence we consider the capacity achieving sequences with feedback\\n(k)\\n(k)\\n(k)\\nhaving message sets of the form M(k) = M1 × M2 . Unlike the previous subsection where size of M1\\n\\n\\x0c13\\n\\nwas fixed, we now allow its size to vary with the index of the code. We restrict ourselves to the cases where\\n|M1(k) |\\nlim inf lnE[τ\\n= r . This limit gives us the rate of the special bits. It is worth noting at this point that even when\\n(k) ]\\nk→∞\\n\\n(k)\\n\\nthe rate r of special bits is zero, the number of special bits might not be bounded, i.e. lim inf |M1 | might be\\nk→∞\\n\\nf\\ninfinite. The error exponent Ebits\\n,Q at a given rate r of special bits is defined as follows,\\nDefinition 8: For any capacity achieving sequence with feedback Q with the message sets M(k) of the form\\n(k)\\n(k)\\nf\\nM(k) = M1 × M2 , rQ and Ebits\\n,Q are defined as\\nln |M1(k) |\\n(k)\\nk→∞ E[τ ]\\n\\nrQ , lim inf\\nf\\nf\\nThen Ebits\\n(r) is defined as Ebits\\n(r) ,\\n\\n− ln Pr (k) [M̂1 6=M1 ]\\nE[τ (k) ]\\nk→∞\\n\\nf\\nEbits\\n,Q , lim inf\\n\\nf\\nsup Ebits\\n,Q\\n\\nQ:rQ ≥r\\n\\nNext theorem shows how this exponent decays linearly with rate r of the special bits.\\nTheorem 6:\\n\\x01\\nf\\nEbits\\n(r) = 1 − Cr C̃\\n\\nf\\nNotice that the exponent Ebits\\n(0) = C̃, i.e. it is as high as the exponent in the single bit case, in spite of the\\nfact that here the number of bits can be growing to infinity with E [τ ]. This linear trade off between rate and\\nreliability reminds us of Burnashev’s result [9].\\n\\nOptimal strategy: Like the single bit case, we use a fixed length block code with erasures as our building block.\\nFirst transmitter sends M1 using a capacity achieving code of length Cr k. If the tentative decision M̃1 is correct,\\ntransmitter sends M2 with a capacity achieving code of length (1 − Cr )k. Otherwise transmitter sends the channel\\ninput xr for (1 − Cr )k time units. If the output sequence in the second phase is not typical with PY∗ an erasure\\nis declared and same strategy is repeated afresh. Else receiver uses a ML decoder to decide M̂2 and decodes\\nthe message M as M̂ = (M̃1 , M̂2 ). A decoding error for M1 happens only when an error happens in the first\\nphase and the output sequence in the second phase is typical with PY∗ when the reject codeword is sent. But\\nr\\n.\\nthe probability of the later event is = e−(1− C )C̃k . The factor of (1 − Cr ) arises because the relative duration of\\nthe second phase to the over all communication block. Similar to the single bit case, erasure probability remains\\nvanishingly small in this case. Thus not only the expected decoding time of the variable length block code is\\nroughly equal to the block length of the fixed length block code, but also its error probabilities are roughly equal\\nto the corresponding error probabilities associated with the fixed length block code.\\n\\nC. Multiple layers of priority\\nWe can generalize this result to the case when there are multiple levels of priority, where the most important\\nlayer contains E [τ ] r1 / ln 2 bits, the second-most important layer contains E [τ ] r2 / ln 2 bits and so on. For\\n(k)\\n(k)\\n(k)\\nan L-layer situation, message set M(k) is of the form M(k) = M1 × M2 × · · · × ML . We assume\\nwithout loss of generality that the order of importance of the Mi ’s is M1 ≻ M2 ≻ · · · ≻ ML . Hence we\\nhave Pe M1 ≤ Pe M2 ≤ · · · ≤ Pe ML .\\nThen for any L-layer capacity achieving sequence with feedback, we define the error exponent of the sth layer\\nas\\n− ln Pr (k) [M̂s 6=Ms ]\\nf\\nEbits\\n.\\n,s,Q = lim inf\\nE[τ (k) ]\\nk→∞\\n\\nThe achievable error exponent region of the L-layered capacity achieving sequences with feedback is the set\\nf\\nf\\nf\\nof all achievable exponent vectors (Ebits\\n,1,Q , Ebits ,2,Q , . . . , Ebits ,L−1,Q ). The following theorem determines that\\nregion.\\n\\n\\x0c14\\n\\nTheorem 7: Achievable error exponent region of the L-layered capacity achieving sequences with feedback,\\nfor rate vector (r1 , r2 , . . . , rL−1 ) is the set of vectors (E1 , E2 , . . . , EL−1 ) satisfying,\\n!\\nPi\\nr\\nj\\nj=1\\nEi ≤ 1 −\\nC̃\\n∀i ∈ {1, 2, . . . , (L − 1)}.\\nC\\nNote that the least important layer cannot achieve any positive error exponent because we are communicating at\\ncapacity, i.e. EL = 0.\\nOptimal strategy: Transmitter first sends the most important layer, M1 , using a capacity achieving code of length\\nr1\\nk. If it is decoded correctly, then it sends the next layer with a capacity achieving code of length rC2 k. Else\\nC\\nit starts sending the input letter xr for not only rC2 k time units but also for all remaining L − 2 phases. Same\\nstrategy is repeated for M3 , M4 , . . . , ML .\\nOnce the whole block of channel outputs, Y k , is observed; receivers checks the empirical distribution of the\\noutput in all of the phases except the first one. If they are all typical with PY∗ receiver uses the tentative decisions\\nto decode, M̂ = (M̃1 , M̃2 , . . . M̃L ). If one or more of the output sequences are not typical with PY∗ an erasure is\\ndeclared for the whole block and transmission starts from scratch.\\nFor each layer i, with the above strategy we can achieve an exponent as if there were only two kinds of bits\\n(as in Theorem 6)\\n• bits in layer i or in more important layers k < i (i.e. special bits)\\n• bits in less important layers (i.e. ordinary bits).\\nHence Theorem 7 does not only specify the optimal performance when there are multiple layers, but also shows\\nthat the performance we observed in Theorem 6, is successively refinable. Figure 4 shows these simultaneously\\nachievable exponents of Theorem 6, for a particular rate vector (r1 , r2 , . . . , rL−1 ).\\n\\nC )C̃\\nr1 +r2\\n)C̃\\nC\\n\\n(1 −\\n\\n(1 −\\n(1 −\\n\\n(1 −\\n(1 −\\n\\nexponent\\n6\\nC̃ Q\\nr1\\n\\nP3\\n\\ni=1 ri\\n\\nP4C\\n\\ni=1 ri\\n\\nC\\nP5\\n\\ni=1 ri\\n\\nC\\n\\nQ\\n\\n)C̃\\n\\nQ\\n\\nQ\\n\\nQ\\n\\n)C̃\\n\\nQ\\n\\n)C̃\\nr1\\n\\nFig. 4.\\n\\nQ\\n\\nr2\\n\\nr3\\n\\nr4\\n\\nQ\\n\\nQ\\n\\nr5\\n\\nQ rate\\n\\nC\\n\\nSuccessive refinability for multiple layers of priority, demonstrated on an example with six layers;\\n\\nP6\\n\\ni=1\\n\\nri = C.\\n\\nNote that the most important layer can achieve an exponent close to C̃ if its rate is close to zero. As we move\\nto the layers with decreasing importance, the achievable error exponent decays gradually.\\nD. Special message\\nNow consider one particular message, say the first one, which requires small missed-detection probability.\\nf\\nSimilar to the no-feedback case, define Emd\\nas its missed-detection exponent at capacity.\\nDefinition 9: For any capacity achieving sequence with feedback, Q, missed detection exponent is defined as\\nf\\nEmd\\n,Q , lim inf\\nk→∞\\n\\nf\\nf\\nf\\nThen Emd\\nis defined as Emd\\n, supQ Emd\\n,Q .\\n\\n− ln Pr\\n\\n[M̂ 6=1|M =1]\\n.\\nE[τ (k) ]\\n\\n(k)\\n\\n\\x0c15\\n\\nTheorem 8:\\nf\\nEmd\\n= C̃.\\n\\nTheorem 2 and 8 implies following corollary,\\nf\\nCorollary 1: Feedback doesn’t improve the missed detection exponent of a single special message: Emd\\n= Emd .\\nIf red-alert exponent were defined as the best protection of a special message achievable at capacity, then this result\\ncould have been thought of as an analog the “feedback does not increase capacity” for the red-alert exponent.\\nf\\nAlso note that with feedback, Emd\\nfor the special message and Ebf for the special bit are equal.\\nE. Many special messages\\nNow let us consider the problem where the first ⌈eE[τ ]r ⌉ messages are special, i.e. Ms = {1, 2, . . . , ⌈eE[τ ]r ⌉}.\\nUnlike previous problems, now we will also impose a uniform expected delay constraint as follows.\\nDefinition 10: For any reliable variable length block code with feedback,\\nΓ ,\\n\\nmaxi∈M E[τ |M =i]\\nE[τ ]\\n\\nA reliable sequence with feedback, Q, is a uniform delay reliable sequence with feedback if and only if\\nlim Γ(k) = 1.\\nk→∞\\n\\nThis means that the average E [τ | M = i] for every message i is essentially equal to E [τ ] (if not smaller). This\\nuniformity constraint reflects a system requirement for ensuring a robust delay performance, which is invariant of\\nf\\nthe transmitted message.8 Let us define the missed-detection exponent Emd\\n(r) under this uniform delay constraint.\\nDefinition 11: For any uniform delay capacity achieving sequence with feedback, Q, the missed detection\\nexponent achieved on sequence of subsets Ms is defined as\\nf\\nEmd\\n,Q,Ms\\n\\n, lim inf\\nn→∞\\n\\n− ln max\\n\\n[M̂ 6=i|M =i]\\ni\\nh\\n.\\nE τ (k)\\n(k)\\n\\nPr\\n\\n(k)\\n\\ni∈Ms\\n\\nf\\nf\\nThen for a given r < C , we define Emd\\n(r) , supQ,Ms Emd\\n,Q,Ms where maximization is over Ms ’s such that\\n(k)\\n\\nln |Ms |\\n\\x02\\n\\x03 = r.\\nk→∞ E τ (k)\\nThe following theorem shows that the special messages can achieve the minimum of the red-alert exponent\\nand the Burnashev’s exponent at rate r .\\nTheorem 9:\\n\\x08\\n\\t\\nf\\nEmd\\n(r) = min C̃, (1 − Cr )Dmax , ∀ r < C.\\n\\n\\x01\\nwhere Dmax , max D WY |X (·|i)\\n WY |X (·|j) .\\n\\nlim inf\\n\\ni,j∈X\\n\\nC̃\\n)C] each special message achieves the best missed detection exponent C̃ for a single special\\nFor r ∈ [0, (1 − Dmax\\nC̃\\n)C, C) special messages achieve\\nmessage, as if the rest of the special messages were absent. For r ∈ [(1 − Dmax\\nthe Burnashev’s exponent as if the ordinary messages were absent.\\nThe optimal strategy is based on transmitting a special bit first. This result demonstrates, yet another time,\\nhow feedback connects bit-wise UEP with message-wise UEP. In the optimal strategy for bit-wise UEP with\\nmany bits a special message was used, whereas now in message wise UEP with many messages a special bit is\\nused. The roles of bits and messages, in two optimal strategies are simply swapped between the two cases.\\n\\nOptimal strategy: We combine the strategy for achieving C̃ for a special bit and the Yamamoto-Itoh strategy\\nfor achieving Burnashev’s exponent [40]. In the first phase, a special bit, b, is sent with a repetition code of\\n8\\n\\nOptimal exponents in all previous problems remain unchanged irrespective of this uniform delay constraint.\\n\\n\\x0c16\\n\\n√\\n\\nk symbols. This is the indicator bit for special messages: it is 1 when a special message is to be sent and 0\\notherwise.\\nIf b is decoded incorrectly as b̂ = 0, input letter xr is sent for the remaining k time unit. If it is decoded\\ncorrectly as b̂ = 0, then the ordinary message is sent using a codeword from a capacity achieving code. If the\\n∗\\noutput sequence in the second phase is typical with\\n√ PY receiver use an ML decoder to chose one of the ordinary\\nmessages, else an erasure is declared for (k + k) long block.\\nIf b̂ = 1, then a length k two phase code with errors and erasure decoding, like the one given in [40] by\\nYamamoto and Itoh, is used to send the message. In the communication phase a length Cr k capacity achieving\\ncode is used to send the message, M , if M ∈ Ms . If M ∈\\n/ Ms an arbitrary codeword from the length Cr k\\ncapacity achieving code is sent. In the control phase, if M ∈ Ms and if it is decoded correctly at the end of\\ncommunication phase, the accept letter xa is sent for (1 − Cr )k time units, else the reject letter, xd , is sent for\\n(1 − Cr )k time units. If the empirical distribution in the control phase is typical with WY |X (·|xa ) then special\\nmessage\\n√ decoded at the end of the communication phase becomes the final M̂ , else an erasure is declared for\\n(k + k) long block.\\nWhenever an erasure is declared for the whole block, transmitter and receiver applies above strategy again\\nfrom scratch. This scheme is repeated until a non-erasure decoding is reached.\\n\\nV. AVOIDING FALSE A LARMS\\nIn the previous sections while investigating message wise UEP we have only considered the missed detection\\nformulation of the problems. In this section we will focus on an alternative formulation of message wise\\nUEP problems based on false alarm probabilities.\\nA. Block Codes without Feedback\\nWe first consider the no-feedback case. When hfalse-alarm of ai special message is a critical event, e.g. the\\n“reboot” instruction, the false alarm probability Pr M̂ = 1|M 6= 1 for this message should be minimized, rather\\nh\\ni\\nthan the missed detection probability Pr M̂ 6= 1|M = 1 .\\nUsing Bayes’ rule and assuming uniformly chosen messages we get,\\nh\\ni\\nh\\ni Pr M̂ = 1, M 6= 1\\nPr M̂ = 1|M 6= 1 =\\nPr [M 6= 1]\\nh\\ni\\nP\\nPr\\nM̂\\n=\\n1|M\\n=\\nj\\nj6=1\\n.\\n=\\n(|M| − 1)\\n\\nIn classical error exponent analysis, [20], the error probability for a given message usually means its missed\\ndetection probability. However, examples such as the “reboot” message necessitate this notion of false alarm\\nprobability.\\nDefinition 12: For a capacity-achieving sequence, Q, such that\\n\\x0c\\nh\\ni\\n\\x0c\\nlim sup Pr (n) M̂ 6= 1\\x0c M = 1 = 0,\\nn→∞\\n\\nfalse alarm exponent is defined as\\n\\nEfa ,Q , lim inf\\nn→∞\\n\\nThen Efa is defined as Efa , supQ Efa ,Q .\\n\\n− ln Pr\\n\\n(n)\\n\\n[M̂ =1|M 6=1]\\nn\\n\\n.\\n\\n\\x0c17\\n\\nThus Efa is the best exponential decay rate of false alarm probability with n. Unfortunately we do not have\\nthe exact expression for Efa . However upper bound given below is sufficient to demonstrate the improvement\\nintroduced by feedback and variable decoding time.\\nTheorem 10:\\nEfal ≤ Efa ≤ Efau .\\nThe upper and lower bounds to the false alarm exponent are given by\\nEfal , max\\n\\ni∈X P\\n\\nmin\\n\\nj\\n\\nVY |X :\\n∗\\nVY |X (·|j)PX\\n(j)=WY |X (·|i)\\n\\n\\n\\x0c\\n\\x01\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX∗\\n\\n\\n\\x0c\\n\\x01\\nEfau , max D WY |X (·|i)\\n WY |X (·|X)\\x0c PX∗ .\\ni∈X\\n\\nThe maximizers of the optimizations for Efal and Efau are denoted by xfl and xfu\\n\\n\\x0c\\n\\x01\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX∗\\nEfal =\\nmin\\nP\\n\\nj\\n\\nVY |X :\\n∗\\n(j)=WY |X (·|xfl )\\nVY |X (·|j)PX\\n\\n\\n\\x0c\\n\\x01\\nEfau = D VY |X (·|xfu )\\n WY |X (·|X)\\x0c PX∗ .\\n\\nStrategy to reach lower bound Codeword for the special message M = 1 is a repetition sequence of input letter\\nxfl . Its decoding region is the typical ‘noise ball’ around it, the output sequences whose empirical distribution is\\napproximately equal to WY |X (·|xfl ). For the ordinary messages, we use a capacity achieving code-book where\\nall codewords have the same empirical distribution (approx.) PX∗ . Then for y n whose empirical distribution is\\nnot in the typical ‘noise ball’ around the special codeword, receiver makes an ML decoding among the ordinary\\ncodewords.\\nNote the contrast between this strategy for achieving Efal and the optimal strategy for achieving Emd . For achieving\\nEmd , output sequences of any type other than the ones close to PY∗ were decoded as the special message; whereas\\nfor achieving Efa , only the output sequences of types that are close to WY |X (·|xfl ) are decoded as the special\\nmessage.\\n\\nFig. 5.\\n\\nAvoiding false-alarm\\n\\nIntuitive interpretation: A false alarm exponent for the special message corresponds to having the smallest\\npossible decoding region for the special message. This ensures that when some ordinary message is transmitted,\\nprobability of the event {M̂ = 1} is exponentially small. We cannot make it too small though, because when the\\nspecial message is transmitted, the probability of the very same event should be almost one. Hence the decoding\\nregion of the special message should at least contain the typical noise ball around the special codeword. The\\nblue region in Fig. 5 denotes such a region.\\n\\n\\x0c18\\n\\nNote that Efal is larger than channel capacity C due to the convexity of KL divergence.\\n\\n\\x0c\\n\\x01\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX∗\\nEfal = max\\nmin\\ni∈X P\\n\\n> max\\n\\ni∈X P\\n\\nj\\n\\nVY |X :\\n∗\\nVY |X (·|j)PX\\n(j)=WY |X (·|i)\\n\\nmin\\n\\nVY |X :\\n∗\\nj VY |X (·|j)PX (j)=WY |X (·|i)\\n\\nD\\n\\nX\\nk\\n\\n\\n\\x01\\n= max D WY |X (·|i)\\n PY∗ (·)\\ni∈X\\n\\n\\n!\\n\\nX\\n\\nP ∗ (k′ )WY |X (·|k′ )\\nPX∗ (k)VY |X (·|k)\\n\\n ′ X\\nk\\n\\n=C\\n\\nwhere PY∗ denotes the output distribution corresponding to the capacity achieving input distribution PX∗ and the\\nlast equality follows from KKT condition for achieving capacity we mentioned previously [20, Theorem 4.5.1].\\nNow we can compare our result for a special message with the similar result for classical situation where all\\nmessages are treated equally. It turns out that if every message in a capacity-achieving code demands equally\\ngood false-alarm exponent, then this uniform exponent cannot be larger than C. This result seems to be directly\\nconnected with the problem of identification via channels [1]. We can prove the achievability part of their capacity\\ntheorem using an extension of the achievability part of Efal . Perhaps a new converse of their result is also possible\\nusing such results. Furthermore we see that reducing the demand of false-alarm exponent to only one message,\\ninstead of all, enhances it from C to at least Efal .\\nB. Variable Length Block Codes with Feedback\\nRecall that feedback does not improve the missed-detection exponent for a special message. On the contrary,\\nthe false-alarm exponent of a special message is improved when feedback is available and variable decoding\\ntime is allowed. We again restrict to uniform delay capacity achieving sequences with feedback, i.e. capacity\\nachieving sequences satisfying lim Γ(k) = 1.\\nk→∞\\nDefinition 13: For a uniform delay capacity-achieving sequence with feedback, Q, such that\\n\\x0c\\nh\\ni\\n\\x0c\\nlim sup Pr (k) M̂ 6= 1\\x0c M = 1 = 0,\\nk→∞\\n\\nfalse alarm exponent is defined as\\n\\nEfaf ,Q , lim inf\\nk→∞\\n\\n− ln Pr\\n\\n[M̂ =1|M 6=1]\\n.\\nE[τ k ]\\n\\n(k)\\n\\nThen Efaf is defined as Efaf , supQ Efaf ,Q .\\nTheorem 11:\\nEfaf = Dmax .\\n\\nNote that Dmax > Efau . Thus feedback strictly improves the false alarm exponent, Efaf > Efa .\\nOptimal strategy: We use\\n√ a strategy similar to the one employed in proving Theorem 9 in subsection IV-E. In\\nthe first phase, a length k code is used to convey whether M = 1 or not, using a special bit b = I{M =1} .\\n•\\n\\n•\\n\\nIf b̂ = 0, a length k capacity achieving code √\\nwith Emd = C̃ is used. If the decoded message for the length\\nk code is 1, an erasure is declared for (k + k) long\\n√ block. Else the decoded message of length k code\\nbecomes the decoded message for the whole (k + k) long block.\\nIf b̂ = 1,\\n– and M = 1, input symbol xa is transmitted for k time units.\\n– and M 6= 1, input symbol xd is transmitted for k time units.\\n\\n\\x0c19\\n√\\n\\nIf the output sequence, Y√ k+k , is typical with WY |X (·|xa ) then M̂ = 1 else an erasure is declared for\\nk+1\\n√\\n(k + k) long block.\\nReceiver and transmitter starts from scratch if an erasure is declared at the end of second phase.\\nNote that, this strategy simultaneously achieves the optimal missed-detection exponent C̃ and the optimal\\nfalse-alarm exponent Dmax for this special message.\\nVI. F UTURE\\n\\nDIRECTIONS\\n\\nIn this paper we have restricted our investigation of UEP problems to data rates that are essentially equal to\\nthe channel capacity. Scenarios we have analyzed provides us with a rich class of problems when we consider\\ndata rates below capacity.\\nMost of the UEP problems has a coding theoretic version. In these coding theoretic versions deterministic\\nguarantees, in terms of Hamming distances, are demanded instead of the probabilistic guarantees, in terms of\\nerror exponents. As we have mentioned in section I-A, coding theoretic versions of bit-wise UEP problems have\\nbeen studied for the case of linear codes extensively. But it seems coding theoretic versions of both message-wise\\nUEP problems and bit-wise UEP problem for non-linear codes are scarcely investigated [3], [5].\\nThroughout this paper, we focused on the channel coding component of communication. However, often times,\\nthe final objective is to communicate a source within some distortion constraint. Message-wise UEP problem itself\\nhas first come up within this framework [12]. But the source we are trying to convey can itself be heterogeneous,\\nin the sense that some part of its output may demand a smaller distortion than other parts. Understanding optimal\\nmethods for communicating such sources over noisy channels present many novel joint-source channel coding\\nproblems.\\nAt times the final objective of communication is achieving some coordination between various agents [14]. In\\nthese scenarios channel is used for both communicating data and achieving coordination. A new class of problem\\nlends itself to us when we try to figure out the tradeoffs between error exponents of the coordination and data?\\nWe can also actively use UEP in network protocols. For example, a relay can forward some partial information\\neven if it cannot decode everything. This partial information could be characterized in terms of special bits as\\nwell as special messages. Another example is two-way communication, where UEP can be used for more reliable\\nfeedback and synchronization.\\nInformation theoretic understanding of UEP also gives rise to some network optimization problems. With UEP,\\nthe interface to physical layer is no longer bits. Instead, it is a collection of various levels of error protection.\\nThe achievable channel resources of reliability and rate need to be efficiently divided amongst these levels, which\\ngives rise to many resource allocation problems.\\nVII. B LOCK C ODES\\n\\nWITHOUT\\n\\nF EEDBACK : P ROOFS\\n\\nIn the following sections, we use the following standard notation for entropy, conditional entropy and mutual\\ninformation,\\nX\\nH(PX ) =\\nPX (j) ln PX1(j)\\nj∈X\\n\\nH(WY |X |PX ) =\\nI(P, W ) =\\n\\nX\\n\\nj∈X ,k∈Y\\n\\nX\\n\\nj∈X ,k∈Y\\n\\nPX (j)WY |X (k|j) ln WY |X1(k|j)\\nPX (j)WY |X (k|j) ln P\\n\\ni∈X\\n\\nWY |X (k|j)\\nWY |X (k|i)PX (i) .\\n\\nIn addition we denote the decoding region of a message i ∈ M by G(i), i.e.\\nG(i) , {y n : M̂ (y n ) = i}.\\n\\n\\x0c20\\n\\nA. Proof of Theorem 1\\nProof:\\nWe first show that any capacity achieving sequence Q with Eb ,Q can be used to construct another capacity\\nE\\nachieving sequence, Q′ with Eb ,Q′ = b2,Q , all members of which are fixed composition codes. Then we show\\nthat Eb ,Q′ = 0 for any capacity achieving sequence, Q′ which only includes fixed composition codes.\\n(n)\\nConsider a capacity achieving sequence, Q with message sets M(n) = M1 × M2 , where M1 = {0, 1}. As\\n4\\na result of Markov inequality, at least 5 |M(n) | of the messages in M(n) satisfy,\\n\\x0c\\nh\\ni\\nh\\ni\\n\\x0c\\nPr M̂1 6= M1 \\x0c M = i ≤ 5 Pr M̂1 6= M1 .\\n(1)\\nSimilarly at least 45 |M(n) | of the messages in M(n) satisfy,\\n\\x0c\\nh\\ni\\nh\\ni\\n\\x0c\\nPr M̂ 6= M \\x0c M = i ≤ 5 Pr M̂ 6= M .\\n\\n(2)\\n\\n1\\n|M(n) |\\nThus at least 35 |M(n) | of the messages in M(n) satisfy both (1) and (2). Consequently at least 10\\nmessages are of the form (0, M2 ) and satisfy equations (1) and (2). If we group them according to their empirical\\n|M(n) |\\ndistribution at least one of the groups will have more than 10(n+1)\\n|X | messages because the number of different\\n(n)\\n\\n|M |\\nempirical distributions for elements of X n is less than (n + 1)|X | . We keep the first 10(n+1)\\n|X | codewords of this\\n′\\nmost populous type, denote them by x̄A (·) and throw away all of other codeword corresponding to the messages\\nof the form (0, M2 ). We do the same for the messages of the form M = (1, M2 ) and denote corresponding\\ncodewords by x̄′B (·).\\nThus we have a length n code with message set M′ of the form M′ = M1 × M′2 where M1 = {0, 1} and\\n|M′2 |\\n|M′2 | = 10(n+1)\\n|X | . Furthermore,\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\nh\\ni\\ni\\nh\\n\\x0c\\n\\x0c\\nPr M̂1′ 6= M1′ \\x0c M ′ = i ≤ 5 Pr M̂1 6= M1\\nPr M̂ ′ 6= M ′ \\x0c M ′ = i ≤ 5 Pr M̂ 6= M\\n∀i ∈ M′ .\\n\\nNow let us consider following 2n long block code with message set M′′ = M1 × M′′2 × M′′3 where M′′2 =\\nM′′3 = M′2 . If M ′′ = (0, M2′′ , M3′′ ) then x̄(M ′′ ) = x̄′A (M2′′ )x̄′B (M3′′ ). If M ′′ = (1, M2′′ , M3′′ ) then x̄(M ′′ ) =\\nx̄′B (M2′′ )x̄′A (M3′′ ). Decoder of this new length 2n code uses the decoder of the original length n code first on y n\\n2n . If the concatenation of length n codewords corresponding to the decoded halves, is a codeword\\nand then on yn+1\\nfor an i ∈ M′′ then M̂ ′′ = i. Else an arbitrary message is decoded. One can easily see that the error probability\\nof the length 2n code is less than the twice the error probability of the length n code, i.e.\\n\\x0c\\n\\x0c\\n\\x0c\\ni\\ni\\nh\\nh\\ni\\nh\\n\\x0c\\n\\x0c\\n\\x0c\\nPr M̂ ′′ 6= M ′′ \\x0c M ′′ ≤ 1 − (1 − Pr M̂ ′ 6= M ′ \\x0c M ′ = M2′′ )(1 − Pr M̂ ′ 6= M ′ \\x0c M ′ = M3′′ )\\nh\\ni\\n≤ 2 Pr M̂ ′ 6= M ′ .\\n\\nFurthermore bit error probability of the new code is also at most twice the bit error probability of the length n\\ncode, i.e.\\n\\x0c\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\nh\\ni\\n\\x0c\\n\\x0c\\n\\x0c\\nPr M̂1′′ 6= M1′′ \\x0c M1′′ ≤ 1 − (1 − Pr M̂1′ 6= M1′ \\x0c M1′ = M1′′ )(1 − Pr M̂1′ 6= M1′ \\x0c M1′ = M1′′ )\\ni\\nh\\n≤ 2 Pr M̂1′ 6= M1′\\nE\\n\\nThus using these codes one can obtain a capacity achieving sequence Q′ with Eb ,Q′ = b2,Q all members of\\nwhich are fixed composition codes.\\nIn the following discussion we focus on capacity achieving sequences, Q’s which are composed of fixed\\ncomposition codes only. We will show that Eb ,Q = 0 for all capacity achieving Q’s with fixed composition\\ncodes. Consequently the discussion above implies that Eb = 0.\\n\\n\\x0c21\\n\\nWe call the empirical distribution of a given output sequence, y n , conditioned on the code word, x̄(i), the\\nconditional type of y n given the message i and denote it by V(y n , i). Furthermore we call the set of y n ’s whose\\nconditional type with message i is V , the V -shell of i and denote it by TV (i). Similarly we denote the set of\\noutput sequences y n with the empirical distribution UY , by TUY .\\n(n)\\nWe denote the empirical distribution of the codewords of the nth code of the sequence by PX and the\\n(n)\\ncorresponding output distribution by PY , i.e.\\nX\\n(n)\\n(n)\\nWY |X (·|i)PX (i).\\nPY (·) =\\ni∈X\\n\\nWe simply use PX and PY whenever the value of n is unambiguous from the context. Furthermore PnY (·) stands\\nfor the probability measure on Y n such that\\nPnY\\n\\nn\\n\\n(y ) =\\n\\nn\\nY\\n\\nPY (yk ).\\n\\nk=1\\n(n)\\n\\nS0,V is the set of y n ’s for which M̂1 = 0 and V(y n , M̂ (y n )) = V .\\n(n)\\n\\n(3)\\nS0,V , {y n : V(y n , M̂ (y n )) = V and M̂ (y n ) = (0, j) for some j ∈ M2 }\\n\\x10\\n\\x11\\n(n)\\nIn other words, S0,V is the set of y n ’s such that y n ∈ TV M̂ (y n ) and decoded value of the first bit is zero.\\n\\nNote that since for each y n ∈ Y n there is a unique M̂ (y n ) and for each y n ∈ Y n and message i ∈ M there is\\n(n)\\n(n)\\n(n)\\n(n)\\nunique V(y n , i); each y n belongs to a unique S0,V or S1,V , i.e. S0,V ’s and S1,V ’s are disjoint sets that collectively\\ncover the set Y n .\\nLet us define the typical neighborhood of WY |X as [W ]\\np\\n(n)\\n(n)\\n[W ] , {VY |X : |VY |X (j|i)PX (i) − WY |X (j|i)PX (i)| ≤ 4 1/n ∀i, j}\\n(4)\\n[\\n(n)\\n(n)\\n(n)\\nS0,V . We will establish the following\\nLet us denote the union of all S0,V ’s for typical V ’s by S0 =\\ninequality later. Let us assume for the moment that it holds.\\n\\x10\\n\\x10\\n\\x11\\n(n)\\n(n)\\n≥ en(R −(C+ǫn )) 21 −\\nPnY S0\\n\\nV ∈[W ]\\n|X ||Y|\\n√\\n8 n\\n\\n− Pe\\n\\n\\x11\\n\\n(5)\\n\\nwhere lim ǫn = 0.\\nn→∞\\nAs a result of bound given in (5) and the blowing up lemma [13, Ch. 1, Lemma 5.4], we can conclude that\\nfor any capacity achieving sequence Q, there exists a sequence of (ℓn , ηn ) pairs satisfying lim ηn = 1 and\\nℓn\\nn→∞ n\\n\\nlim\\n\\nn→∞\\n\\n= 0 such that\\n\\n\\x11\\n\\x10\\n(n)\\nPnY Γℓn (S0 ) ≥ ηn\\n\\nwhere Γℓn (A) is the set of all y n ’s which differs from an element of A in at most ℓn places. Clearly one can\\n(n)\\nrepeat the same argument for Γℓn (S1 ) to get,\\n\\x10\\n\\x11\\n(n)\\nPnY Γℓn (S1 ) ≥ ηn .\\n\\nConsequently,\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\n[\\n\\\\\\n(n)\\n(n)\\n(n)\\n(n)\\n(n)\\n(n)\\nPnY Γℓn (S0 ) Γℓn (S1 ) = PnY Γℓn (S0 ) + PnY Γℓn (S1 ) − PnY Γℓn (S0 ) Γℓn (S1 )\\n\\x10\\n\\x11\\n\\\\\\n(n)\\n(n)\\nPnY Γℓn (S0 ) Γℓn (S1 ) ≥ 2ηn − 1.\\n\\n\\x0c22\\n(n)\\n\\nNote that if y n ∈ Γℓn (S1 ), then there exist at least one element ỹ n ∈ TPY which differs from y n in at most\\n(|Y||X |n3/4 + ℓn ) places.9 Thus we can upper bound its probability by,\\n(n)\\n\\ny n ∈ Γℓn (S1 ) ⇒ PnY (y n ) ≤ e−nH(PY )−(|Y||X |n\\n\\n3/4\\n\\n+ℓn ) ln λ\\n\\nwhere λ = mini,j WY |X (j|i). Thus we have\\n\\\\\\n3/4\\n(n)\\n(n)\\n(6)\\n|Γℓn (S0 ) Γℓn (S1 )| ≥ (2ηn − 1)enH(PY )+(|Y||X |n +ℓn ) ln λ .\\n(n)\\n(n) T\\nNote that for any y n ∈ Γℓn (S0 ) Γℓn (S1 ), there exist a ỹ n ∈ TW (i) for an i of the form i = (0, M2 ) which\\nn\\n3/4\\ndiffers from y in at most (|Y||X |n + ℓn ) places.10 Consequently\\nSince M2 =\\nas follows,\\n\\ne\\n\\nnR(n)\\n\\n2\\n\\nPr [y n | M = i] ≥ e−nH(WY |X |PX )+(|Y||X |n\\n\\n3/4\\n\\n+ℓn ) ln λ\\n\\n.\\n\\n(7)\\n\\nusing equation (7) we can lower bound the probability of y n under the hypothesis M1 = 0\\nPr [y n | M1 = 0] =\\n\\nX\\n\\nj∈M2\\n\\nPr [y n | M = (0, j)] Pr [M = (0, j)| M1 = 0]\\n(n)\\n\\n≥ 2e−n(H(WY |X |PX )+R\\n\\n)+(|Y||X |n3/4 +ℓn ) ln λ\\n\\n.\\n\\n(8)\\n\\nClearly same holds for M1 = 1 too, thus\\n(n)\\n\\nPr [y n | M1 = 1] ≥ 2e−n(H(WY |X |PX )+R\\n\\n)+(|Y||X |n3/4+ℓn ) ln λ\\n\\n.\\n\\n(9)\\n\\nConsequently,\\nh\\ni X\\nn\\nn\\n1\\nPr M̂1 6= M1 ≥\\n2 min(Pr [y | M1 = 0] , Pr [y | M1 = 1])\\nyn\\n\\n(a)\\n\\n≥\\n(b)\\n\\nX\\n\\ny n ∈Γℓn (S0(n) )\\n\\nT\\n\\n(n)\\n\\ne−n(H(WY |X |PX )+R\\n\\n)+(|Y||X |n3/4 +ℓn ) ln λ\\n\\nΓℓn (S1(n) )\\n\\n≥(2ηn − 1)enH(PY )+(|Y||X |n\\n(n)\\n\\n= (2ηn − 1)en(I(PX ,W )−R\\n\\n3/4\\n\\n+ℓn ) ln λ −n(H(WY |X |PX )+R(n) )+(|Y||X |n3/4+ℓn ) ln λ\\n\\ne\\n\\n)+2(|Y||X |n3/4+ℓn ) ln λ\\n\\n(10)\\n\\nwhere (a) follows from equations (8) and (9) and (b) follows from equation (6).\\nUsing Fano’s inequality we get,\\nI (M ; Y n ) − nR(n) ≥ − ln 2 − nR(n) Pe (n)\\n\\nn)\\n\\n(11)\\n\\nwhere I (M ; Y is the mutual information between the message M and channel output Y\\nupper bound I (M ; Y n ) as follows,\\nX\\nn\\n|i]\\nI (M ; Y n ) =\\nPr [i, y n ] ln Pr[y\\nPr[y n ]\\ni∈M,y n ∈Y n\\n\\n=\\n\\nX\\n\\nn\\n\\nPr [i, y n ] ln QnPr[yPY|i](yk ) −\\nk=1\\n\\nX\\n\\n= nI(PX , W )\\n\\nIn addition we can\\n\\nn\\n\\n]\\nPr [y n ] ln QnPr[y\\nPY (yk )\\n\\ny n ∈Y n\\ni∈M,y n ∈Y n\\nn\\nXX\\n(a) X\\nW\\n(y |x̄ (i))\\n1\\n≤\\nWY |X (yk |x̄k (i)) ln Y |XPY (yk k )k\\n|M|\\ni∈M\\nk=1 yk\\n\\n(a)\\n\\nn.\\n\\nk=1\\n\\n(12)\\n\\n9\\nBecausePof the integer constraints TPY might actually be an empty set. If so we can make a similar argument for the UY∗ which\\nminimizes j |UY (j) − PY (j)|. However this technicality is inconsequential.\\n10\\nInteger constraints here are inconsequential too.\\n\\n\\x0c23\\n\\nP\\nwhere PY (·) = j∈X WY |X (·)PX (j). Step (a) follows the non-negativity of KL divergence and step (b) follows\\nfrom the fact that all the code words are of type PX (·).\\nUsing equations (10), (11) and (12) we get\\nh\\ni\\n(n)\\n(n)\\n3/4\\nPr M̂1 6= M1 ≥ (2ηn − 1)e− ln 2−nR Pe +2(|Y||X |n +ℓn ) ln λ\\n\\nThus using lim Pe (n) = 0, lim ηn = 1 and lim\\nn→∞\\n\\nℓn\\nn→∞ n\\n\\nn→∞\\n\\nlim\\n\\nn→∞\\n\\n= 0 we conclude that,\\n\\n− ln Pr\\n\\n(n)\\n\\n[M̂1 6=M1 ]\\n\\n=0\\n\\nn\\n\\nNow only think left, for proving Eb = 0, is to establish inequality (5). One can write the error probability of the\\nnth code of Q as\\nX\\nX\\n1\\n(1 − I{M̂ (yn )=i} ) Pr [y n | M = i]\\nPe (n) =\\nM\\nn\\nn\\ny ∈Y\\ni∈M(n)\\nX\\nX X\\n−nR(n)\\n=\\n(1 − I{M̂ (yn )=i} )e−n(D(VY |X (·|X)kWY |X (·|X)|PX )+H(VY |X |PX ))\\ne\\nV\\n\\ni∈M(n)\\n\\n=\\n\\nX\\n\\ny n ∈TV (i)\\n\\n(n)\\n\\ne−n(D(VY |X (·|X)kWY |X (·|X)|PX )+H(VY |X |PX )+R\\n\\n)\\n\\nV\\n\\n=\\n\\nX\\n\\nX\\n\\ni∈M(n)\\n−n(D(VY |X (·|X)kWY |X (·|X)|PX )+H(VY |X |PX )+R(n) )\\n\\ne\\n\\nX\\n\\ny n ∈TV\\n\\n(i)\\n\\n(1 − I{M̂ (yn )=i} )\\n\\n(Q0,V + Q1,V )\\n\\n(13)\\n\\nV\\n\\nwhere Qk,V =\\n\\nX\\n\\nX\\n\\ni=(k,j) y n ∈TV (i)\\nj∈M2\\n\\n(1 − I{M̂ (yn )=i} ) for k = 0, 1.\\n\\nNote that Qk,V is the sum, over the messages i for which M1 = k, of the number of the elements in TV (i)\\nthat are not decoded to message i. In a sense it is a measure of the contribution of the V -shells\\n\\x10 of\\x11 different\\n(n)\\nn\\ncodewords to the error probability. We will use equation (13) to establish lower bounds on PY S0,V ’s.\\n(n)\\n\\nNote that all elements of S0,V have the same probability under PnY (·) and\\n\\x11\\n\\x10\\nX\\n(n)\\n(n)\\nwhere ζ =\\nPX (x)VY |X (y|x) ln PY1(y) .\\nPnY S0,V = |S0,V |e−ζn\\n\\n(14)\\n\\nx,y\\n\\nNote that\\nζ=\\n\\nX\\nx,y\\n\\nPX (x)VY |X (y|x) ln\\n\\nWY |X (y|x)\\nPY (y)\\n\\n+\\n\\nX\\nx,y\\n\\nPX (x)VY |X (y|x) ln WY |X1(y|x)\\n\\n\\n\\x0c\\n\\x01\\n= I(PX , WY |X ) + D VY |X (·|X)\\n WY |X (·|X)\\x0c PX + H(VY |X |PX )\\nX\\n+\\nPX (x)(VY |X (y|x) − WY |X (y|x)) ln\\n\\nWY |X (y|x)\\nPY (y)\\n\\nx,y\\n\\nRecall that I(PX , WY |X ) ≤ C and min WY |X (i|j) = λ. Thus using the definition of [WY |X ] given in equation\\ni,j\\n\\n(4) we get,\\n\\nwhere ǫn =\\nNote that\\n\\n\\n\\x0c\\n\\x01\\nζ ≤ C + ǫn + D VY |X (·|X)\\n WY |X (·|X)\\x0c PX + H(VY |X |PX )\\n\\n|X ||Y|\\n√\\n4\\nn\\n\\n∀VY |X ∈ [WY |X ]\\n\\n(15)\\n\\nln λ1 .\\n\\n(n)\\n\\n(n)\\n\\n(n)\\n\\n|S0,V | = |M2 | · |TV (i) | − Q0,V = 12 |TV (i) |enR\\n\\n− Q0,V .\\n\\n(16)\\n\\n\\x0c24\\n(n)\\n\\nRecalling that S0,V ’s are disjoint and using equations (14), (15) and (16) we get\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\nX\\n(n)\\n(n)\\nPnY S0\\n≥\\nPnY S0,V\\nV ∈[W ]\\n\\n≥\\n\\nX\\n\\ne−n(C+ǫn )\\n\\nV ∈[W ]\\n\\n(a)\\n\\n(n)\\n\\n≥ en(R\\n\\n(n)\\n\\n= en(R\\n(b)\\n\\n(n)\\n\\n≥ en(R\\n\\n\\x10\\n\\n\\uf8eb\\n\\n−(C+ǫn )) \\uf8ed\\n−(C+ǫn ))\\n\\n−(C+ǫn ))\\n\\n\\uf8eb\\n\\n1\\n2 |TV\\n\\nX\\n\\nV ∈[W ]\\n\\n\\uf8ed1\\n\\n\\x10\\n\\n2\\n\\n1\\n2\\n\\n(n)\\n\\n(i) |enR\\n1\\n2 |TV\\n\\nX\\n\\n\\uf8f6\\n\\n(i) |e−n(D(VY |X (·|X)kWY |X (·|X)|PX )+H(VY |X |PX )) − Pe \\uf8f8\\n\\nX\\n\\nV ∈[W ] y n ∈TV (i)\\n\\n−\\n\\n|X√\\n||Y|\\n8 n\\n\\n\\x11\\n− Q0,V e−n(D(VY |X (·|X)kWY |X (·|X)|PX )+H(VY |X |PX ))\\n\\n− Pe\\n\\n\\x11\\n\\n\\uf8f6\\n\\nPr [y n | M = i] − Pe \\uf8f8\\n\\nwhere (a) follows the equation (13) and (b) follows from the Chebyshev’s inequality.11\\n\\n•\\n\\nB. Proof of Theorem 2\\n1) Achievability: Emd ≥ C̃:\\nProof:\\nFor each block length n, the special message is sent with the length n repetition sequence x̄n (1) = (xr , xr · · · , xr )\\nwhere xr is the input letter satisfying\\n\\x01\\n\\x01\\nD PY∗ (·)k WY |X (·|xr ) = max D PY∗ (·)k WY |X (·|i) .\\ni\\n\\nThe remaining |M| − 1 ordinary codewords are generated randomly and independently of each other using\\ncapacity achieving input distribution PX∗ i.i.d. over time.\\nLet us denote the empirical distribution of a particular output sequence y n by Q(yn ) . The receiver decodes to\\nthe special message only when the output distribution is not close to PY∗ . Being more precise, the set of output\\nsequences close to PY∗ , [PY∗ ], and decoding region of the special message, G(1), are given as follows,\\np\\n[PY∗ ] = {PY (·) : kPY (i) − PY∗ (i)k ≤ 4 1/n ∀i ∈ Y}\\nG(1) = {y n : Q(yn ) ∈ [PY∗ ]}.\\nSince there are at most (n + 1)|Y| different empirical output distribution for elements of Y n we get,\\n\\n[y n ∈\\n/ G(1)| M = 1] ≤ (n + 1)|Y| e−n minQY ∈[PY ] D(QY (·)kWY |X (·|xr ))\\n\\x01\\n(n) n\\n/\\n=1]\\n= D PY∗ (·)k WY |X (·|xr ) = C̃.\\nThus lim − ln Pr [y n∈G(1)|M\\nn→∞\\nNow the only thing we are left with to prove is that we can have low enough probability for the remaining\\nmessages. For doing that we will first calculate the average error probability of the following random code\\nensemble.\\nEntries of the codebook, other than the ones corresponding to the special message, are generated independently\\nusing a capacity achieving input distribution PX∗ . Because of the symmetry average error probability is same for\\nall i 6= 1 in M. Let us calculate the error probability of the message M = 2.\\nAssuming that the second message was transmitted, Pr [y n ∈ G(1)| M = 2] is vanishingly small. It is because,\\nthe output distribution for the random ensemble for ordinary codewords is i.i.d. PY∗ . Chebyshev’s inequality\\nPr\\n\\n11\\n\\n(n)\\n\\nThe claim in (b) is identical to the one in [13][Remark on page 34]\\n\\n∗\\n\\n\\x0c25\\n\\np\\n4\\nguarantees\\nthat\\nprobability\\nof\\nthe\\noutput\\ntype\\nbeing\\noutside\\na\\n1/n ball around PY∗ , i.e. [PY∗ ], is of the order\\np\\n1/n.\\nAssuming that the second message was transmitted, Pr [y n ∈ ∪i>2 G(i)| M = 2] is vanishingly small due to\\nthe standard random coding argument for achieving capacity [35].\\nThus for any Pe > 0 for all large enough n average error probability of the code ensemble is smaller than Pe\\nthus we have at least one code with that Pe . For that code at least half of the codewords have an error probability\\nless then 2Pe .\\n•\\n\\n2) Converse: Emd ≤ C̃: In the section VIII-D.2, we will prove that even with feedback and variable decoding\\ntime, the missed-detection exponent of a single special message is at most C̃. Thus Emd ≤ C̃.\\nC. Proof of Theorem 3\\n1) Achievability: Emd ≥ E(r):\\nProof:\\nSpecial codewords: At any given block length n, we start with a optimum codebook (say Cspecial ) for ⌈enr ⌉\\nmessages. Such optimum codebook achieves error exponent E(r) for every message in it.\\nh\\ni\\n.\\nPr M̂ 6= i|M = i = e−nE(r)\\n∀i ∈ Ms ≡ {1, 2, · · · , ⌈enr ⌉}\\nnr\\n\\n⌈e ⌉\\nSince there are at most (n + 1)|X | different types, there is at least one type TPX which has (1+n)\\n|X | or more\\ncodewords. Throw away all other codewords from Cspecial and lets call the remaining fixed composition codebook\\nas Cs′pecial . Codebook Cs′pecial is used for transmitting the special messages.\\nAs shown in Fig. 3(a), let the noise ball around the codeword for the special message i be Bi . These balls\\nneed not be disjoint. Let B denote the union of these balls of all special messages.\\n[\\nBi\\nB=\\ni∈Ms\\n\\nyn\\n\\nIf the output sequence\\n∈ B , the first stage of the decoder decides a special message was transmitted. The\\nsecond stage then chooses the ML candidate amongst the messages in Ms .\\nLet us define Bi precisely now.\\n\\nBi = {y n : V(y n , i) ∈ W(r + ǫ, PX )}\\n\\n\\x0c\\n\\x01\\nwhere W(r + ǫ, PX ) = {VY |X : D VY |X (·|X)\\n WY |X (·|X)\\x0c PX ≤ Esp (r + ǫ; PX )}. Recall that the spherepacking exponent for input type PX at rate r , Esp (r; PX ) is given by,\\n\\n\\x0c\\n\\x01\\nEsp (r; PX ) =\\nmin\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX\\nVY |X :I(PX ,VY |X )≤r\\n\\nOrdinary codewords: The ordinary codewords are generated randomly using a capacity achieving input distribution PX∗ . This is the same as Shannon’s construction for achieving capacity. The random coding construction\\nprovides a simple way to show that in the cavity B c (complement of B ), we can essentially fit enough typical\\nnoise-balls to achieve capacity. This avoids the complicated task of carefully choosing the ordinary codewords\\nand their decoding regions in the cavity, B c .\\nIf the output sequence y n ∈ B c , the first stage of the decoder decides an ordinary message was transmitted.\\nThe second stage then chooses the ML candidate from ordinary codewords.\\nError analysis: First, consider the case when a special codeword x̄n (i) is transmitted. By Stein’s lemma and\\ndefinition of Bi , the probability of y n ∈\\n/ Bi has exponent Esp (r + ǫ; PX ). Hence the first stage error exponent is\\nat least Esp (r + ǫ; PX ).\\n\\n\\x0c26\\n\\nAssuming correct first stage decoding, the second stage error exponent for special messages equals E(r).\\nHence the effective error exponent for special messages is\\nmin{E(r), Esp (r + ǫ; PX )}\\n\\nSince E(r) is at most the sphere-packing exponent Esp (r; PX ), [19], choosing arbitrarily small ǫ ensures that\\nmissed-detection exponent of each special message equals E(r).\\nNow consider the situation of a uniformly chosen ordinary codeword being transmitted. We have to make sure\\nthat the error probability is vanishingly small now. In this case, the output sequence\\ndistribution is i.i.d. PY∗ for\\nS\\nthe random coding ensemble. The first stage decoding error happens when y n ∈ Bi . Again by Stein’s lemma,\\nthis exponent for any particular Bi equals Eo :\\n\\n\\x01\\nEo =\\nmin\\nD VY |X (·|X)\\n PY∗ (·)| PX\\nVY |X ∈W(r+ǫ,PX )\\n\\n(a)\\n\\n=\\n\\n(b)\\n\\n≥\\n\\nmin\\n\\nI(PX , VY |X ) + D ((P V )Y (·)k PY∗ (·))\\n\\nmin\\n\\nI(PX , VY |X )\\n\\nVY |X ∈W(r+ǫ,PX )\\nVY |X ∈W(r+ǫ,PX )\\n\\n(c)\\n\\n≥r+ǫ\\n\\nP\\nwhere in (P V )Y in (a) is given by (P V )Y (j) = i PX (i)VY |X (j|i), (b) follows from the non-negativity of the\\nKL divergence and (c) follows from the definition of sphere-packing exponent and W(r + ǫ, PX ).\\nApplying union bound over the special messages, the probability of first stage decoding error after sending\\n.\\nan ordinary message is at most = exp(nr − nEo ). We have already shown that Eo ≥ r + ǫ, which ensures that\\n.\\nprobability of first stage decoding error for ordinary messages is at most = e−nǫ for the random coding ensemble.\\nRecall that for the random coding ensemble, average error probability of the second-stage decoding also vanishes\\nbelow capacity. To summarize, we have shown these two properties of the random coding ensemble:\\n.\\n1) Error probability of first stage decoding vanishes as a(n) = exp(−nǫ) with n when a uniformly chosen\\nordinary message is transmitted.\\n2) Error probability of second stage decoding (say b(n) ) vanishes with n when a uniformly chosen ordinary\\nmessage is transmitted.\\nSince the first error probability is at most 4a(n) for some 3/4 fraction of codes in the random ensemble, and the\\nsecond error probability is at most 4b(n) for some 3/4 fraction, there exists a particular code which satisfies both\\nthese properties. The overall error probability for ordinary messages is at most 4(a(n) +b(n) ), which vanishes with\\nn. We will use this particular code for the ordinary codewords. This de-randomization completes our construction\\nof a reliable code for ordinary messages to be combined with the code Cspecial for special messages.\\n•\\n\\n2) Converse: Emd ≤ E(r): The converse argument for this result is obvious. Removing the ordinary messages\\nfrom the code can only improve the error probability of the special messages. Even then, (by definition) the best\\nmissed detection exponent for the special messages equals E(r).\\nD. Proof of Theorem 4\\nLet us now address the case with erasures. In this achievability result, the first stage of decoding remains\\nunchanged from the no-erasure case.\\nProof:\\nWe use essentially the same strategy as before. Let us start with a good code for ⌈enr ⌉ messages allowing erasure\\ndecoding. Forney had shown in [18] that, for symmetric channels an error exponent equal to Esp (r) + C − r\\nis achievable while ensuring that erasure probability vanishes with n. We can use that code for these ⌈enr ⌉\\n\\n\\x0c27\\n\\nS\\ncodewords. As before, for y n ∈ i Bi , the first stage decides a special codeword was sent. Then the second stage\\napplies the erasure decoding method in [18] amongst the special codewords.\\nWith this decoding rule, when a special message is transmitted, error probability of the two-stage decoding is\\nbottle-necked by the first stage: its error exponent Esp (r+ǫ) is smaller than that of the second stage (Esp (r)+C−r ).\\nBy choosing arbitrarily small ǫ, the special messages can achieve Esp (r) as their missed-detection exponent.\\nThe ordinary codewords are again generated i.i.d. PX∗ . If the first stage decides in favor of the ordinary\\nmessages, ML decoding is implemented among ordinary codewords. If an ordinary message was transmitted, we\\ncan ensure a vanishing error probability as before by repeating earlier arguments for no-erasure case.\\n•\\n\\nVIII. VARIABLE L ENGTH B LOCK C ODES\\n\\nWITH\\n\\nF EEDBACK : P ROOFS\\n\\nIn this section we will present a more detailed discussion of bit-wise and message wise UEP for variable\\nlength block codes with feedback by proving the Theorems 5, 6, 7, 8 and 9. In the proofs of converse results we\\nneed to discuss issues related with the conditional entropy of the messages given the observation of the receiver.\\nIn those discussion we use the following notation for conditional entropy and conditional mutual information,\\nX\\nH(M |Y n ) = −\\nPr [M = i| Y n ] ln Pr [M = i| Y n ]\\ni∈M\\n\\nn\\n\\n\\x0c \\x03\\n\\x02\\nI (M ; Yn+1 |Y ) = H(M |Y n ) − E H(M |Y n+1 )\\x0c Y n .\\n\\nIt is worth noting that this notation is different from widely used one, which includes a further expectation\\nover the the conditioned variable. “H(M |Y n )” in the conventional notation, stands for the E [H(M |Y n )] and\\n“H(M |Y n = y n )” stands for H(M |Y n ).\\nA. Proof of Theorem 5\\n1) Achievability: Ebf ≥ C̃:\\nThis single special bit exponent is achieved using the missed detection exponent of a single special message,\\nindicating a decoding error for the special bit. The decoding error for the bit goes unnoticed when this special\\nmessage is not detected. This shows how feedback connects bit-wise UEP to message-wise UEP in a fundamental\\nmanner.\\nProof:\\nWe will prove that Ebf ≥ C̃ by constructing a capacity achieving sequence with feedback, Q, such that Ebf ,Q = C̃.\\nFor that let Q′ be a capacity achieving sequence such that Emd ,Q′ = C̃. Note that existence of such a Q′ is\\nguaranteed as a result of Theorem 2. We first construct a two phase fixed length block code with feedback and\\nerasures. Then using this we obtain the kth element of Q.\\nx0 and x1 , with distinct output distributions12 is send for\\n√In the first phase one of the two input symbols,\\n√\\n⌈ k⌉ time units depending on M1 . At time ⌈ k⌉ receiver makes tentative decision M̃1 on message M1 . Using\\nChernoff bound it can easily be shown that, [36, Theorem 5]\\nh\\ni\\n√\\nwhere µ > 0\\nPr M̃1 6= M1 ≤ e−µ k\\nh\\ni\\nActual value of µ, however, is immaterial to us we are merely interested in finding an upper bound on Pr M̃1 6= M1\\nwhich goes to zero as k increases.\\nIn the second phase transmitter uses the kth member of Q′ . The message in the second phase, M′ , is determined\\nby M2 depending on whether M1 is decoded correctly or not at the end of the first phase.\\nM̃1 6= M1 ⇒ M′ = 1\\n\\nM̃1 = M1 and M2 = i ⇒ M′ = i + 1 ∀i\\n12\\n\\nTwo input symbols x0 and x1 are such that W (·|x1 ) 6= W (·|x0 )\\n\\n\\x0c28\\n\\nAt the end of the second phase decoder decodes M′ using the decoder of Q′ . If the decoded message is one, i.e.\\nM̂′ = 1 then receiver declares an erasure, else M̂1 = M̃1 and M̂2 = M̂′ − 1.\\nNote that erasure probability of the two phase fixed length block code is upper bounded as\\nh\\ni\\nh\\ni\\n\\x0c\\n\\x02\\n\\x03\\nPr M̂′ = 1 ≤ Pr M̃1 6= M1 + Pr M′ = 1\\x0c M′ 6= 1\\n√\\n\\n≤ e−µ\\n\\nk\\n\\n+\\n\\n′\\n\\nM (k)\\nP ′(k)\\nM′ (k) −1 e\\n\\n(17)\\n\\nwhere Pe ′(k) is the error probability of the kth member of Q′ .\\nSimilarly we can upper bound the probabilities of two error events associated with the two phase fixed length\\nblock code as follows\\nh\\ni\\nPr M̂1 6= M1 , M̂′ 6= 1 ≤ Pe ′(k) (1)\\n(18)\\nh\\ni\\nˆ ′ 6= 1 ≤ M′ ′ (k) P ′(k) + P ′(k) (1)\\nPr M̂ 6= M , M\\n(19)\\ne\\nM (k) −1 e\\nwhere Pe ′(k) (1) is the conditional error probability of the 1st message in the kth element of Q′ .\\nIf there is an erasure the transmitter and the receiver will repeat what they have done again, until they get\\nˆ ′ 6= 1. If we sum the probabilities of all the error events, including error events in the possible repetitions we\\nM\\nget;\\nh\\ni Pr M̂ 6=M , M̂′ 6=1\\n]\\n[ 1 1\\n(20)\\nPr M̂1 6= M1 =\\n1−Pr[M̂′ =1]\\nh\\ni Pr M̂ 6=M , M̂′ 6=1\\n]\\n[\\nPr M̂ 6= M =\\n(21)\\n1−Pr[M̂′ =1]\\nNote that expected decoding time of the code is\\n\\nE [τ ] =\\n\\n√\\nk+⌈ k⌉\\n1−Pr[M̂′ =1]\\n\\n(22)\\n\\nUsing equations (17), (18), (19), (20), (21) and (22) one can conclude that the resulting sequence of variable\\nlength block codes with feedback, Q, is reliable. Furthermore RQ = C and Ebf ,Q = C̃.\\n•\\n2) Converse: Ebf ≤ C̃:\\nf\\nWe will use a converse result we have not proved yet, namely converse part of Theorem 8, i.e. Emd\\n≤ C̃.\\nProof:\\n(k)\\nConsider a capacity achieving sequence, Q, with message set sequence M(k) = {0, 1} × M2 . Using Q we\\nconstruct another capacity achieving sequence Q′ with a special message 0, with message set sequence M′ (k) =\\n(k)\\nf\\nf\\nf\\nf\\nf\\n{0} ∪ M2 such that Emd\\n,Q′ = Eb ,Q . This implies Eb ≤ Emd , which together with Theorem 8, Emd ≤ C̃, gives\\nus Ebf ≤ C̃.\\nLet us denote the message of Q by M and that of Q′ by M′ . The kth code of Q′ is as follow. At time 0 receiver\\nchooses randomly an M1 for kth element of Q and send its choice through feedback channel to transmitter. If\\nthe message of Q′ is not 0, i.e. M′ 6= 0 then the transmitter uses the codeword for M = (M1 , M′ ) to convey M′ .\\nIf M′ = 0 receiver pick a M2 with uniform distribution on M2 and uses the code word for M = (1 − M1 , M2 )\\nto convey that M′ = 0.\\nˆ ′ = i, if M̂ = (1 − M , i) then\\nReceiver makes decoding using the decoder of Q: if M̂ = (M1 , i) then M\\n1\\n′\\nM̂ = 0. One can easily show that expected decoding time and error probability of both of the codes are same.\\nFurthermore error probability of M1 in Q is equal to conditional error probability of message M′ = 0 in Q′ thus,\\nf\\nf\\n•\\nEmd\\n,Q′ = Eb ,Q .\\n\\n\\x0c29\\n\\nB. Proof of Theorem 6\\n\\x01\\nf\\n1) Achievability: Ebits\\n(r) ≥ 1 − Cr C̃:\\nProof:\\nWe will construct the capacity achieving sequence with feedback Q using a capacity achieving sequence Q′\\nsatisfying Emd ,Q′ = C̃, as we did in the proof of theorem 5. We know that such a sequence exists, because of\\nTheorem 8.\\nFor kth member of Q, consider the following two phase errors and erasures code. In the first phase transmitter\\nuses the ⌊rk⌋th element of Q′ to convey M1 . Receiver makes a tentative decision M̃1 . In the second phase\\ntransmitter uses the ⌊(C − r)k⌋th element of Q′ to convey M2 and whether M̃1 = M1 or not, with a mapping\\nsimilar to the one we had in the proof of theorem 5.\\nM̃1 6= M1 ⇒ M′ = 1\\n\\nM̃1 = M1 and M2 = i ⇒ M′ = i + 1 ∀i\\n(k)\\n\\n(k)\\n\\n(k)\\n\\nThus M1 = M′ (⌊rk⌋) and M2 ∪ {|M2 | + 1} = M′ (⌊(C−r)k⌋) . If we apply a decoding algorithm, like the\\none we had in the proof of theorem 5; going through essentially the same analysis\\nwith proof of Theorem 5, we\\n\\x01\\nf\\nr\\nC̃\\nand\\nrQ = r .\\n•\\ncan conclude that Q is a capacity achieving sequence and Ebits\\n=\\n1\\n−\\nC\\n,Q\\n\\x01\\nf\\n2) Converse: Ebits\\n(r) ≤ 1 − Cr C̃:\\nIn establishing the converse we will use a technique that was used previously in [4], together with lemma 1\\nwhich we will prove in the converse part Theorem 8.\\nProof:\\nConsider any variable length block code with feedback whose message set M is of the form M = M1 × M2 .\\nLet tδ be the first time instance that an i ∈ M1 becomes more likely than (1 − δ) and let τδ = tδ ∧ τ .\\nRecall that min WY |X (j|i) = λ consequently definition of τδ implies that min (1 − Pr [M1 = i| y τδ ]) ≥ λδ .\\ni,j\\n\\ni∈M1\\n\\nThus using Markov inequality for Pe we get,\\n\\nPr [τδ = τ ] ≤\\n\\nPe\\nλδ\\n\\n(23)\\n\\nWe use equation (23) to bound expected value of the entropy of first part of the message at time τδ as follows,\\n\\x02\\n\\x03\\n\\x02\\n\\x03\\nE [H(M1 |Y τδ )] = E H(M1 |Y τδ )I{τδ =τ } + E H(M1 |Y τδ )I{τδ <τ }\\n≤\\n\\nPe\\nλδ\\n\\nln |M1 | + (ln 2 + δ ln |M1 |)\\n\\n= ln 2 + ( Pλδe + δ) ln |M1 |)\\n\\nIt has already been established in, [4],\\n\\nE[H(M )−H(M |Y τδ )]\\nE[τδ ]\\n\\n≤C\\n\\n(24)\\n\\nThus,\\nE [τδ ] ≥ C1 (E [H(M ) − H(M1 |Y τδ ) − H(M2 |M1 , Y τδ )])\\n≥ C1 (− ln 2 + (1 −\\n\\nPe\\nλδ\\n\\n− δ) ln |M1 |)\\n\\n(25)\\n\\nBound given in inequality (25) specifies the time needed for getting a likely candidate, M̃1 . Like it was the case\\nin [4], remaining time is the time spend for confirmation. But unlike [4] transmitter needs to convey also M2\\nduring this time.\\nFor each realization of Y τδ divide the message set into disjoint subsets, Θ0 , Θ1 , . . . , Θ|M2 | as follows,\\nΘ0 = {l : l ∈ M, l = (i, j) where i 6= M̃1 (Y τδ )}\\nΘj = {l : l ∈ M, l = (M̃1 (Y τδ ), j)}\\n\\n∀j ∈ {1, 2, . . . |M2 |}\\n\\n\\x0c30\\n\\nwhere M̃1 (Y τδ ) is the most likely message given Y τδ . Furthermore let the auxiliary-message, M′ , be the index\\nof the set that M belongs to, i.e. M ∈ ΘM′ .\\nThe decoder for the auxiliary message decodes the index of the decoded message at the decoding time τ , i.e\\nM̂′ (Y τ ) = j ⇔ M̂ (Y τ ) ∈ Θj .\\nWith these definition we have;\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\n\\x0c\\nˆ ′ (Y τ ) 6= M′ \\x0c\\x0c Y τδ\\nPr M̂ (Y τ ) 6= M \\x0c Y τδ ≥ Pr M\\n\\x0c\\n\\x0c\\ni \\x02\\ni\\nh\\nh\\n\\x0c\\n\\x03\\n\\x0c\\n\\x0c\\nPr M̂1 (Y τ ) 6= M1 \\x0c Y τδ ≥ Pr M̂′ (Y τ ) 6= 0\\x0c Y τδ , M′ = 0 Pr M′ = 0\\x0c Y τδ .\\n\\nNow, we apply Lemma 1, which will be proved in section VIII-D.2. To ease the notation we use following\\nshorthand;\\n\\x0c\\nh\\ni\\n′\\n\\x0c\\nPeM {Y τδ } = Pr M̂′ (Y τ ) 6= M′ \\x0c Y τδ\\n\\x0c\\nh\\ni\\n′\\nˆ ′ (Y τ ) 6= 0\\x0c\\x0c Y τδ , M′ = 0\\nPeM {0, Y τδ } = Pr M\\n\\x0c\\n\\x03\\n\\x02\\nξ(Y τδ ) = Pr M′ (Y τδ ) = 0\\x0c Y τδ .\\nAs a result of Lemma 1, for each realization of y τδ ∈ Y τδ such that τδ < τ , we have\\n\\x11\\n\\x10\\n′\\n′\\nH(M′ |Y τδ )−ln 2−PeM {Y τδ } ln |M2 |\\nτδ\\n1\\n]\\nJ\\n≤\\nln\\n2\\n+\\nE\\n[τ\\n−\\nτ\\n|\\nY\\n(1 − ξ(Y τδ ) − PeM {Y τδ }) ln P M′ {0,Y\\nτδ\\nτδ\\nδ\\nE[τ −τδ |Y ]\\n}\\ne\\n\\nBy multiplying both sides of the inequality with I{τδ <τ } , we get an expression that holds for all Y τδ .\\n′\\n\\n1\\n≤\\nI{τδ <τ } (1 − ξ(Y τδ ) − PeM {Y τδ }) ln P M′ {0,Y\\nτδ\\n}\\ne\\n\\x11i\\nh\\n\\x10\\n′\\n′\\nτδ\\n2−PeM {Y τδ } ln |M2 |\\nI{τδ <τ } ln 2 + E [τ − τδ | Y τδ ] J H(M |Y )−ln\\nE[τ −τδ |Y τδ ]\\n\\nNow we take the expectation of both sides over Y τδ . For the right hand side we have,\\n\\x10\\nh\\x10\\ni\\n\\x11\\x11\\n′\\n′\\nτδ\\n2−PeM {Y τδ } ln |M2 |\\nR.H.S. = E ln 2 + E [τ − τδ | Y τδ ] J H(M |Y )−ln\\nI\\n{τδ <τ }\\nE[τ −τδ |Y τδ ]\\n\\x10\\ni\\nh\\n\\x11\\n′\\nτδ\\nM′\\nτδ\\n2−Pe {Y } ln |M2 |\\n≤ ln 2 + E E [τ − τδ | Y τδ ] J H(M |Y )−ln\\nI\\n{τδ <τ }\\nE[τ −τδ |Y τδ ]\\n\\x10\\nh\\ni\\x11\\n′\\n(a)\\n′\\nτδ\\n2−PeM {Y τδ } ln |M2 |\\n≤ ln 2 + E [τ − τδ ] J E I{τδ <τ } H(M |Y )−lnE[τ\\n−τδ ]\\n!\\ni\\nh\\n(b)\\n\\n≤ ln 2 + E [τ − τδ ] J\\n\\nE I{τ <τ } H(M′ |Y τδ ) −ln 2−Pe ln |M2 |\\nδ\\nE[τ −τδ ]\\n\\nE[τ −τδ |Y τδ ]I\\n\\n{\\nwhere (a) follows the concavity of J (·) and Jensen’s inequality when we interpret\\nE[τ −τδ ]\\nbility distribution over Y τδ \\x02and (b) follows the\\x03 fact that J (·) is a decreasing function.\\nNow we lower bound E I{τδ <τ } H(M′ |Y τδ ) in terms of E [H(M |Y τδ )]. Note that\\n\\x0c\\ni\\nh\\n\\x0c\\nH(M |Y τδ ) = H(M′ |Y τδ ) + Pr M1 6= M̃1 (Y τδ )\\x0c Y τδ H(M |M1 6= M̃1 (Y τδ ), Y τδ )\\n\\x0c\\ni\\nh\\n\\x0c\\n≤ H(M′ |Y τδ ) + Pr M1 6= M̃1 (Y τδ )\\x0c Y τδ ln |M1 ||M2 |\\n\\nτδ <τ }\\n\\n(26)\\n\\n(27)\\nas proba-\\n\\n\\x0c31\\n\\n\\x0c\\ni\\nh\\n\\x0c\\nFurthermore for all Y τδ such that τ > τδ , Pr M̃1 (Y τδ ) 6= M1 \\x0c Y τδ ≤ δ. Thus\\n\\x03\\n\\x03\\n\\x02\\n\\x02\\nE I{τδ <τ } H(M′ |Y τδ ) ≥ E I{τδ <τ } (H(M |Y τδ ) − δ ln |M1 ||M2 |)\\n\\x02\\n\\x03\\n= E (1 − I{τδ =τ } )H(M |Y τδ ) − δ ln |M1 ||M2 |\\n\\n≥ E [H(M |Y τδ )] − Pr [τδ = τ ] ln |M1 ||M2 | − δ ln |M1 ||M2 |\\n\\n(a)\\n\\n≥ E [H(M |Y τδ )] − ( Pλδe + δ) ln |M1 ||M2 |\\n\\n(b)\\n\\n≥(1 −\\n\\nPe\\nλδ\\n\\n− δ) ln |M1 ||M2 | − CE [τδ ]\\n\\n(28)\\n\\nwhere (a) follows from the inequality (23), (b) follows from the inequality (24). Since J (·) is decreasing in its\\nargument, inserting (28) in (27) we get\\n\\uf8f6\\n\\uf8eb\\n«\\n„\\nR.H.S. ≤ ln 2 + E [τ − τδ ] J \\uf8ed\\n\\nP\\nln |M1 ||M2 | 1− λδe −δ−Pe −E[τδ ]C−ln 2\\n\\uf8f8\\nE[τ −τδ ]\\n\\nNote that ∀a > 0, b > 0, C > 0,\\n\\x11\\n\\x10\\n\\x11 \\x10\\n\\x10\\na−Cx0\\na−Cx\\nd\\n|\\n=\\n−J\\n− C−\\n(b\\n−\\nx)J\\nx=x0\\ndx\\nb−x\\nb−x0\\n\\na−Cx0\\nb−x0\\n\\n(a)\\n\\n\\x11\\n\\nd\\ndx J\\n\\n(29)\\n\\n(x) |\\n\\na−Cx\\nx= b−x 0\\n0\\n\\n≤ −J (C)\\n\\nwhere (a) follows the concavity of J (·). Thus upper bound given in equation (29) is decreasing in E [τδ ]. Thus\\nusing the lower bound on E [τδ ], given in (23) we get,\\n\\uf8eb„\\n\\uf8f6\\n«\\nP\\n\\x11\\n\\x10\\n1− λδe −δ−Pe ln |M2 |−Pe ln |M1 |−2 ln 2\\n1|\\n\\uf8f8\\n+ lnC2 J \\uf8ed\\n(30)\\nR.H.S. ≤ ln 2 + E [τ ] − (1 − δ − Pλδe ) ln |M\\nC\\nP ln |M1 | ln 2\\nE[τ ]−(1−δ− λδe )\\n\\nC\\n\\n+ C\\n\\nNow let us consider the L.H.S. we get by taking the expectation of the inequality given in (26).\\ni\\nh\\n′\\n1\\nL.H.S. = E I{τδ <τ } (1 − ξ(Y τδ ) − PeM {Y τδ }) ln P M′ {0,Y\\nτδ\\n}\\ne\\nh\\ni\\n′\\ni\\nh\\n(a)\\nE\\nI{τ <τ } (1−ξ(Y τδ )−PeM {Y τδ })\\nM′\\nτδ\\nτδ\\nδ\\ni\\n≥ E I{τδ <τ } (1 − ξ(Y ) − Pe {Y }) ln h\\nτ\\nτ\\nτ\\nM′\\nM′\\n(b)\\n\\nE I{τ <τ } (1−ξ(Y\\nδ\\n\\nh\\n\\nδ )−P\\ne\\n\\n{Y\\n\\nδ })P\\ne\\n\\n{0,Y\\n\\nδ}\\n\\ni\\n\\n′\\n\\n1\\ni\\n≥ −e−1 + E I{τδ <τ } (1 − ξ(Y τδ ) − PeM {Y τδ }) ln h\\nE I{τ <τ } (1−ξ(Y τδ )−PeM′ {Y τδ )}PeM′ {0,Y τδ }\\nδ\\ni\\nh\\n′\\n1\\ni\\n≥ −e−1 + E I{τδ <τ } (1 − ξ(Y τδ ) − PeM {Y τδ }) ln h\\nτ\\nM′\\nE I{τ <τ } Pe {0,Y\\nδ\\n\\nδ}\\n\\nwhere (a) follows log sum inequality and (b) follows from the fact that x ln x ≥ −e−1 .\\nNote that\\ni\\nh ′\\ni\\nh\\n\\x03\\n\\x02\\n′\\nE I{τδ <τ } (1 − ξ(Y τδ ) − PeM {Y τδ }) ≥ E I{τδ <τ } (1 − ξ(Y τδ )) − E PeM {Y τδ }\\n\\x02\\n\\x03\\n≥ E I{τδ <τ } (1 − δ) − Pe\\n≥1−\\n\\nPe\\nλδ\\n\\n−δ\\n\\nwhere in last step we have used the equation (23). Furthermore\\n\\x0c\\nii\\ni\\nh\\nh\\nh\\n′\\n\\x0c\\nE I{τδ <τ } PeM {0, Y τδ } = E I{τδ <τ } Pr M̂1 = M̃1 \\x0c Y τδ , M̃1 6= M1\\n\\x0c\\n\\x0c\\nii\\ni h\\nh\\nh\\n\\x0c\\n\\x0c\\n1\\nE I{τδ <τ } Pr M̂1 = M̃1 \\x0c Y τδ , M̃1 6= M1 Pr M̃1 6= M1 \\x0c Y τδ\\n≤ δλ\\n≤\\n\\nPe M1\\nδλ\\n\\n(31)\\n\\n(32)\\n\\n(33)\\n\\n\\x0c32\\n\\nThus using equations (31), (32) and (33) we get\\nM1\\n\\ne\\nL.H.S. ≥ −e−1 − (1 − Pλδe − δ) ln Pλδ\\n√\\nf\\nUsing the inequalities (30), (34) and choosing δ = Pe we get Ebits\\n,Q ≤ 1 −\\n\\x01\\nf\\nr\\nimplies Ebits (r) ≤ 1 − C C̃.\\n\\n(34)\\nrQ \\x01\\nC\\n\\nJ (C). Since J (C) = C̃ this\\n•\\n\\nC. Proof of of Theorem 7\\n1) Achievability:\\nProof:\\nProof is very similar to the achievability proof for Theorem 6. Choose a capacity achieving sequence Q′ such\\nthat Ebf ,Q′ = C̃. The capacity achieving sequence with feedback, Q uses L elements of Q′ as follows.\\nFor the kth element of code Q, transmitter uses the ⌊k · r1 ⌋th element of Q′ to send the first part of the\\nmessage, M . In the remaining phases, l ≥ 2 transmitter uses ⌊k · r ⌋th element of Q′ . The special message of\\n1\\n\\nl\\n\\nthe code for phase l is allocated to the error event in previous phases.\\n\\n(M̃1 , . . . , M̃(l−1) ) 6= (M1 , . . . , M(l−1) ) ⇒ M′l = 1\\n\\n(M̃1 , . . . , M̃(l−1) ) = (M1 , . . . , M(l−1) ) ⇒\\n(k)\\n\\n(k)\\n\\nM′l\\n\\n∀l\\n\\n= Ml + 1\\n\\n∀l\\n\\n(k)\\n\\nThus M1 = M′ (⌊rk⌋) and for all l ≥ 1 Ml ∪ {|Ml | + 1} = M′ (⌊rl k⌋) . If for all l ∈ {2, 3, . . . , L}, M̂′ l 6= 1,\\nreceiver decodes all parts of the information, else it declares an erasure. We skip the error analysis because it is\\nessentially the same with Theorem 6.\\n•\\n2) Converse:\\nProof:\\nWe prove the converse of Theorem 7 by contradiction. Evidently\\nmax{Pe M1 , Pe M2 , . . . , Pe Mj } ≤ Pe M1 ,M2 ,...,Mj ≤ Pe M1 + Pe M2 + · · · + Pe Mj\\n\\n∀j ∈ {1, 2, . . . L}\\n\\nThus if there exists a scheme that can reachPan error exponent vector outside the region given in Theorem 7,\\ni\\nrj\\nthere is at least one Ei such that Ei ≥ (1 − j=1\\n)C̃. Then we can have two super messages as follows,\\nC\\nM′1 = (M1 , M2 , . . . , Mi )\\n\\nand M′2 = (Mi+1 , Mi+2 , . . . , Ml )\\n\\nRecall that Pe M1 ≤ Pe M2 ≤ · · · ≤ Pe Ml . Thus this new code is a capacity achieving code, whose special bits\\nf\\nf\\n′\\nhave rate rQ′ and Ebits\\n,Q′ > Ebits (rQ ). This is contradicting with the Theorem 6 we have already proved. Thus\\nall the achievable error exponent regions should lie in the region given in Theorem 7.\\n•\\nD. Proof of of Theorem 8\\nf\\n1) Achievability: Emd\\n≥ C̃:\\nNote that any fixed length block code without feedback, is also variable-length block code with feedback, thus\\nf\\nEmd\\n≥ Emd . Using the capacity achieving sequence we have used in the achievability proof of Theorem 2, we\\nf\\nget Emd\\n≥ C̃.\\n\\n\\x0c33\\nf\\n2) Converse: Emd\\n≤ C̃:\\nNow we prove that even with feedback and variable decoding time, the best missed detection exponent of a single\\nf\\nspecial message is less then or equal to C̃, i.e. Emd\\n≤ C̃. Since the set of capacity achieving sequences is a subset\\nof capacity achieving sequences with feedback and variable decoding time, this also implies that Emd ≤ C̃.\\nInstead of directly proving the converse part of Theorem 8 we first prove the following lemma.\\nLemma 1: For any variable length block code with feedback, message set M, initial entropy H(M ) and\\naverage error probability Pe , the conditional error probability of each message is lower bounded as follows,\\n«\\n«\\n„ „\\nH(M )−h(Pe )−Pe ln(|M|−1)\\n1\\n\\x0c\\nh\\ni\\nE[τ ]+ln 2\\n− 1−Pr[M =i]−P J\\n\\x0c\\nE[τ\\n]\\ne\\n∀i\\n(35)\\nPr M̂ 6= i\\x0c M = i ≥ e\\n\\nwhere J (R) is given by the following optimization over probability distributions on X\\n\\n\\n\\x01\\n\\x01\\nJ (R) =\\nmax1 2\\nαD (P 1 W )Y (·)\\n W (·|x1 ) + (1 − α)D (P 2 W )Y (·)\\n W (·|x2 ) (36)\\nα,x1 ,x2 ,PX ,PX :\\n1\\n2\\nαI(PX\\n,WY |X )+(1−α)I(PX\\n,WY |X )≥R\\n\\nIt is worthwhile remembering the notation we introduced previously that\\nX\\nX\\n(P i W )Y (·) =\\nPXi (j)WY |X (·|j) and I(PXi , WY |X ) =\\nj∈X\\n\\nj∈X ,k∈Y\\n\\nW\\n\\n(k|i)\\n\\n|X\\nPXi (j)WY |X (k|i) ln (P iYW\\n)Y (k)\\n\\nFirst thing to note about Lemma 1 is that it is not necessarily for the case of uniform probability\\non\\n\\x0c distribution\\nh\\ni\\n\\x0c\\nthe message set M. Furthermore as long as Pr [M = i] << 1 the lower bound on Pr M̂ 6= i\\x0c M = i depends\\non the a priori probability distribution of the messages only through the entropy of it, H(M ).\\nIn equation (36) α is simply a time sharing variable, which allows us to use a (xi , PXi ) pair with low mutual\\ninformation and high divergence together with another (xi , PXi ) pair with high mutual information and low\\ndivergence. As a result of Carathéodory’s Theorem we see that time sharing between two points of the form\\n(xi , PXi ) is sufficient for obtaining optimal performance, i.e. allowing time sharing between more than two points\\nof the form (xi , PXi ) will not improve the value of J (R).\\nIndeed for any R ∈ [0, C] one can use the optimizing values of α, x1 , x2 , PX1 and PX2 in a scheme like the one\\nin Theorem 2 with time sharing and prove that missed detection exponent of J (R) is achievable for a reliable\\nsequence of rate R. In that α determines how long the input letter x1 ∈ X is used for the special message while\\nPX1 is being used for the ordinary codewords. Furthermore arguments very similar to those of Theorem 8 can\\nbe used to prove no missed detection exponent higher than J (R) is achievable for reliable sequences of rate R.\\nThus J (R) is the best exponent a message can get in a rate R reliable sequence.\\nOne can show that J (R) is a concave function of R over its support [0, C]. Furthermore J (0) = Dmax and\\nJ (C) = C̃. Thus J (R) is a concave strictly decreasing function of R for 0 ≤ R ≤ C.\\nProof (of Lemma 1):\\nRecall that G(i) is the decoding region for M = i i.e. G(i) = {y τ : M̂ (y τ ) = i}. Then as a result of data\\nprocessing inequality for KL divergence we have\\ni\\nh\\ni\\nh\\nPr[G(i)]\\nPr[Y τ ]\\nPr[G(i)]\\nG(i)\\nln\\nE ln Pr[Y\\n≥\\nPr\\n[G(i)]\\nln\\n+\\nPr\\nτ |M =i]\\nPr[G(i)|M =i]\\nPr[G(i)|M =i]\\nh\\ni\\n1\\n≥ −h(Pr [G(i)]) + Pr G(i) ln\\nPr[G(i)|M =i]\\nh\\ni\\n1\\n(37)\\n≥ − ln 2 + Pr G(i) ln\\nPr[G(i)|M =i]\\nwhere in the last step we have used, the fact that h(Pr [G(i)]) ≤ ln 2. In addition\\n\\x0c\\ni\\ni\\nh\\nh\\n\\x0c\\nPr G(i) ≥ Pr G(i)\\x0c M 6= i Pr [M 6= i]\\nX\\n≥\\nPr [G(j)| M = j] Pr [M = j]\\nj6=i\\n\\n≥ (1 − Pe − Pr [M = i]) .\\n\\n(38)\\n\\n\\x0c34\\n\\nThus using the equations (37) and (38) we get\\n„\\n»\\n–«\\nPr[Y τ ]\\n\\x0c\\nh\\ni\\n1\\n− 1−P −Pr[M\\nln 2+E ln Pr[Y τ |M =i]\\n\\x0c\\n=i]\\ne\\nPr G(i)\\x0c M = i ≥ e\\n.\\n\\n(39)\\nh\\n\\nτ\\n\\ni\\n\\nPr[Y ]\\nNow we lower bound the error probability of the special message by upper bounding E ln Pr[Y\\nτ |M =i] . For that\\nlet us consider the following stochastic sequence,\\nn\\n\\x0c\\nh\\ni\\nX\\nPr[Yt |Y t−1 ] \\x0c t−1\\nPr[Y n ]\\nE\\nln\\n−\\nSn = ln Pr[Y\\nY\\n\\x0c\\nn |M =i]\\nPr[Yt |M =i,Y t−1 ]\\nt=1\\n\\nNote that E [Sn+1 | Y = Sn and since min Wi,j = λ we have E [|Sn+1 − Sn || Y n ] ≤ 2 ln λ1 . Thus Sn is a\\nmartingale, furthermore since E [τ ] < ∞ we can use [37, Theorem 2 p 487], to get\\nn]\\n\\nE [Sτ ] = S0 = 0.\\n\\nThus\\nh\\n\\nτ\\n\\ni\\n\\n]\\n=E\\nE ln Pr[YPr[Y\\nτ |M =1]\\n\\nNote that\\n\\n\" τ\\nX\\nt=1\\n\\n#\\n\\x0c\\nh\\ni\\nt−1\\n\\x0c t−1\\n]\\nt |Y\\nE ln Pr[YPr[Y\\n.\\nt−1 ] \\x0c Y\\nt |M =1,Y\\n\\n(40)\\n\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\nt−1\\n\\x0c t−1\\n]\\nPr[Yt |Y t−1 ] \\x0c t−1\\nt |Y\\nE ln Pr[YPr[Y\\n=\\nE\\nln\\n.\\nY\\nY\\n\\x0c\\n\\x0c\\nt−1 ]\\nWY |X (Yt |x̄t (1))\\nt |M =1,Y\\n\\nAs a result of definition of J (·) given in equation (36) we have,\\n\\x0c\\ni\\nh\\n\\x0c\\n\\x01\\x01\\n\\x0c\\nPr[Yt |Y t−1 ]\\nE ln Pr[Yt |M =1,Y t−1 ] \\x0c Y t−1 ≤ J I Xt ; Yt \\x0cY t−1\\n\\x0c\\n\\x01\\nwhere I Xt ; Yt \\x0cY t−1 is given by13\\n\\x0c\\ni\\nh\\n\\x0c t−1 \\x01\\n\\x0c\\nPr[Xt ,Yt |Y t−1 ]\\n\\x0c\\nI Xt ; Yt Y\\n= E ln Pr[Xt |Y t−1 ] Pr[Yt |Y t−1 ] \\x0c Y t−1\\n\\n(41)\\n\\nGiven Y t−1 random variables M − Xt − Yt forms a Markov chain. Thus\\n\\x0c\\n\\x0c\\n\\x01\\n\\x01\\nI Xt ; Yt \\x0cY t−1 ≥ I M ; Yt \\x0cY t−1 .\\n\\n(42)\\n\\nSince J (·) is a decreasing function, equations (40), (41) and (42) lead to\\n\" τ\\n#\\ni\\nh\\nX\\n\\x0c t−1 \\x01\\x01\\nPr[Y τ ]\\nJ I M ; Yt \\x0cY\\nE ln Pr[Y τ |M =1] ≤ E\\n\\n(43)\\n\\nt=1\\n\\nNote that\\n\\nE\\n\\n\" τ\\nX\\nt=1\\n\\n#\\n#\\n\" τ\\nX\\n\\x0c t−1 \\x01\\x01\\n\\x0c t−1 \\x01\\x01\\n1\\n\\x0c\\nJ I M ; Yt \\x0cY\\n=E τ\\nτ J I M ; Yt Y\\nt=1\\n\\n(a)\\n\\n\"\\n\\n≤ E τJ\\n\\n= E [τ ] E\\n\\nτ\\nX\\n\\n\" t=1\\n\\nτ\\nE[τ ] J\\n\\n(b)\\n\\n≤ E [τ ] J\\n\\n1\\nτI\\n\\nE\\n\\n\"\\n\\nτ\\nE[τ ]\\n\\n\\x0c\\n\\x01\\nM ; Yt \\x0cY t−1\\n\\nτ\\nX\\n\\nt=1\\nτ\\nX\\n\\n!#\\n\\n\\x0c t−1 \\x01\\n1\\n\\x0cY\\nI\\nM\\n;\\nY\\nt\\nτ\\n\\n!#\\n\\n#!\\n\\x0c\\n\\x01\\n1\\n\\x0c t−1\\nτ I M ; Yt Y\\n\\ni\\x11\\n\\x10 h Pτ t=1\\nt−1\\n)\\nt=i I(M ;Yt |Y\\n= E [τ ] J E\\nE[τ ]\\n\\n(44)\\n\\n˛\\n`\\n´\\nNote that unlike the conventional definition of conditional mutual information, I Xt ; Yt ˛Y t−1 is not averaged over the conditioned\\nt−1\\nrandom variable Y\\n.\\n13\\n\\n\\x0c35\\n\\nwhere in both (a) and (b) we use the the concavity of the J (·) function together with Jensen’s inequality. Thus\\nusing equations (39), (43) and (44) we get,\\n1\\n\\x0c\\nh\\ni\\n− 1−P −Pr[M =i]\\n\\x0c\\ne\\nPr M̂ 6= i\\x0c M = i ≥ e\\n\\n„ „ Pτ\\n«\\n«\\nE[ t=i I(M ;Yt |Y t−1 )]\\nJ\\nE[τ ]+ln 2\\nE[τ ]\\n\\nSince J (R) is decreasing in R, the only thing we are left to show is that\\n#\\n\" τ\\nX\\n\\x0c t−1 \\x01\\nI M ; Yt \\x0c Y\\n≥ H(M ) − h(Pe ) − Pe ln(|M| − 1)\\nE\\n\\n(45)\\n\\nt=i\\n\\nFor that consider the stochastic sequence,\\n\\nn\\n\\nVn = H(M |Y ) +\\nn]\\n\\nn\\nX\\nt=1\\n\\n\\x0c\\n\\x01\\nI M ; Yt \\x0cY t−1 .\\n\\nClearly E [Vn+1 | Y = Vn and E [|Vn |] < ∞, thus {Vn } is a martingale. Furthermore E [|Vn+1 − Vn || Y n ] ≤ K\\nand E [τ ] < ∞ thus using a version of Doob’s optional stopping theorem, [37, Theorem 2 p 487], we get,\\nV0 = E [Vτ ]\\nτ\\n\\n= E [H(M |Y )] + E\\n\\n\" τ\\nX\\nt=1\\n\\n#\\n\\x0c t−1 \\x01\\nI M ; Yt \\x0cY\\n.\\n\\n(46)\\n\\nOne can write Fano’s inequality as follows,\\n\\x0c i\\x11\\n\\x0c i\\n\\x10 h\\nh\\n\\x0c\\n\\x0c\\nH(M |Y τ ) ≤ h Pr M̂ (Y τ ) 6= M \\x0c Y τ + Pr M̂ (Y τ ) 6= M \\x0c Y τ ln(|M| − 1).\\n\\nConsequently\\n\\n\\x0c ii\\n\\x0c i\\x11i\\nh h\\nh \\x10 h\\n\\x0c\\n\\x0c\\n+ E Pr M̂ (Y τ ) 6= M \\x0c Y τ ln(|M| − 1).\\nE [H(M |Y τ )] ≤ E h Pr M̂ (Y τ ) 6= M \\x0c Y τ\\n\\nUsing the concavity of binary entropy,\\n\\nE [H(M |Y τ )] ≤ h(Pe ) + Pe ln(|M| − 1).\\n\\n(47)\\n\\nUsing equation (46) together with equation (47) we get the desired condition given in the equation (45).\\n•\\nAbove proof is for encoding schemes which does not have any randomization (time sharing), but same ideas can\\nbe used to establish the exact same result for general variable length block codes with randomization. Now we\\nare ready to prove the converse part of the Theorem 8.\\nProof (of Converse part of Theorem 8):\\nf\\nIn order to prove Emd\\n≤ C̃, first note that for capacity achieving sequences we consider Pr [M = i] = |M1(k) | .\\nThus\\n\\x11\\n\\x11\\n\\x10 \\x10\\nM\\n(k)\\nln |M(k) |−h(Pe (k))−Pe (k) ln(|M(k)|−1)\\ne (i))\\nln 2\\n1\\n+\\n(48)\\n≤\\nJ\\n− ln(PE[τ\\n(k) ]\\n1\\nE[τ (k) ]\\nE[τ (k) ] .\\n(k)\\n1−Pe\\n\\n− |M(k) |\\n\\nThus for any capacity achieving sequence with feedback,\\nM\\n\\n(k)\\n\\ne (i))\\nlim − ln(PE[τ\\n(k) ]\\n\\nk→∞\\n\\n≤ J (C) = C̃.\\n•\\n\\n\\x0c36\\n\\nE. Proof of of Theorem 9\\nIn this subsection we will show how the strategy for sending a special bit can be combined with the YamamotoItoh strategy when many special messages demand a missed-detection exponent. However unlike previous results\\nabout capacity achieving sequences, Theorems 5, 6, 7, 8, we will have and additional uniform delay assumption.\\nWe will restrict ourself to uniform delay capacity achieving sequences.14 Clearly capacity achieving sequences\\nin general need not to be uniform delay. Indeed many messages, i ∈ M, can get an expected delay, E [τ | M = i]\\nmuch larger than the average delay, E [τ ]. This in return can decrease the error probability of these messages.\\nThe potential drawback of such codes, is that their average delay is sensitive to assumption of messages being\\nchosen according to a uniform probability distribution. Expected decoding time, E [τ ], can increase a lot if the\\ncode is used in a system in which the messages are not chosen uniformly.\\nf\\nIt is worth emphasizing that all previously discussed exponents (single message exponent Emd\\n, single bit\\nf\\nf\\nexponent Eb , many bits exponent Eb (r) and achievable multi-layer exponent regions) remain unchanged whether\\nor not this uniform delay constraint is imposed. Thus the flexibility to provide different expected delays to different\\nmessages does not improve those exponents.\\nHowever, this is not true for the message-wise UEP with exponentially many messages. Removing the uniform\\nC̃\\ndelay constraint can considerably enhance the protection of special messages at rate higher than (1 − Dmax\\n)C.\\nIndeed one can make the exponent of all special messages, C̃. The flexibility of providing more resources\\n(decoding delay) to special messages achieves this enhancement. However, we will not discuss those cases in\\nthis article and stick to uniform delay codes.\\nf\\n1) Achievability: Emd\\n(r) ≥ min{C̃, (1 − Cr )Dmax }:\\nThe optimal scheme here reverses the trick for achieving Ebf : first a special bit tells to the receiver whether the\\nmessage being transmitted is special one or not. After the decoding of this bit the message itself is transmitted. This\\nfurther emphasizes how feedback connects bit-wise and message-wise UEP, when used with variable decoding\\ntime.\\nProof:\\nLike all the previous achievability results, we construct a capacity achieving sequence, Q, with the desired\\nasymptotic behavior. A sequence of multi phase fixed length errors and erasures codes, Q′ is used as the building\\nblock of Q. Let us consider the kth member √\\nof Q′ . In the first phase transmitter sends one of the two input\\n(k)\\nsymbols with distinct output distributions for ⌊ k⌋ time units in order to tell whether M ∈ Ms or not. Let b\\nbe b = I{M ∈Ms(k) } .Then, as it was mentioned in subsection VIII-A.1, with a threshold decoding we can achieve\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\n√\\n\\x0c\\n\\x0c\\nPr b̂ 6= 1\\x0c b = 1 = Pr b̂ 6= 0\\x0c b = 0 ≤ e− kµ\\nwhere µ > 0.\\n(49)\\nActual value of µ is not important for us, we are merely interested in an upper bound vanishing with increasing\\nk.\\nIn the second phase one of two length k codes is used depending on b̂.\\n• If b̂ = 0, in the second phase, transmitter uses the k th member of a capacity achieving sequence, Q′′ such\\nthat Eb ,Q′′ = C̃. We know that such a sequence exists because of Theorem 2. The message, M′ of the Q′′\\nis determined using the following mapping\\nM ∈ Ms ⇒ M′ = 1\\n\\nM ∈\\n/ Ms ⇒ M′ = M − |Ms | + 1\\nˆ ′ = 1, then receivers declares an erasure, M̃ =\\nAt the end of the second phase, receiver decodes M′ . If M\\nerasure. If M̂′ 6= 1, then M̂ = M̃ = M̂′ + |Ms | − 1.\\n14\\n\\nRecall that for any reliable variable length block code with feedback Γ is defined as Γ =\\n\\nreliable sequences are the ones that satisfy\\n\\n(k)\\nlimk→∞ ΓQ\\n\\n= 1.\\n\\nmaxi∈M E[τ |M =i]\\nE[τ ]\\n\\nand uniform delay\\n\\n\\x0c37\\n\\n•\\n\\nIf b̂ = 1, transmitter uses a two phase code with errors and erasures in the second phase, like the one\\ndescribed by Yamamoto and Itoh in [40]. The two phases of this code are called communication and control\\nphases, respectively.\\nIn communication phase transmitter uses ⌈rk⌉th member of a capacity achieving sequence, Q′′ with Eb ,Q′′ =\\nC̃, to convey its message, M′ . The auxiliary message M′ is determined as follows,\\nM ∈\\n/ Ms ⇒ M′ = 1\\n\\nM ∈ Ms ⇒ M′ = M + 1\\n\\nThe decoded message of the ⌈rk⌉th member of Q′′ is called the tentative decision of communication phase\\nand denoted by M̃′ . In the control phase,\\n– if M̃′ = M′ tentative decision is confirmed by sending accept symbol xa for ℓ(k) = k − ⌈ Cr k⌉ time\\nunits.\\n˜ ′ 6= M′ tentative decision is rejected by sending reject symbol x for ℓ(k) = k − ⌈ r k⌉ time units.\\n– if M\\nd\\nC\\nwhere xa and xd are the maximizers in the following optimization problem.\\n\\n\\n\\x01\\n\\x01\\nDmax = max D WY |x (·|i)\\n WY |X (·|j) = D WY |x (·|xa )\\n WY |X (·|xd )\\ni,j\\n\\nIf the output sequence in last k − ⌈ Cr k⌉ time steps is typical with WY |X (·|xa ) then M̂′ = M̃′ else erasure is\\ndeclared for M′ . Note that the total probability of WY |X (·|xa ) typical sequences are less than e−ℓ(k)(Dmax −δℓ(k) )\\nwhen M̃′ 6= M′ and more than 1 − δℓ(k) when M̃′ = M′ where lim δℓ(k) = 0, [13, Corrollary 1.2, p19].\\nℓ(k)→∞\\n\\nˆ ′ = erasure or if M\\nˆ ′ = 1 then receiver declares erasure for M , M̃ = erasure. If M\\nˆ ′ ∈ {2, 3, . . . , |M |+1},\\nIf M\\ns\\nthen M̂ = M̃ = M̂ − 1.\\nNow we can calculate the error and erasure probabilities of the two phase fixed length block code. Let us denote\\nthe erasures by M̃ = erasure for each k.\\nFor i ∈ Ms using the equation (49) and Bayes rule we get\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\n(k−ℓ(k))\\n+ δℓ(k) )\\n(50)\\nPr M̃ = erasure\\x0c M = i ≤ e−µ k + (Pe ,Q′\\n\\x0c\\ni\\nh\\n√\\n\\x0c\\n(k−ℓ(k)) −ℓ(k)(Dmax −δℓ(k) )\\nPr M̃ 6= i, M̃ 6= erasure\\x0c M = i ≤ e−µ k Pe kQ′ (1) + Pe ,Q′\\ne\\n.\\n(51)\\n\\nFor i ∈\\n/ Ms using the equation (49) and Bayes rule we get\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\n(k)\\nPr M̃ = erasure\\x0c M = i ≤ e−µ k + Pe ,Q′\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\n(k)\\nPr M̃ 6= i, M̃ 6= erasure\\x0c M = i ≤ e−µ k + Pe ,Q′ .\\n\\n(52)\\n(53)\\n\\nWhenever M̃ = erasure than transmitter and receiver try to send the message once again from scratch using\\nsame strategy. Then for any i ∈ M\\n\\x0c\\nh\\ni Pr M̃ 6=i,M̃ 6=erasure M =i\\n|\\n]\\n[\\n\\x0c\\n(54)\\nPr M̂ 6= i\\x0c M = i = 1−Pr M̃ =erasure M =i\\n|\\n]\\n[\\nE [τ | M = i] =\\n\\n√\\nk+ k\\n1−Pr[M̃ =erasure|M =i]\\n\\n(55)\\n\\nUsing equations (50), (51), (52), (53), (54) and (55) we conclude that that Q is capacity achieving sequence such\\nthat\\nln maxi∈Ms Pr[M̃ 6=i,M̂ 6=erasure|M =i]\\n= min{C̃, (1 − Cr )Dmax }\\nlim −\\nE[τ ]\\nk→∞\\n\\nln |Ms(k) |\\nk→∞ E[τ ]\\n\\nlim\\n\\n=r\\n•\\n\\n\\x0c38\\nf\\n2) Converse: Emd\\n(r) ≤ min{C̃, (1 − Cr )Dmax }:\\nProof:\\n(k)\\nConsider any uniform delay capacity achieving sequence, Q. Note that by excluding all i ∈\\n/ Ms we get a\\nreliable sequence, Q′ such that\\n\\x0c\\ni\\nh\\n′\\n\\x0c\\nPe (k) ≤ Pr (k) M̂ 6= M \\x0c M ∈ Ms\\nh ′ i\\nh\\ni\\nE τ (k) ≤ Γ(k) E τ (k)\\n\\nThus\\n\\n(k)\\n\\n− ln Pr[M̂ 6=M |M ∈Ms ]\\nE[τ (k) ]\\n\\n′ (k)\\n\\nPe\\n≤ − ln\\nΓ(k)\\nE [τ ′ (k) ]\\n\\nf\\nConsequently Emd\\n(r) ≤ (1 − Cr )Dmax . Similarly by excluding all but one of the elements of Ms we can prove\\nf\\nthat Emd\\n(r) ≤ C̃, using Theorem 8 and uniform delay condition.\\n•\\n\\nIX. AVOIDING FALSE A LARMS : P ROOFS\\nA. Block Codes without Feedback: Proof of Theorem 10\\n1) Lower Bound: Efa ≥ Efal :\\nProof:\\nAs a result of the coding theorem [13, Ch. 2 Corollary 1.3, page 102 ] we know that there exits a reliable\\n(n)\\nsequence Q′ of fixed composition codes whose rate is C and whose nth elements composition PX satisfies,\\nq\\nX (n)\\n|PX (i) − PX∗ (i)| ≤ 4 n1 .\\ni∈X\\n\\nWe use the codewords of the nth element of Q′ as the codewords of the ordinary messages in the nth code in\\nQ. For the special message we use a length-n repetition sequence x̄n (1) = (xfl , xfl , · · · , xfl ).\\nThe decoding region for the special message is essentially the bare minimum. We include the typical channel\\noutputs within the decoding region of the special message to ensure small missed detection probability for the\\nspecial message, but we exclude all other output sequence y n .\\nX\\np\\nG(1) = {y n :\\n|Q(yn ) (i) − WY |X (i|xfl )| ≤ 4 1/n}\\ni∈Y\\n\\nNote that this\\nof G(1)\\nitself ensures that special message is transmitted reliably whenever it is sent,\\n\\x0c\\nh definition\\ni\\n\\x0c\\n(n)\\nlim Pr\\nM̂ 6= 1\\x0c M = 1 = 0.\\nn→∞\\n\\nThe decoding regions of the ordinary messages, j = {2, 3, . . . M(n) }, is the intersection of the corresponding\\ndecoding region in Q′ with the complement of G(1). Thus the fact that Q′ is a reliable sequence implies that,\\n\\x0c\\n\\uf8ee\\n\\uf8f9\\n\\x0c\\n[\\n\\x0c\\nlim Pr (n) \\uf8f0y n ∈\\nG(j)\\x0c\\x0c M = i\\uf8fb = 0\\nn→∞\\n\\x0c\\nj ∈{1,i}\\n/\\n\\nConsequently we have reliable communication for ordinary messages as long as lim Pr (n) [G(1)| M = j] = 0,\\n\\x0c n→∞i\\nh\\n\\x0c\\n∀j 6= 1. But we prove a much stronger result to ensure that Pr (n) M̂ = 1\\x0c M 6= 1 is decaying fast enough.\\nBefore doing that let us note that in the second stage of the decoding, when we are choosing a message among\\nthe ordinary ones, ML decoder can be used instead of the decoding rule of the original code. Doing that will\\nonly decrease the average error probability.\\n\\n\\x0c39\\n\\nNote the probability of a V -shell of a message i is equal to,\\nPr\\n\\n(n)\\n\\n(n)\\n[TV (i)| M = i] = e−nD(VY |X (·|X)kWY |X (·|X)|PX )\\n\\nNote that also that G(1) can be written as the union of V -shells of a message i as follow.\\n[\\nG(1) =\\nTV (i)\\n∀i 6= 1\\nVY |X ∈V (n)\\n\\nwhere V (n) = {VY |X : j | k VY |X (j|k)PXn (k) − WY |X (j|xfl )| ≤\\n(1 + n)|X ||Y| different conditional types.\\nP\\n\\nPr\\n\\nP\\n\\n(n)\\n\\n[G(1)| M = i] ≤ (1 + n)|X ||Y|\\n\\nmax\\n\\nVY |X ∈V (n)\\n\\np\\n4\\n\\n1/n}. Note that since there are at most\\n\\nPr [TV (i)| M = i]\\n\\nThus for all i > 1\\nlim\\n\\nn→∞\\n\\n− ln Pr\\n\\n(n)\\n\\n[G(1)|M =i]\\nn\\n\\n=\\n\\nVY |X :\\n\\nP\\n\\nmin\\n\\n∗\\nj PX (j)VY |X (·|j)=WY |X (·|xfl )\\n\\n\\n\\x0c\\n\\x01\\nD VY |X (·|X)\\n WY |X (·|X)\\x0c PX∗\\n\\n2) Upper Bound: Efa ≤ Efau :\\nProof:\\nAs a result of data processing inequality for KL divergence we have\\n\\x0c\\ni Pr G(1) M =1\\nh\\nX\\n[ |\\n]\\n\\x0c\\nPr[y n |M =1]\\nPr[G(1)|M =1]\\nn\\nPr [y | M = 1] ln Pr[yn |M 6=1] ≥ Pr [G(1)| M = 1] ln Pr[G(1)|M 6=1] Pr G(1)\\x0c M = 1 ln\\nPr[G(1)|M 6=1]\\nn\\nn\\ny ∈Y\\n\\n≥ − ln 2 − Pr [G(1)| M = 1] ln Pr [G(1)| M 6= 1]\\n\\n•\\n\\n(56)\\n\\nUsing the convexity of the KL divergence we get\\nX\\n\\ny n ∈Y n\\n\\nn\\n\\nPr [y | M = 1] ln\\n\\nPr[y n |M =1]\\nPr[y n |M 6=1]\\n\\n≤\\n=\\n\\n|M|\\nX\\ni=2\\n\\n|M|\\n\\nX\\ni=2\\n\\n=\\n\\n1\\n|M|−1\\n1\\n|M|−1\\n\\n|M|\\nn X\\nX\\nk=1 i=2\\n\\nn\\n\\nX\\n\\n|M =1]\\nPr [y n | M = 1] ln Pr[y\\nPr[y n |M =i]\\n\\nX\\n\\nPr [y n | M = 1]\\n\\ny n ∈Y n\\n\\ny n ∈Y n\\n\\n1\\n|M|−1 D\\n\\nn\\nX\\nk=1\\n\\nk−1\\n\\n]\\nk |M =1,y\\nln Pr[y\\nPr[yk |M =i,y k−1 ]\\n\\n\\n\\x01\\nWY |X (·|x̄k (1))\\n WY |X (·|x̄k (i))\\n\\n(57)\\n\\nwhere x̄k (i) denotes the input letter for codeword of message i, at time k.\\nLet us denote the empirical distribution of the x̄k (i) for time k, by PXk .\\nPXk (i) =\\n\\nP\\n\\nI{x̄ (j)=i}\\nk\\n|M|\\n\\nj∈M\\n\\n∀i ∈ X\\n\\nUsing equation (56) and (57) we get\\n1\\n− Pr[G(1)|M =1]\\n\\nPr [G(1)| M 6= 1] ≥ e\\n\\n„\\n\\n«\\n|M| P\\nD\\nW\\n(\\nY |X (·|x̄k (1))kWY |X (·|Xk )|PXk )+ln 2\\nk\\n|M|−1\\n\\n(58)\\n\\nWe show below that for all capacity achieving codes, almost all of the k’s has a PXk which is essentially equal\\nto PX∗ . For doing that let us first define the set P (ǫ) and δ(ǫ)\\nX\\nP (ǫ) , {PX : I(PX , WY |X ) ≥ C − ǫ} and δ(ǫ) , max\\n|PX (i) − PX∗ (i)|\\nPX ∈P(ǫ)\\n\\ni\\n\\n\\x0c40\\n\\nNote that lim δ(ǫ) = 0. As a result of Fano’s inequality we have,\\nǫ→0\\n\\nI (M ; Y n ) ≥ nR(n) (1 − Pe ) − ln 2\\n\\n(59)\\n\\nOn the other hand using standard manipulations on mutual information we get\\nI (M ; Y n ) =\\n\\nn\\nX\\nk=1\\n\\nI(PXk , WY |X )\\n\\n≤ Cn − ǫ\\n\\nUsing equation (60) in equation (59) we get,\\nn\\nX\\nk=1\\n\\nLet ǫ(n) be ǫ(n) =\\n\\nq\\n\\nn\\nX\\nk=1\\n\\nI{PX ∈P(ǫ)\\n/\\n}\\nk\\n\\n(n)\\n\\n(C−R\\nI{PX ∈P(ǫ)\\n/\\n}≤n\\nk\\n\\nC − R(n) (1 − Pe ) −\\n\\nln 2\\nn ,\\nn\\nX\\nk=1\\n\\n(60)\\n\\n(1−Pe )−ln 2/n)\\nǫ\\n\\nthen lim ǫ(n) = 0 and\\nn→∞\\n\\n(n)\\nI{PX ∈P(ǫ\\n(n) )\\n/\\n} ≤ nǫ .\\nk\\n\\n\\x01\\nNote for any PX ∈ P ǫ(n) we have\\n\\n\\n\\x0c\\n\\x0c\\n\\x01\\n\\x01\\nD WY |X (·|xk (1))\\n WY |X (·|Xk )\\x0c PX ≤ D WY |X (·|xk (1))\\n WY |X (·|X)\\x0c PX∗ + δ(ǫ(n) )Dmax\\n\\n≤ Efau + δ(ǫ(n) )Dmax\\n\\n\\x0c\\n\\x01\\nwhere Efau = maxi∈X D WY |X (·|i)\\n WY |X (·|X)\\x0c PX∗\\nUsing equations (61) and (62)\\nX\\n\\n\\x0c\\n\\x01\\nD WY |X (·|xk (1))\\n WY |X (·|Xk )\\x0c PXk ≤ n(Efau + δ(ǫ(n) )Dmax + ǫ(n) Dmax )\\n\\n(61)\\n\\n(62)\\n\\nk\\n\\nInserting this in equation (58) we get\\n\\nlim\\n\\nn→∞\\n\\n\\x10\\n\\n− ln Pr\\n\\n(n)\\n\\n[G(1)|M 6=1]\\nn\\n\\n\\x11\\n\\n≤ Efau\\n•\\n\\nB. Variable Length Block Codes with Feedback: Proof of Theorem 11\\n1) Achievability: Efaf ≥ Dmax :\\nProof:\\nWe construct a capacity achieving sequence with feedback, Q, by using a construction like the one we have for\\nf\\nEmd\\n(r). In fact, this scheme achieves the false alarm exponent simultaneously with the best missed detection\\nexponent, C̃, for the special message.\\nWe use a fixed length multi-phase errors and erasure √\\ncode as the building block for the kth member of Q. In\\nthe first phase, b = I{M =1} is conveyed using a length ⌈ k⌉ repetition code, like we did in subsections VIII-A.1\\nand VIII-E.1. Recall that\\n\\x0c\\n\\x0c\\nh\\ni\\nh\\ni\\n√\\n\\x0c\\n\\x0c\\nµ>0\\n(63)\\nPr b̂ 6= 1\\x0c b = 1 = Pr b̂ 6= 0\\x0c b = 0 ≤ e−µ k\\n\\nIn the second phase one of the two length k codes is used depending on b̂.\\n\\n\\x0c41\\n\\n•\\n\\nIf b̂ = 0, transmitter uses the kth member of a capacity achieving sequence, Q′ such that Emd ,Q′ = C̃ to\\nconvey the message. We know that such a sequence exists because of Theorem 2. Let the message of Q be\\nthe message of Q′ , i.e. the auxiliary message,\\nM′ = M .\\n\\n•\\n\\nˆ ′ = 1, receiver declares an erasure, M̃ = erasure, else M is decoded\\nIf at the end of the second phase M\\n′\\nˆ\\nM̂ = M̃ = M .\\nIf b̂ = 1, transmitter uses a length k repetition code to convey whether M = 1 or not.\\n– If M = 1, M′ = 1 and transmitter sends the codeword (xa , xa , . . . , xa ).\\n– If M 6= 1, M′ = 0 and transmitter sends the codeword (xd , xd , . . . , xd ).\\nwhere xa and xd are the maximizers achieving Dmax :\\n\\n\\n\\x01\\n\\x01\\nDmax = max D WY |x (·|i)\\n WY |X (·|j) = D WY |x (·|xa )\\n WY |X (·|xd )\\ni,j\\n\\nˆ ′ = 1 only when output sequence is typical with W\\nReceiver decodes M\\nY |X (·|xa ). Evidently as before we\\nhave, [13, Corrollary 1.2, p19].\\n\\x0c\\nh\\ni\\nˆ ′ = 0\\x0c\\x0c M = 1 ≤ δ\\nPr M\\n(64)\\nk\\n\\x0c\\nh\\ni\\n\\x0c\\nPr M̂′ = 1\\x0c M = 0 ≤ e−k(Dmax −δk )\\n(65)\\n\\nwhere lim δk = 0.\\nk→∞\\n\\nIf M̂′ = 1 then M̂ = 1, else receiver declares erasure for the whole block, i.e. M̃ = erasure.\\nNow we can calculate the error and erasure probabilities for (⌈k⌉ + k) long block code. Using the equations\\n(63), (64), (65) and Bayes’ rule we get\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\nPr M̃ = erasure\\x0c M = 1 ≤ e−µ k + δk\\n(66)\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\n(k)\\ni 6= 1\\n(67)\\nPr M̃ = erasure\\x0c M = i ≤ e−µ k + Pe Q′\\n\\x0c\\nh\\ni\\n√\\n\\x0c\\n(k)\\nPr M̃ ∈ M \\\\ {1}\\x0c M = 1 ≤ e−µ k Pe Q′ (1)\\n(68)\\n\\x0c\\nh\\ni\\n\\x0c\\n(k)\\nPr M̃ ∈ M \\\\ {1, i}\\x0c M = i ≤ Pe Q′\\ni 6= 1\\n(69)\\n\\x0c\\ni\\nh\\n√\\n\\x0c\\nPr M̃ = 1\\x0c M = i ≤ e−µ k e−k(Dmax −δk )\\ni 6= 1\\n(70)\\n\\nWhenever M̃ = erasure than transmitter tries to send the message again from scratch, using same strategy.\\n1\\nwhen we consider\\nConsequently all of the above error probabilities are scaled by a factor of 1−Pr M̃ =erasure\\n|M =i]\\n[\\nthe corresponding error probabilities for the variable decoding time code. Furthermore\\nE [τ | M = i] =\\n\\n√\\nk+ k\\n1−Pr[M̃ =erasure|M =i]\\n\\n(71)\\n\\nUsing equations (66), (67), (68), (69), (70) and (71) we conclude that Q is a capacity achieving code with\\nf\\nf\\nEmd\\n•\\n,Q = C̃ and Efa ,Q = Dmax .\\n\\n\\x0c42\\n\\n2) Converse: Efaf ≤ Dmax :\\nProof:\\nNote that as result of convexity of KL divergence we have\\n\\x0c\\n\\x0c\\nh\\nh\\ni\\ni Pr G(1) M =1\\n]\\n[ |\\n\\x0c\\nPr[Y τ |M =1] \\x0c\\nPr[G(1)|M =1]\\nE ln Pr[Y τ |M 6=1] \\x0c M = 1 ≥ Pr [G(1)| M = 1] ln Pr[G(1)|M 6=1] + Pr G(1)\\x0c M = 1 ln\\nPr[G(1)|M 6=1]\\n1\\n≥ − ln 2 + Pr [G(1)| M = 1] ln Pr[G(1)|M\\n6=1]\\nIt has already been proved in [4] that,\\n\\x0c\\nh\\ni\\nPr[Y τ |M =1] \\x0c\\nE ln Pr[Y\\n≤ Dmax E [τ | M = 1]\\nτ |M 6=1] \\x0c M = 1\\n\\n(72)\\n\\n(73)\\n\\nNote that as a result of definition of Γ we have E [τ | M = 1] ≤ E [τ ] Γ using this together with equations (72)\\nand (73) the we get,\\nln 2+ΓDmax E[τ ]\\n− Pr[G(1)|M =1]\\n\\nPr [G(1)| M 6= 1] ≥ e\\n\\nThus for any uniform delay reliable sequence, Q, we have Efaf ,Q ≤ Dmax .\\n\\n•\\n\\nA PPENDIX\\nA. Equivalent definitions of UEP exponents\\nWe could have defined all the UEP exponents in this paper without using the notion of capacity achieving\\nsequences. As an example in this section we define the single-bit exponent in this alternate manner and show\\nthat both definitions leads to identical results. In this alternative first Ēb (R) is defined as the best exponent for\\nthe special bit at a given data-rate R and then it is minimized over all R < C to obtain Ēb .\\nDefinition 14: For any R ≥ 0, Z(R) is the set of sequence of codes, Q, with message sets M(n) such that\\n|M(n) | ≥ eRn\\n\\nand\\n\\n(n)\\n\\nM(n) = M1 × M2\\n\\nwhere M1 = {0, 1}.\\nh\\ni\\nDefinition 15: For a sequence of codes, Q, such that lim Pr (n) M̂ 6= M = 0, singe bit exponent Eb ,Q\\nn→∞\\nequals\\n− ln Pr (n) [M̂1 6=M1 ]\\nEb ,Q , lim inf\\n.\\n(74)\\nn\\nn→∞\\n\\nDefinition 16: Ēb (R) and the single bit exponent Ēb are defined as\\nĒb (R) ,\\n\\nsup Eb ,Q\\n\\nQ∈Z(R)\\n\\nĒb , inf Ēb (R).\\nR<C\\n\\nNote that according to this definition the special bit can achieve the exponent Ēb , no matter how close the rate is\\nto capacity. We now show why this definition is equivalent to the earlier definition in terms of capacity achieving\\nsequences given in section III.\\nLemma 2: Ēb = Eb\\nProof:\\nEb ≤ E¯b :\\nBy definition of Eb , for any given δ > 0, there exists a capacity-achieving sequence Q such that Eb Q = Eb and\\nfor large enough n, R(n) ≥ C − δ. If we replace first n members of Q with codes whose rate are (C − δ) or\\nhigher we get another sequence Q′ such that Q′ ∈ Z(C − δ) where Eb Q′ = Eb . Thus Ēb (C − δ) ≥ Eb for all\\nδ > 0. Consequently\\nĒb ≥ Eb\\n\\n\\x0c43\\n\\nEb ≥ E¯b :\\nLet us first fix an arbitrarily small δ > 0. In the table in Figure 6, row k represents a code-sequence Q̄k ∈\\nZ(C − 1/k), whose single-bit exponent\\nEb ,Q̄k ≥ Ēb (R) − δ\\n\\nLet Q̄k (l) represent length-l code in this sequence. We construct a capacity achieving sequence Q from this table\\nby sequentially choosing elements of Q from rows 1, 2, · · · as follows .\\n1\\nQ̄1 (1)\\n\\n2\\nQ̄1 (2)\\n\\nBlock Length\\n3 · · · · · · · · · · · · · · n1 · · · n2 · · · · · · · · n3 · · · · · ·\\n· · · · · · · · · · · · · · · · · ·\\nQ̄1 (3) Q̄1 (4) · · · · · · · ·\\n\\nQ̄2 Q̄2 (1)\\n\\nQ̄2 (2)\\n\\nQ̄2 (3) · · · · · · · · · · · · · · · · · ·\\n\\nQ̄3 Q̄3 (1)\\n\\nQ̄3 (2) · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\\n\\nQ̄1\\n\\nQ̄4\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nFig. 6.\\n•\\n\\n· · · · · · ·\\n\\n-\\n\\nQ̄4 (1) · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\\n\\nRow k denotes a reliable code sequence at rate C − 1/k. Bold path shows capacity achieving sequence Q.\\n\\nFor each sequence Q̄i , let ni denote the smallest block length n at which,\\n1) The single bit error probability satisfies\\nh\\ni\\nPr (n) M̂1 6= M1 ≤ e−n(Ēb (R)−2δ))\\n2) The over all error probability satisfies\\n\\nPr\\n•\\n\\n(n)\\n\\nh\\n\\ni\\nM̂ 6= M ≤ 1/i\\n\\n3) ni ≥ ni−1\\nGiven the sequence, n1 , n2 , · · · , we choose the members of our capacity achieving code from the code-table\\nshown in Figure 6 as follows.\\n– Initialize: We use first n2 − 1 members of Q̄1 as the first n2 − 1 members of the new code.\\n– Iterate: We choose codes of length ni to ni+1 − 1 from the code sequence Q̄i+1 , i.e.,\\n\\x01\\nQ̄i (ni ), Q̄i (ni + 1) · · · , Q̄i (ni+1 − 1)\\n\\nThus Q is a sampling of the code-table as shown by the bold path in Figure 6. Note that this choice of Q is a\\ncapacity achieving sequence, moreover it will also achieve a single bit exponent\\nEb ,Q = inf {Ēb (R) − 2δ} = Ēb − 2δ\\nR<C\\n\\nChoosing arbitrarily small δ proves Eb ≥ Ēb .\\n\\n•\\n\\nACKNOWLEDGMENT\\nThe authors are indebted to Bob Gallager for his insights and encouragement for this work in general. In\\nparticular, Theorem 3 was mainly inspired from his remarks. Helpful discussions with David Forney and Emre\\nTelatar are also gratefully acknowledged.\\n\\n\\x0c44\\n\\nR EFERENCES\\n[1] R. Ahlswede and G. Dueck. Identification via channels. IEEE Transactions on Information Theory, 35(1):15–29, 1989.\\n[2] A. Albanese, J. Blomer, J. Edmonds, M. Luby, and M. Sudan. Priority encoding transmission. IEEE Transactions on Information\\nTheory, 42(6):1737–1744, Nov 1996.\\n[3] L. A. Bassalygo, V. A. Zinoviev, V. V. Zyablov, M. S. Pinsker, and G. Sh. Poltyrev. Bounds for codes with unequal protection of\\ntwo sets of messages. Problemy Perdachi Informatsii, 15(3):44–49, July-Sept 1979.\\n[4] P. Berlin, B. Nakiboğlu, B. Rimoldi, and E. Telatar. A simple converse of burnashev’s reliability function. Information Theory, IEEE\\nTransactions on, 55(7):3074–3080, July 2009.\\n[5] S. Borade and S. Sanghavi. Some fundamental coding theoretic limits of unequal error protection. In Information Theory, 2009.\\nISIT 2009. IEEE International Symposium on, pages 2231–2235, 28 2009-July 3 2009.\\n[6] S. Borade and L. Zheng. Euclidean information theory. In Forty-Fifth Annual Allerton Conference, pages 633–640, September 26-28,\\n2007.\\n[7] I. Boyarinov and G. Katsman. Linear unequal error protection codes. IEEE Transactions on Information Theory, 27(2):168–175,\\nMar 1981.\\n[8] S.I. Bross and S. Litsyn. Improved upper bounds for codes with unequal error protection. IEEE Transactions on Information Theory,\\n52(7):3329–3333, July 2006.\\n[9] M. V. Burnashev. Data transmission over a discrete channel with feedback, random transmission time. Problemy Perdachi Informatsii,\\n12(4):10–30, 1976.\\n[10] A. R. Calderbank and N. Seshadri. Multilevel codes for unequal error protection. IEEE Transactions on Information Theory,\\n39(4):1234–1248, 1968.\\n[11] Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience, New York, NY, USA, 1991.\\n[12] I. Csiszár. Joint source-channel error exponent. Problems of Control and Information Theory, Vol. 9, Iss.5:315–328, 1980.\\n[13] Imre Csiszár and János Körner. Information Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, Inc.,\\nOrlando, FL, USA, 1982.\\n[14] P. Cuff. Communication requirements for generating correlated random variables. In Information Theory, 2008. ISIT 2008. IEEE\\nInternational Symposium on, pages 1393–1397, July 2008.\\n[15] S. Diggavi and D. Tse. On successive refinement of diversity. In Forty-Second Annual Allerton Conference, pages 1641–1650,\\nSeptember 29- October 1, 2004.\\n[16] V. N. Dynkin and V. A. Togonidze. Cyclic codes with unequal protection of symbols. Problemy Perdachi Informatsii, 12(1):24–28,\\nJan-Mar 1976.\\n[17] G. D. Forney Jr. On exponential error bounds for random codes on the bsc. unpublished manuscript.\\n[18] G. D. Forney Jr. Exponential error bounds for erasure, list, and decision feedback schemes. IEEE Transactions on Information\\nTheory, 14(2):206–220, 1968.\\n[19] R. G. Gallager.\\nFixed composition arguments and lower bounds to error probability.\\nunpublished\\nmanuscript, http://web.mit.edu/gallager/www/notes/notes5.pdf.\\n[20] Robert G. Gallager. Information Theory and Reliable Communication. John Wiley & Sons, Inc., New York, NY, USA, 1968.\\n[21] W. J. van Gils. Two topics on linear unequal error protection codes: Bounds and their lengths and cyclic code classes. IEEE\\nTransactions on Information Theory, 29(9):866876, Sep 1983.\\n[22] C. Kilgus and W. Gore. A class of cyclic unequal error protection codes. IEEE Transactions on Information Theory, 18(5):687–690,\\nSep 1972.\\n[23] B. D. Kudryashov. On message transmission over a discrete channel with noiseless feedback. Problemy Perdachi Informatsii,\\n15(1):3–13, 1973.\\n[24] B. Masnick and J. Wolf. On linear unequal error protection codes. IEEE Transactions on Information Theory, 13(4):600–607, Oct\\n1967.\\n[25] A. Montanari and G. D. Forney Jr.\\nOn exponential error bounds for random codes on the dmc.\\nunpublished\\nmanuscript, http://www.stanford.edu/˜montanar/PAPERS/FILEPAP/dmc.ps.\\n[26] R. H. Morelos-Zaragoza and Lin S. On a class of optimal nonbinary linear unequal error protection codes for two sets of messages.\\nIEEE Transactions on Information Theory, 40(1):196–200, Jan 1994.\\n[27] F. Ozbudak and H. Stichtenoth. Constructing linear unequal error protection codes from algebraic curves. IEEE Transactions on\\nInformation Theory, 49(6):1523–1527, Jun 2003.\\n[28] H. Pishro-Nik, N. Rahnavard, and F. Fekri. Nonuniform error correction using low-density parity-check codes. IEEE Transactions\\non Information Theory, 51(7):2702–2714, July 2005.\\n[29] C. Poulliat, D. Declercq, and I. Fijalkow. Optimization of ldpc codes for uep channels. In Information Theory, 2004. ISIT 2004.\\nProceedings. International Symposium on, pages 450–, June-2 July 2004.\\n[30] C. Poulliat, D. Declercq, and I. Fijalkow. Enhancement of unequal error protection properties of ldpc codes. EURASIP J. Wirel.\\nCommun. Netw., 2007(3):1–13, 2007.\\n[31] N. Rahnavard and F. Fekri. Unequal error protection using low-density parity-check codes. In Information Theory, 2004. ISIT 2004.\\nProceedings. International Symposium on, pages 449–, June-2 July 2004.\\n[32] N. Rahnavard, H. Pishro-Nik, and F. Fekri. Unequal error protection using partially regular ldpc codes. Communications, IEEE\\nTransactions on, 55(3):387–391, March 2007.\\n\\n\\x0c45\\n\\n[33] A. Sahai and S. Mitter. The necessity and sufficiency of anytime capacity for control over a noisy communication link: Part ii:\\nvector systems. arXiv:cs/0610146v2 [cs.IT], http://arxiv.org/abs/cs/0610146.\\n[34] A. Sahai and S. Mitter.\\nSource coding and channel requirements for unstable processes.\\narXiv:cs/0610143v2\\n[cs.IT], http://arxiv.org/abs/cs/0610143.\\n[35] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, Vol. 27:379–423 and 623–656, July and\\nOctober 1948.\\n[36] C.E. Shannon, R.G. Gallager, and E.R. Berlekamp. Lower bounds to error probability for coding on discrete memoryless channels.\\nInformation and Control, 10(1):65–103, 1967.\\n[37] Albert N. Shiriaev. Probability. Springer-Verlag Inc., New York, NY, USA, 1996.\\n[38] M. Trott. Unequal error protection codes: theory and practice. In Proc. IEEE Information Theory Workshop, Haifa, June 1996.\\n[39] B. Vasic, A. Cvetkovic, S. Sankaranarayanan, and M. Marcellin. Adaptive error protection low-density parity-check codes for joint\\nsource-channel coding schemes. In Information Theory, 2003. Proceedings. IEEE International Symposium on, pages 267–267,\\nJune-4 July 2003.\\n[40] H. Yamamoto and K. Itoh. Asymptotic performance of a modified schalkwijk-barron scheme for channels with noiseless feedback.\\nIEEE Transactions on Information Theory, 25(6):729–733, 1979.\\n\\n\\x0c', 'On the hitting times of quantum versus random walks ∗\\n\\narXiv:0808.0084v1 [quant-ph] 1 Aug 2008\\n\\nFrédéric Magniez†\\n\\nAshwin Nayak‡\\n\\nPeter C. Richter†\\n\\nMiklos Santha§\\n\\nAbstract\\nThe hitting time of a classical random walk (Markov chain) is the time required to detect the presence\\nof – or equivalently, to find – a marked state. The hitting time of a quantum walk is subtler to define;\\nin particular, it is unknown whether the detection and finding problems have the same time complexity.\\nIn this paper we define new Monte Carlo type classical and quantum hitting times, and we prove several\\nrelationships among these and the already existing Las Vegas type definitions. In particular, we show\\nthat for some marked state the two types of hitting time are of the same order in both the classical and\\nthe quantum case.\\nFurther, we prove that for any reversible ergodic Markov chain P , the quantum hitting time of the\\nquantum analogue of P has the same order as the square root of the classical hitting time of P . We also\\ninvestigate the (im)possibility of achieving a gap greater than quadratic using an alternative quantum\\nwalk. In doing so, we define a notion of reversibility for a broad class of quantum walks and show how to\\nderive from any such quantum walk a classical analogue. For the special case of quantum walks built on\\nreflections, we show that the hitting time of the classical analogue is exactly the square of the quantum\\nwalk.\\nFinally, we present new quantum algorithms for the detection and finding problems. The complexities\\nof both algorithms are related to the new, potentially smaller, quantum hitting times. The detection\\nalgorithm is based on phase estimation and is particularly simple. The finding algorithm combines a\\nsimilar phase estimation based procedure with ideas of Tulsi from his recent theorem [Tul08] for the 2D\\ngrid. Extending his result, we show that for any state-transitive Markov chain with unique marked state,\\nthe quantum hitting time is of the same order for both the detection and finding problems.\\n\\n1\\n\\nIntroduction\\n\\nMany classical randomized algorithms are based on random walks, or Markov chains. Some use random\\nwalks to generate random samples from the Markov chain’s stationary distribution, in which case the mixing time of the Markov chain is the complexity measure of interest. Others use random walks to search\\nfor a “marked” state in the Markov chain, in which case the hitting time is of interest. In recent years,\\nresearchers studying quantum walks have attempted to define natural notions of “quantum mixing time”\\n[NV00, ABN+ 01, AAKV01] and “quantum hitting time” [AKR05, Sze04, MNRS07] and to develop quantum\\nalgorithmic applications to sampling and search problems.\\nAmbainis [Amb04] was the first to solve a natural problem—the “element distinctness problem”—using\\na quantum walk. Following this, quantum walk algorithms were discovered for triangle finding [MSS07],\\nmatrix product verification [BŠ06], and group commutativity testing [MN07]. All of these are “hitting time”\\n∗ Research supported in part by the European Commission IST Integrated Project Qubit Applications (QAP) 015848, the\\nANR Blanc AlgoQP grant of the French Research Ministry, NSERC Canada, CIFAR, an Ontario ERA, QuantumWorks, and\\nARO/NSA (USA).\\n† LRI, Univ. Paris-Sud, CNRS; F-91405 Orsay, France; magniez@lri.fr, richterp@lri.fr.\\n‡ C&O and IQC, U. Waterloo, and Perimeter Institute, Ontario N2L 3G1, Canada; anayak@uwaterloo.ca. Some of this work\\nwas conducted while this author was visiting LRI, U. Paris-Sud, Orsay, France. Research at Perimeter Institute is supported\\nin part by the Government of Canada through NSERC and by the Province of Ontario through MEDT.\\n§ LRI, Univ. Paris-Sud, CNRS; F-91405 Orsay, France and Centre for Quantum Technologies, National University of Singapore, Singapore 117543; santha@lri.fr. Research at the Centre for Quantum Technologies is funded by the Singapore Ministry\\nof Education and the National Research Foundation.\\n\\n1\\n\\n\\x0capplications involving quantum walk search on Johnson graphs—highly-connected graphs whose vertices\\nare subsets of a fixed set and whose edges connect subsets differing by at most two elements. Quantum\\nwalk algorithms for the generic spatial search problem [AA05] were given by Shenvi et al. [SKW03] on the\\nhypercube, and by Childs and Goldstone [CG04] and Ambainis et al. [AKR05] on the torus. Szegedy [Sze04]\\nshowed that for any symmetric Markov chain and any subset M of marked elements, we can detect whether\\nor not M is nonempty in at most (of the order of) the square-root of the classical hitting time. To achieve this\\ngoal, Szegedy defined the quantum analogue of any symmetric Markov chain. Later Magniez et al. [MNRS07]\\nextended this to define the quantum analogue of the larger class of irreducible Markov chains.\\nUnresolved by Szegedy’s work is the question: with what probability does the algorithm output a marked\\nstate, as opposed to merely detecting that M is nonempty? This issue was addressed by Magniez et\\nal. [MNRS07], who gave an algorithm which finds a marked state with constant probability √\\nbut whose\\n√\\ncomplexity may be more than the square root of the classical hitting time. Indeed, for the N × N\\ngrid their bound is Θ(N ) whereas the classical hitting time is Θ(N log N ). The algorithms of Ambainis et\\nal. [AKR05] and Szegedy√[Sze04] perform better on the grid if there is a unique marked state: they find the\\nmarked state in time O( N · log N ). (The case of multiple marked elements may be reduced to this case\\nat the cost of a polylog factor in run-time.) For some time it remained\\n√ unclear if one could do better, until\\nTulsi [Tul08] showed how to find a unique marked element in time O( N log N ). His algorithm seems to be\\nsomething of a departure from previous quantum walk algorithms, most of which have been analyzable as\\nthe product of two reflections à la the Grover algorithm [Gro96]. The 2D grid was the canonical example\\nof a graph on which it was unknown how to find a marked state quantumly with the same complexity as\\ndetection. Tulsi’s result thus raises the question: is finding ever any harder than detection?\\nIn this paper we address several questions related to classical and quantum hitting times. In the literature\\non Markov chains, hitting time is usually defined as the complexity of the natural Las Vegas algorithm for\\nfinding a marked element by running the chain. We first give an alternative definition based on the Monte\\nCarlo version of the same algorithm. To our knowledge, this variant of the hitting time has not been\\nconsidered previously. We show that for some marked state, the two hitting times are of the same order\\n(Theorem 2.3).\\nWithin the setting of abstract search algorithms presented by Ambainis et al. [AKR05], we introduce\\nquantum analogues of the two classical hitting times (Definition 3.2). The analogue of the Las Vegas version\\nwas already present in Szegedy’s work [Sze04], whereas the other is new. Unlike in the classical case, detection\\nand finding are substantially different problems in quantum computing. We address both problems here.\\nFor the detection problem, we introduce a new algorithm Detect based on phase estimation which is\\nsimilar to the approach of Magniez et al. [MNRS07]. Our algorithm can detect the presence of a marked\\nelement in the starting state. The advantages of this algorithm are its simplicity and the fact that its\\ncomplexity is related to the new Monte Carlo type quantum hitting time (Theorem 3.5).\\nThis is an\\nimprovement over the Szegedy detection algorithm whose complexity is related to the potentially larger Las\\nVegas quantum hitting time.\\nWe then present a variant of the above algorithm, called Rotate, which can be used for the more\\ndifficult problem of finding, and whose complexity is also related to the Monte Carlo type quantum hitting\\ntime (Theorem 3.9). This improves the finding algorithm due to Ambainis et al. whose complexity was\\ncharacterized by a potentially larger quantity, the inverse of the smallest eigenphase of the search algorithm.\\nOur algorithm is also simpler. Combining Rotate with the ideas in the Tulsi algorithm for the 2D grid, we\\ncan find a marked element with constant probability and with the same complexity as detection for a large\\nclass of quantum walks—the quantum analogue of state-transitive reversible ergodic Markov chains.\\nAs in the classical case, for some marked elements the two types of the quantum hitting time are of the\\nsame order (Fact 3.10 and Theorem 3.14). For any reversible ergodic Markov chain P , we prove that the\\nquantum hitting time of the quantum analogue of P is of same order as the square root of the classical\\nhitting time of P (Theorem 3.15). Moreover, for the Las Vegas hitting times they are exactly the same.\\nFinally, we investigate the (im)possibility of achieving a greater than quadratic gap using some other\\nquantum walk. For this we consider general quantum walks on the edges of an undirected graph G; these\\nwere defined, for example, in the survey paper of Ambainis [Amb03], see also [San08]. We define a quite\\n\\n2\\n\\n\\x0cnatural notion of reversibility for general quantum walks. We conjecture that for any reversible quantum\\nwalk U2 on an undirected graph G, there exists a reversible ergodic Markov chain P on G such that for\\nevery marked state, the quantum hitting time of U2 is at most the square root of the classical hitting time\\nof P . We are able to prove this in the special case of quantum walks built on reflections (Theorem 3.18).\\nOur proof introduces a classical analogue of such quantum walks which might be of independent interest\\n(Definition 3.5). Curiously, the classical analogue is reversible if and only if the quantum walk is reversible\\n(Lemma 3.17).\\n\\n2\\n\\nClassical hitting times\\n\\nLet P be an ergodic and reversible Markov chain over state space X = {1, . . . , n}, which we identify with\\nits transition probability matrix. We suppose that the eigenvalues of P are nonnegative, by replacing P\\nwith (P + I)/2 if necessary. More generally, one may also assume that the eigenvalues of P are at least α,\\nwhere α ∈ [0, 1), by replacing P with ((1 − α)P + (1 + α)I)/2 if necessary. We make this further assumption\\nwhen needed, for instance in Section 4. Let π denote the stationary distribution of P . Let P−z be the\\n(n − 1) × (n − 1) matrix we get by deleting from P the row and column indexed by z. Similarly, for a vector\\nv, we let v−z stand for the vector obtained by omitting the z-coordinate of v.\\nClaim 2.1. The eigenvalues of P−z are all in the interval [κn , 1), where κn is the smallest eigenvalue of P .\\nProof. The proof globally proceeds along the lines of the proof of Lemma 8 in [Sze04]. For x ∈ X let ex\\ndenote the characteristic vector of x. Let w1 , . . . , wn be the eigenvectors of P with associated eigenvalues\\n1 = κ1 ≥ . . . ≥ κn > 0. Let v be an arbitrary eigenvector of Pz with eigenvalue λ. Since P is ergodic,\\nkPz k < 1, therefore λ < 1. We show that λ ≥ κn . This is obviously true if λ = κk for some k; let us suppose\\nthat this is not the case.\\nLet w be the vector obtained from P\\nv by augmenting it with\\nPn a 0 in the z-coordinate. We express both w\\nn\\nand ez in the eigenbasis of P : let w = k=1 γk wk and ez = k=1 δk wk . Then wP = λw + νez for some real\\nnumber ν. Moreover, ν 6= 0; otherwise w would have been an eigenvector of P , meaning that λ = κk , which\\ncontradicts our supposition. For k = 1, . . . , k, we have κk γk = λγk + νδk . Since w and ez are orthogonal, we\\nP\\nP\\n|2\\nalso have nk=1 γk δ¯k = 0. Therefore nk=1 κ|δkk−λ\\n= 0. The statement then follows since the left hand side of\\nthe above equation would be positive if λ were less than κn .\\nDefinition 2.1. For z ∈ X, the z-hitting time of P , denoted by HT(P, z), is the expected number of steps\\nthe chain P takes to reach the state z when started in the initial distribution π.\\n†\\nIt is well known that the z-hitting time of P is given by the formula HT(P, z) = π−z\\n(I − P−z )−1 u−z ,\\nwhere u is the all-ones vector. Simple algebra shows that\\n√\\n√\\n†\\nπ−z\\n(I − P−z )−1 u−z =\\nπ−z † (I − S−z )−1 π−z ,\\n\\np\\np\\n−1\\n√\\nwhere π−z is the entry-wise square root of π−z and S−z is the “symmetrized form” S−z = Π−z P−z Π−z\\nof P−z with Π−z = diag(πx )x6=z . The matrices P−z and S−z have the same spectrum since they are similar.\\nLet {vj : j ≤ n − 1} be the set of normalized eigenvectors of S−z where the eigenvalue of vj is λj = cos θj\\nwith 0 <Pθj ≤ π/2. By reordering the eigenvalues we can suppose that 1 > λ1 ≥ . . . ≥ λn−1 ≥ 0. If\\n√\\n√\\nπ−z = j νj vj is the decomposition of π−z in the eigenbasis of S−z then the z-hitting time satisfies:\\nHT(P, z)\\n\\n=\\n\\nX\\nj\\n\\nνj2\\n.\\n1 − λj\\n\\nWhen 0 < θ ≤ π/2 then 1 − θ2 /2 ≤ cos θ ≤ 1 − θ2 /4. Therefore we can approximate the hitting time with\\nanother expectation that is very closely related to the analogous quantum notion. More precisely,Plet Hz be\\nthe random variable which takes the value 1/θj2 with probability νj2 , and 0 with probability 1 − j νj2 . We\\ndenote the expectation of Hz by E[Hz ]. Then we have 2 E[Hz ] ≤ HT(P, z) ≤ 4 E[Hz ].\\n3\\n\\n\\x0cIn the definition of the hitting time the Markov chain is used in a Las Vegas algorithm: we count the\\n(expected) number of steps to reach the marked element without error. We can also use the chain as an\\nalgorithm that reaches the marked element with some probability smaller than 1, leading to a Monte Carlo\\ntype definition. Technically, to be able to underline the analogies between the classical and quantum notions,\\nwe define the hitting time with error via the random variable Hz .\\nDefinition 2.2. For z ∈ X and for 0 < ε < 1, the ε-error z-hitting time of P , denoted by HTε (P, z) is\\ndefined as\\nHTε (P, z) = min {y : Pr[Hz > y] ≤ ε}.\\nObserve that for all z, if ε ≤ ε′ then HTε′ (P, z) ≤ HTε (P, z). We first show that the use of Hz for the\\ndefinition of the Monte Carlo hitting time is indeed justified (proof in Appendix A). For this, let us denote\\nby hε (P, z) the smallest integer k such that the probability that the chain does not reach z in the first k\\nsteps is at most ε.\\nTheorem 2.2. For all z and ε, we have\\nhε (P, z)\\nHTε (P, z)\\n\\n\\x12\\n\\x13\\n2\\n≤\\n4 ln\\nHTε/2 (P, z),\\nε\\n1\\nhε/3 (P, z).\\n≤\\n2\\n\\nand\\n\\nHow much smaller than the Las Vegas hitting time can the Monte Carlo hitting time be? The following\\nresults state that for some z they are of the same order of magnitude.\\nTheorem 2.3. We have the following inequalities between the two notions of hitting time:\\n• For all z and ε,\\n\\nHTε (P, z) ≤\\n\\n1\\n2ε\\n\\nHT(P, z).\\n\\n• There exists z such that for all ε < 1/2,\\n\\nHT(P, z) ≤ 4 HTε (P, z).\\n\\nProof. The first statement simply follows from the Markov inequality and from the relation E[Hz ] ≤\\n2\\nHT(P, z)/2. For the second statement, let z be an element such\\nP that ν1 ≥ 1/2. The existence of such\\nan element is assured by Lemma 8 in [Sze04]. Then HT(P, z) ≤ j 4νj2 /θj2 ≤ 4/θ12 ≤ 4HTε (P, z).\\n\\n3\\n3.1\\n\\nQuantum hitting times\\nTwo notions of quantum walk\\n\\nLet U = U2 U1 be an abstract search algorithm as in [AKR05], where U1 = I − 2|µihµ| for a “target vector” |µi\\nwith real entries, and U2 is a real unitary matrix with a unique 1-eigenvector |φ0 i. Without loss of generality\\nwe always assume that |φ0 i has real entries. The state |µi is the quantum analogue of the state z which we\\nseek in the classical walk P , U2 the analogue of P , and |φ0 i the analogue of the stationary distribution π.\\nThe abstract search algorithm usually starts with state |φ0 i, and iterates U several times in order to get a\\nlarge deviation from |φ0 i. In this paper, we prefer to start with a slightly different initial state. The general\\nbehavior of the abstract search algorithm remains unchanged by this. We replace the starting state |φ0 i by\\n|φe0 i = |φ0 i− hφ0 |µi|µi, the (unnormalized) projection of the 1-eigenvector |φ0 i of U2 on the space orthogonal\\nto |µi. This substitution was first considered in [Sze04], and corresponds to first making a measurement\\naccording (|µi, |µi⊥ ). If the measurement outputs |µi we are done. Otherwise we run the abstract search\\nalgorithm.\\nThis choice of the initial state is motivated by the results in Section 3.4 which relate quantum hitting\\ntime to classical hitting time. All other results in this paper remain valid if we keep |φ0 i.\\nAmbainis et al. characterized the spectrum of U in term of the decomposition of |µi in the eigenvector\\nbasis of U2 . One of their results is:\\n4\\n\\n\\x0cTheorem 3.1 ([AKR05]). Let U2 be a unitary matrix with real entries and a unique 1-eigenvector |φ0 i. Let\\n|µi be a unit vector with real entries, and let U1 = I − 2|µihµ|. Let U = U2 U1 .\\n• If hφ0 |µi = 0, then |φe0 i = |φ0 i and U |φe0 i = |φe0 i.\\n\\n• If hφ0 |µi 6= 0, then U has no 1-eigenspace.\\n\\nThus one can use U in order to detect if hφ0 |µi =\\n6 0. Indeed, in that case, after a certain number T\\ne\\nof iterations of U on |φ0 i, the state moves far from the intial state |φe0 i. Such a deviation caused by some\\noperator V (in our case V = U T , i.e., U iterated T times) is usually detected by using the well known control\\ntest which requires the use the controlled operator c-V . Namely observe that (H ⊗ I)(c-V )(H ⊗ I)|0i|ψi =\\n1\\n1\\n2 |0i(|ψi + V |ψi) + 2 |1i(|ψi − V |ψi). Therefore a measurement of the first register gives outcome 1 with\\nprobability k|ψi − V |ψik2 /4.\\nSzegedy [Sze04] designed a generic method for constructing an abstract search algorithm given a (classical)\\nMarkov chain. Let P = (pxy ) be an ergodic Markov chain over state space X = {1, . . . , n} with stationary\\ndistribution |πi. The time-reversal P ∗ of this chain is defined by equations πy p∗yx = πx pxy . The chain P is\\nreversible if P = P ∗ .\\nThe quantum analogue of P may be thought of as a walk on the edges of the original Markov chain,\\nrather than on its vertices. Thus, its state space is a vector subspace of H = CX×X . For a state |ψi ∈ H,\\nlet Πψ = |ψihψ| denote the orthogonal projector onto Span(|ψi), and let ref(ψ) = 2Πψ − Id denote the\\nreflection through the line generated by |ψi, where Id is the identity operator on\\nP H. If K is a subspace of\\nH spanned by a set of mutually orthogonal states {|ψi i : i ∈ I}, then let ΠK = i∈I Πψi be the orthogonal\\nprojector onto K, and let ref(K) = 2ΠK − Id be the reflection through K. Let A = Span(|xi|px i : x ∈ X)\\nand B = Span(|p∗y i|yi : y ∈ X) be vector subspaces of H, where\\nXp\\nX√\\npxy |yi and |p∗y i =\\np∗yx |xi.\\n|px i =\\nx∈X\\n\\ny∈X\\n\\nDefine similarly for any z ∈ X the subspaces A−z = Span(|xi|px i : x ∈ X \\\\ {z}) and B−z = Span(|p∗y i|yi :\\ny ∈ X \\\\ {z}).\\nDefinition 3.1 ([Sze04, MNRS07]). Let P be an ergodic Markov chain. The unitary operation W (P ) =\\nref(B) · ref(A) defined on H is called the quantum analogue of P ; and the unitary operation W (P, z) =\\nref(B−z ) · ref(A−z ) defined on H is called the quantum analogue of P−z .\\nThe unitary operation SWAP is defined by SWAP|xi|yi = |yi|xi. When P is reversible, the connection\\nbetween the quantum walk of Szegedy and the quantum walk of Ambainis et al. is made explicit by the\\nfollowing fact.\\nFact 3.2. Let z ∈ X and |µi = |zi|pz i. Let U2 = SWAP · ref(A) and U1 = I − 2|µihµ|. If P is reversible\\nthen (U2 U1 )2 = W (P, z). In particular, the unitary operators U = U2 U1 and W (P, z) are diagonal in the\\nsame orthonormal basis.\\n\\n3.2\\n\\nPhase estimation and quantum hitting time\\n\\nLet U be a unitary matrix with real entries. The potential eigenvalues of U are then 1, −1, and pairs of\\nconjugate complex numbers (eiαj , e−iαj ) with 0 < αj < π, for 1 ≤ j ≤ J, for some J.\\nLet |ψi be a vector with real entries and of norm at most one. Then |ψi uniquely decomposes as\\nX\\n|ψi = δ0 |w0 i +\\nδj (|wj+ i + |wj− i) + δ−1 |w−1 i,\\n(1)\\n1≤j≤J\\n\\nwhere δ0 , δ−1 , δj are reals, |w0 i is a unit eigenvector of U with eigenvalue 1, |w−1 i is a unit eigenvector\\nwith eigenvalue −1, and |wj+ i, |wj− i are unit eigenvectors with respective eigenvalues eiαj and e−iαj , and\\n\\n|wj− i = |wj+ i.\\n\\n5\\n\\n\\x0cWe now describe a procedure whose purpose is to detect the state |ψi has a component orthogonal to\\nthe 1-eigenspace of U . In the context of the abstract search algorithm, this is equivalent to hφ0 |µi 6= 0. The\\nidea, similar to the approach of Magniez et al. [MNRS07], is to apply the phase estimation algorithm of\\nKitaev [Kit95] and Cleve et al. [CEMM98] to U .\\nTheorem 3.3 ([Kit95, CEMM98]). Given an eigenvector |vi of a unitary operator U with eigenvalue eiα ,\\nthe corresponding phase α ∈ (−π, π] can be determined with precision ∆ and error probability at most 1/3\\nby a circuit Estimate. If |vi is a 1-eigenvector of U , then Estimate determines α = 0 with probability\\n1. Moreover, Estimate makes O(1/∆) calls to the controlled operator c-U and its inverse, and it contains\\nO((log 1/∆)2 ) additional gates.\\nBased on the circuit Estimate, we can detect the presence of components orthogonal to the 1-eigenspace\\nin an arbitrary state |ψi.\\nDetect(U, ∆, ε) — Input: |ψi\\n1. Apply Θ(log(1/ε)) times the phase estimation circuit\\nEstimate for U with precision ∆ to the same state |ψi.\\n2. If at least one of the estimated phases is nonzero, ACCEPT.\\nOtherwise REJECT.\\nLet QH be the random variable which takes the value 1/αj with probability 2δj2 , the value 1/π with\\n2\\nprobability δ−1\\n, and the value 0 otherwise. Observe that in the following lemma, and in the analysis of all\\nour algorithms, the probabilities in fact sum to kψk2 , since |ψi is not necessarily normalized, and has norm\\nat most 1.\\nLemma 3.4. Assume that Pr[QH > 1/∆] ≤ ε. Then the procedure Detect(U, ∆, ε) accepts |ψi with\\nprobability kψk2 − δ02 − O(ε), and moreover with probability 0 if |δ0 | = kψk. In addition, the number of\\napplications of c-U is O(log(1/ε)/∆).\\nProof. Let us first assume that Estimate can compute the eigenphase of any eigenvector with certainty.\\nThis assumption is in fact valid when |δ0 | = kψk. Then the procedure Detect rejects exactly with probability\\nδ02 .\\nAssume now that Pr[QH > 1/∆] ≤ ε, and |δ0 | < kψk. First observe that Estimate with precision ∆\\nuses 1/∆ applications of c-U . Then the precision ∆ in Estimate ensures a nonzero approximation of an\\neigenphase ±αj with probability at least 2/3Pprovided that αj ≥ ∆. By hypothesis, the contribution of these\\neigenphases has squared Euclidean norm 2 j δj2 . The success probability is then amplified to 1 − O(ε) by\\nchecking that all the O(log(1/ε)) outcomes of Estimate are nonzero. For the special case of eigenphase 0,\\nwhose contribution has squared Euclidean norm δ02 , Estimate gives approximation 0 with probability 1.\\nThe contribution of the other eigenphases has squared Euclidean norm less than ε in the vector |ψi.\\nTherefore the overall acceptance probability is kψk2 − δ02 − O(ε).\\nIn the case of quantum walk, the above theorem justifies the following definitions of quantum hitting\\ntimes. Let U be some abstract search U2 U1 , where U1 = I − 2|µihµ|, starting from state |φe0 i = |φ0 i − a0 |µi,\\nwhere a0 = hµ|φ0 i. We now set |ψi = |φe0 i. Again, QH is the random variable which takes the value 1/αj\\n2\\nwith probability 2δj2 , the value 1/π with probability δ−1\\n, and 0 otherwise.\\nDefinition 3.2. The quantum |µi-hitting time of U2 is the expectation of QH , that is\\nQHT(U2 , |µi)\\n\\n=\\n\\n2\\n\\nX δj2\\nδ2\\n+ −1 .\\nαj\\nπ\\nj\\n\\nFor 0 < ε < 1, the quantum ε-error |µi-hitting time of U2 is defined as\\nQHTε (U2 , |µi)\\n\\n=\\n\\nmin{y : Pr[QH > y] ≤ ε}.\\n6\\n\\n\\x0cUsing Theorem 3.1, Lemma 3.4 and our definition of quantum hitting time, we directly get:\\nTheorem 3.5. For every T ≥ max {1, QHTε (U2 , |µi)}, the procedure Detect(U, 1/T, ε) accepts |φe0 i with\\nprobability kφe0 k2 − O(ε) if hφ0 |µi =\\n6 0, and accepts with probability 0 otherwise. Moreover the number of\\napplications of c-U is O(log(1/ε) × T ).\\nIf one would like to deal only with normalized states, and to come back to the original starting state |φ0 i,\\nwe can encapsulate the projection to the space orthogonal to |µi into our algorithm such as in the following\\nmain procedure, and deduce its behavior from the above theorem.\\nMainDetect(U2 , |µi, ∆, ε) — Input: |ψi\\n1. Make a measurement according (|µi, |µi⊥ ).\\n2. If the measurement outputs |µi, ACCEPT.\\nOtherwise apply Detect(U, ∆, ε).\\nCorollary 3.6. For every T ≥ max {1, QHTε (U2 , |µi)}, the procedure MainDetect(U2 , |µi, 1/T, ε) accepts\\n|φ0 i with probability 1 − O(ε) if hφ0 |µi =\\n6 0, and accepts with probability 0 otherwise.\\nWhen the abstract search is built on the quantum analogue of a reversible Markov chain P and |µi =\\n|zi|pz i for some z, we use the following terminology:\\n• The quantum z-hitting time of P is QHT(P, z) = QHT(SWAP · ref(A), |zi|pz i);\\n• For 0 < ε < 1, the quantum ε-error z-hitting time of P is QHTε (P, z) = QHTε (SWAP · ref(A), |zi|pz i).\\nWith different, more technical arguments, Szegedy proved results similar to Theorem 3.5 albeit with the\\nparameter QHT(P, z) for symmetric Markov chains:\\nTheorem 3.7 ([Sze04]). When t is chosen uniformly at random in {1, 2, . . . , ⌈QHT(P, z)⌉}, then the expectation of the deviation k(W (P, z))t |φe0 i − |φe0 ik is Ω(kφe0 k).\\n\\nUnder certain assumptions, Ambainis et al. [AKR05] have a similar result in terms of the smallest\\neigenphase of U2 U1 .\\nSuppose we wish to not only detect if hµ|φ0 i =\\n6 0, but also to map |ψi to |µi. Then we are led to a\\nprocedure different from Detect. One possibility is to try to use U in order to move |ψi to an orthogonal\\nstate that is closer to |µi.\\nP\\nDefinition 3.3. The U -rotation of |ψi is defined as δ0 |w0 i + j δj (|wj+ i − |wj− i) + δ−1 |w−1 i, where the\\n\\x08\\n\\t\\ndecomposition of |ψi in terms of the orthonormal set of eigenvectors |wj+ i, |wj− i of U is given by Eq. (1).\\n\\nFact 3.8. If |ψi is orthogonal to both the 1-eigenspace and the (−1)-eigenspace of U , then the U -rotation of\\n|ψi is orthogonal to |ψi.\\n\\nThis operation can be implemented efficiently by the following procedure with further assumptions on U .\\nNamely, we would like U to avoid having any eigenvalue close to −1. This is naturally the case when we\\nconsider the quantum analogue of a reversible Markov chain whose eigenvalues are all positive.\\nRotate(U, ∆, ε) — Input: |ψi\\n1. Apply Θ(log(1/ε)) times the phase estimation circuit\\nEstimate for U with precision ∆ to the same state |ψi.\\n2. If the majority of estimated phases are negative\\nPerform a Phase a Flip.\\nOtherwise do nothing.\\n3. Undo the Phase Estimations of Step 1.\\n\\n7\\n\\n\\x0cTheorem 3.9. Assume that all eigenvalues eiα of U satisfy |α| ≤ π/2. Then for every T ≥ QHTε (U2 , |µi),\\n√\\nthe procedure Rotate(U, 1/T, ε) maps |φe0 i to a state at Euclidean distance O( ε ) from the U -rotation of\\n|φe0 i. Moreover, the number of applications of c-U is O(log(1/ε) × QHTε (U2 , |µi)).\\n\\nProof. The proof follows the same argument as in Theorem 3.5.\\n\\n3.3\\n\\nComparison between QHT and QHTε\\n\\nWe assume henceforth that hφ0 |µi =\\n6 0; otherwise, the problem is trivial—by our definition QHTε (U2 , |µi) =\\nQHT(U2 , |µi) = 0.\\nThe Markov inequality immediately implies the following relationship between these two measures:\\nFact 3.10. For all U2 , |µi, and ε,\\n\\n1\\nQHT(U2 , |µi).\\nε\\nThe other direction requires a closer look at the spectral decomposition of U2 U1 . In this section, we again\\nfollow the framework of the abstract search algorithm U = U2 U1 . The eigenvalues of U2 different from 1 are\\neither −1 or they appear as complex conjugates e±iθj , where θj ∈ (0, π). For convenience, we assume that\\nθ−1 = π, 0 = θ0 ≤ θ1 ≤ θ2 ≤ . . ., and we always use index j for positive integers. Recall that both |µi and\\nthe 1-eigenvector |φ0 i of U2 have real entries. Writing |µi in the eigenspace decomposition of U2 we get\\nX\\n\\x01\\n−\\naj |φ+\\n|µi = a0 |φ0 i +\\nj i + |φj i + a−1 |φ−1 i,\\nQHTε (U2 , |µi)\\n\\n≤\\n\\nj\\n\\n±iθj\\n-eigenvectors of U2 such that a−1 and aj are real,\\nwhere |φ−1 i is a (−1)-eigenvector of U2 , and |φ±\\nj i are e\\n\\n+\\n|φ−1 i has real entries, and |φ−\\nj i = |φj i. Since |φ0 i has real entries, a0 is also real.\\nAmbainis et al. [AKR05] (see also Tulsi [Tul08]) show the following relation between the spectrum of U2\\nand that of U .\\n\\nLemma 3.11 ([AKR05, Tul08]). The eigenvalues e±iα of the operator U are solutions of the equation:\\n\\x12\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\x13\\nα + θj\\nα X 2\\nα\\nα − θj\\n2\\na0 cot +\\naj cot\\n+ cot\\n− a2−1 tan\\n= 0.\\n2\\n2\\n2\\n2\\nj\\nThe corresponding unnormalized eigenvectors |wα i = |µi + i|wα′ i satisfy hµ|wα′ i = 0 and\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n\\x13\\nX \\x12\\nα − θj\\nα\\nα\\nα + θj\\n−\\n|wα′ i = a0 cot |φ0 i +\\naj cot\\n|φ+\\ni\\n+\\ncot\\n|φ\\ni\\n− a−1 tan |φi.\\nj\\nj\\n2\\n2\\n2\\n2\\nj\\nAs in the classical case, we are only able to upper bound QHT by QHTε for some particular target\\nstates |µi. Therefore, we consider in the following lemma (proof in Appendix B) an arbitrary set of orthonormal vectors M = {|µz i} whose span contains |φ0 i. In the case of the quantum analogue of a Markov\\nchain P as in Definition 3.1, a natural choice for |µz i is |zi|pz i for some z in the state space of the Markov\\nchain. Recall that |φe0 i = |φ0 i − a0 |µi.\\nLemma 3.12. Let M = {|µz i} be a set of orthonormal vectors with real coefficients in the standard basis,\\nsuch that |φ0 i ∈ Span(M ). For every z, let αz be the smallest positive real number α such that e±iα are\\neigenvalues of the operator U = U2 (I − 2|µz ihµz |). Then there exists z such that the length of the projection\\n√\\nof |φe0 i onto the subspace generated by |wαz i and |w−αz i is at least 1/ 2.\\n\\nCorollary 3.13. Let M = {|µz i} be a set of real orthonormal vectors such that |φ0 i ∈ Span(M ). For all U2\\nthere exists z such that for all ε ≤ 1/2,\\nQHTε (U2 , |µz i)\\n8\\n\\n=\\n\\n1\\n.\\nαz\\n\\n\\x0cTheorem 3.14. Let M = {|µz i} be a set of real orthonormal vectors such that |φ0 i ∈ Span(M ). For all U2\\nthere exists z such that for all ε ≤ 1/2,\\nQHT(U2 , |µz i)\\n\\n≤\\n\\nQHTε (U2 , |µz i).\\n\\nProof. This is a consequence of Corollary 3.13 since QHT(U2 , |µz i) is by definition at most\\n\\n3.4\\n\\n1\\nαz .\\n\\nQuadratic detection speedup for reversible chains\\n\\nLet P be an ergodic Markov chain over state space X = {1, . . . , n}. We further suppose that P is a reversible\\nMarkov chain with positive eigenvalues, otherwise we simply replace P with γP + (1 − γ)I, for any γ < 1/2.\\nLet z ∈ X.\\nTheorem 3.15. Assume that the eigenvalues of P are all positive. Then we have the following relations:\\np\\n• For all z, QHT(P, z) ≤ HT(P, z)/2.\\np\\n• For all z and ε, QHTε (P, z) = HTε (P, z).\\nP √\\nProof. We follow the notation introduced in Sections 2 and 3.1. Then |φ0 i = x πx |xi|px i, |µi = |zi|pz i,\\nP\\nP\\n√\\n√\\n√\\n|φe0 i =\\nπ−z =\\nπ−z in the normalized\\nx∈X\\\\{z} πx |xi|px i. Let\\nj νj vj be the decomposition of\\neigenbasis of P−z where the eigenvalue of vj is cos θP\\n< π/2. Let vj [x] denote\\nj , with 0 < θ1 ≤ . . . ≤ θn−1 P\\n∗\\nthe x-coordinate of the vector vj . We set |ξj i =\\ny6=z vj [y]|py i|yi. Then\\nx6=z vj [x]|xi|px i and |ζj i =\\nP\\nνj |ξj i. For every j, the vectors |ξj i and |ζj i generate an eigenspace of W (P, z) that is also\\n|φe0 i =\\nj\\n\\ngenerated by two normalized eigenvectors with eigenvalues respectively e2iθj and e−2iθj . This argument is still\\ntrue for SWAP · ref(A−z ) when we divide the phases by 2, leading to eigenvalues eiθj and e−iθj (cf. Fact 3.2).\\n√\\nPn−1 ν 2\\nSince the length of the projection of |φe0 i to this eigenspace is νj2 , we have QHT(P, z) = j=1 θjj = E[ Hz ].\\nBy the Jensen inequality we get\\np\\np\\nE[Hz ] ≤\\nHT(P, z)/2.\\nQHT(P, z) ≤\\n\\nThe second relation above immediately follows from QH 2 = Hz .\\n\\nThe same quadratic speed-up as above holds when there are multiple marked elements in the state\\nspace X. The search algorithm and its analysis are similar and are omitted from this article.\\n\\n3.5\\n\\nOn the quadratic speedup threshold\\n\\nIn this section we consider a broad class of quantum walks defined on undirected graphs. We are able to\\nshow that for a special case of walks on graphs, the quadratic speedup is tight.\\nLet X = {1, 2, . . . , n}. Our notion of quantum walk can be seen as a walk on the edges of a given\\nundirected graph G(X, E). Let H(G) = Span(|xyi : (x, y) ∈ E) be the Hilbert space that a quantum walk\\non G should preserve. In the rest of this section, we only consider operators and states in H(G) for some\\ngiven G. Observe that SWAP preserves H(G) since G is undirected.\\nWe introduce a notion of reversibility that is justified by Lemma 3.17 below.\\nDefinition 3.4. A quantum walk on an undirected graph G = (X, E) is a unitary\\nU2 = SWAP · F defined\\nP\\non a subspace of H(G), where F is matrix with real entries of the form F = x∈X |xihx| ⊗ F x , and where\\nU2 has a single 1-eigenvector |φ0 i. The quantum walk is reversible when SWAP(|φ0 i) = |φ0 i.\\nObserve that the definition implies that |φ0 i can be chosen with real entries. This definition of quantum\\nwalk appears, for example, in the survey paper of Ambainis [Amb03], see also [San08]. Szegedy considered\\nfor F x a specific kind of reflection based on Markov chain transition probabilities (see Section 3.1).\\n\\n9\\n\\n\\x0cP √\\nDefinition 3.5. Let U2 = SWAP · F be a quantum walk with unit 1-eigenvector |φ0 i = x πx |xi|φx i,\\nwhere πx ≥ 0 and |φx i is a unit vector with real entries. Then the classical analogue P = (pxy ) of U2 is\\ndefined as pxy = hy|φx i2 .\\nSince |φ0 i is a 1-eigenvector of SWAP · F we directly state:\\nP √\\nFact 3.16. Let |ψ x i = F x |φx i. Then |φ0 i = x πx |ψ x i|xi.\\n\\nLemma 3.17. The classical analogue P of a quantum walk U2 on G is a Markov chain on G with stationary\\nprobability distribution π. Moreover, P is reversible if and only if U2 is a reversible quantum walk.\\nProof. First we show that P is a Markov chain on G. For every x, we have\\nX\\nX\\npxy =\\nhy|φx i2 = kφx k2 = 1.\\ny\\n\\ny\\n\\nMoreover pxy 6= 0 implies hy|φx i =\\n6 0, which implies that (x, y) ∈ E since |φ0 i ∈ H(G).\\nNow we verify that π is a stationary probability distribution. First, π is a probability distribution since\\n|φx i for all x ∈ X and |φ0 i are unit vectors. That π is a stationary probability distribution can be seen from\\nthe following sequence of equalities which hold for every y ∈ X:\\nX\\nX\\nπx pxy =\\nhxy|φ0 i2\\nby definition of P and |φ0 i\\nx\\n\\nx\\n\\n=\\n\\nX\\nx\\n\\n=\\n\\nπy hx|ψ y i2\\n\\nby Fact 3.16\\n\\nπy k|ψ y ik2 = πy .\\n\\nFor reversibility, observe that we similarly have πx pxy = hxy|φ0 i2 and πy pyx = hyx|φ0 i2 = (hxy|SWAP|φ0 i)2 .\\nP is reversible when these two expressions are equal for every x, y, which happens precisely when the quantum\\nwalk U2 is reversible.\\nFinally, we show that the quadratic speedup is tight in the special case of walks for which all of the\\nunitaries F x are reflections. We state the result using the notation above.\\nTheorem 3.18. Let U2 = SWAP · F be a reversible quantum walk such that F x = 2|φx ihφx | − I, for all\\nx ∈ X. Then for all z and ε,\\np\\nQHTε (U2 , |zi|φz i) = QHTε (P, z) = HTε (P, z).\\n\\nProof. Let U1 = I − 2|zihz| ⊗ |φz ihφz |, for some fixed z. Under the hypothesis of the theorem, (U2 U1 )2\\nis a product of two reflections ref(A−z ) and ref(B−z ), where A−z = Span(|xi|φx i : x ∈ X \\\\ {z}) and\\nB−z = Span(|φy i|yi : y ∈ X \\\\ {z}) = SWAP(A).\\n2\\nFrom [Sze04], we know\\nspectrum of (U\\nmatrix\\nP2 U1 ) yis completely defined by the discriminant\\nP that the\\nx\\n∗\\nD = A B, where A = x6=z |xi|φ ihx| and B = y6=z |φ i|yihy|. We get that D = (hx|φy ihφx |yi)x6=z,y6=z .\\n√\\n√\\nThe reversibility of U2 guarantees that hxy|πi = hyx|πi, which implies that πx hy|φx i = πy hx|φy i. Since\\n√ −1\\n√\\n|φy i has real entries, we have D = ΠP−z Π , where Π = diag(πx )x6=z and P−z is the matrix obtained\\nfrom P by deleting the row and column indexed by z.\\nObserve that this discriminant is exactly that P\\nof the quantum analogue W (P, z). So W (P, z) and\\n√\\n(U2 U1 )2 are equal up to a basis change which maps x πx |xi|px i to |φ0 i, |zi|pz i to |zi|φz i, and therefore\\nP\\n√\\nπx |xi|px i to |φe0 i.\\nx6=z\\n\\n10\\n\\n\\x0c4\\n\\nFinding with constant probability\\n\\nIn this section, we extend a technique devised by Tulsi [Tul08] for finding a marked state on the 2D grid in\\ntime that is the square-root of the classical hitting time. We prove that it may be applied to a larger class\\nof Markov chains and target states. The technique may be combined with ideas developed in the earlier\\nsections to give an algorithm for the quantum analogue of an arbitrary reversible ergodic Markov chain.\\nWe use the notation of Section 3.1. In our application, there is no (−1)-eigenvector of U2 . Therefore the\\nmarked state |µi has the following decomposition in an eigenvector basis of U2 :\\nX\\n−\\n|µi = a0 |φ0 i +\\naj (|φ+\\n(2)\\nj i + |φj i),\\n1≤j≤J\\n\\nwhere J is some positive integer. Last, we assume in the rest of this section that hφ0 |µi 6= 0.\\nLemma 4.1. The vectors |µi and |φe0 i have the following representation in the basis {|wα i} consisting of\\nP\\nP a i cot( α )\\nthe eigenvectors of U = U2 U1 as given by Lemma 3.11: |µi = α kw1α k2 |wα i, and |φe0 i = α 0kwα k22 |wα i.\\n\\nP\\nα |ψi\\nProof. Any vector |ψi may be expressed in the orthogonal basis {|wα i} as |ψi = α hw\\nkwα k2 |wα i. The first\\nequation now follows from hwα |µi = (hµ| − ihwα′ |)|µi = 1.\\nBy Lemma 3.11, hφ0 |wα i = hφ0 |(|µi + i|wα′ i) = a0 + a0 i cot α2 . The second equation follows by combining\\nthe above with |φe0 i = |φ0 i − a0 |µi.\\nP\\ncot( α )\\nLemma 4.2. The inner product of the target state |µi and the U -rotation of |φe0 i is 2a0 α>0 kwα k2 2 .\\n\\nTheorem 3.15 shows that the quantum hitting time is bounded by the square-root of the classical hitting\\ntime when U2 is derived from a reversible Markov chain P , i.e., U2 = SWAP · ref(A) in the notation of\\nSection 3.2. This allows for the detection of marked elements (or more generally for checking if hµ|φ0 i 6= 0)\\nand also the creation of the U -rotation of |φe0 i in the stated time bound. However, the overlap of the U rotation of |φe0 i with the target |µi may be o(1). Tulsi [Tul08] discovered a technique, described below, to\\nboost this overlap to Ω(1) in the case of a quantum walk on the 2D grid.\\nLet θ ∈ [0, π/2). Let Rθ denote the rotation in C2 by angle θ:\\n\\x14\\n\\x15\\ncos θ − sin θ\\nRθ =\\n,\\nsin θ\\ncos θ\\nand let |θi = Rθ† |0i, and |θ⊥ i = Rθ† |1i. Define U1θ = |0ih0| ⊗ Id + |1ih1| ⊗ U1 , and U2θ = (|θihθ| ⊗ (−Id) +\\n|θ⊥ ihθ⊥ | ⊗ U2 ). Then U1θ = Id − 2|1ih1| ⊗ |µihµ|, meaning that the modified marked state is |1i|µi. Then the\\nmodified abstract search algorithm becomes:\\nT(U1 , U2 , θ)\\n\\n=\\n\\nU2θ U1θ\\n\\n=\\n\\n(|θihθ| ⊗ (−Id) + |θ⊥ ihθ⊥ | ⊗ U2 )(|0ih0| ⊗ Id + |1ih1| ⊗ U1 )\\n\\n(3)\\n\\nThis is precisely the circuit used by Tulsi: his rotation R̂θ = Rθ† in our notation. Tulsi [Tul08] proved that\\nthe principal eigenvalue of the operator above is closely related to that of the unitary operator U2 U1 . We\\nextend his findings in terms that are more readily used in our context.\\nThe eigenvalues of U2θ are the same as those of U2 , except for the addition of the new eigenvalue −1.\\nThe eigenvectors corresponding to eigenvalues e±iθj are now |θ⊥ i|φ±\\nj i. Any state of the form |θi|ψi is a −1\\nθ\\neigenvector of U2 .\\nFact 4.3. The decomposition of |1i|µi in the eigenvector basis of U2θ is:\\n\\uf8eb\\n\\uf8f6\\nX\\n− \\uf8f8\\n|1i|µi = cos θ |θ⊥ i \\uf8eda0 |φ0 i +\\naj (|φ+\\n− sin θ |θi|µi,\\nj i + |φj i)\\n1≤j≤J\\n\\nwhere the coefficients a0 , aj are precisely those in Eq. (2).\\n11\\n\\n\\x0cθ\\n\\nLemma 4.4. The eigenvalues e±iα , of the operator T(U1 , U2 , θ) are solutions to the equation\\n\\x12\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\x13\\nx X 2\\nx + θj\\nx\\nx − θj\\n2\\na0 cot +\\naj cot\\n+ cot\\n− tan2 θ tan\\n= 0.\\n2\\n2\\n2\\n2\\nj\\n′\\n′\\nThe corresponding unnormalized eigenvectors |wα,θ i = |1i|µi + i|wα,θ\\ni satisfy h1, µ|wα,θ\\ni = 0 and\\n′\\n|wα,θ\\ni\\n\\n\\uf8f6\\n\\x12 θ\\n\\x13\\n\\x13\\n\\x12 θ\\n\\x13\\n\\x13\\nθ\\nX \\x12\\nα\\n−\\nθ\\nα\\n+\\nθ\\nα\\nj\\nj\\n\\uf8f8\\naj cot\\n|φ0 i +\\n|φ+\\n|φ−\\n= cos θ |θ⊥ i \\uf8eda0 cot\\nj i + cot\\nj i\\n2\\n2\\n2\\nj\\n\\x12\\n\\x12 θ\\x13 \\x13\\nα\\n+ sin θ |θi tan\\n|µi .\\n2\\n\\uf8eb\\n\\n\\x12\\n\\nProof. We apply Lemma 3.11 from Section 3.3 with aθ0 = a0 cos θ, aθj = aj cos θ, aθ−1 = sin θ. Note that U2\\ndoes not have any (−1)-eigenvectors (by assumption), but U2θ does.\\nThe target vector in the modified algorithm is |1i|µi. The start state is chosen to be |φe0,θ i = |θ⊥ i|φe0 i.\\nThe following are analogous to Lemmata 4.1 and 4.2:\\nCorollary 4.5. The vectors |1i|µi and |φe0,θ i have the following representation in the basis {|wα,θ i} consisting\\nof the eigenvectors of T(U1 , U2 , θ) as given by Corollary 4.4:\\n|1i|µi =\\n\\nX\\nαθ\\n\\n1\\nkwα,θ k\\n\\n|wα,θ i,\\n2\\n\\nand\\n\\n|φe0,θ i = (a0 i cos θ)\\n\\nX cot( αθ )\\n2\\n|wα,θ i.\\nkw\\nk2\\nα,θ\\nθ\\nα\\n\\nCorollary 4.6. The inner product of the target state |1i|µi and the T(U1 , U2 , θ)-rotation of |φe0,θ i is given\\nθ\\nP\\ncot( α )\\nby the expression (2a0 cos θ) αθ >0 kwα,θ2k2 .\\n\\nWe choose for the rest of this section θ ∈ [0, π/2] such that tan θ = a0 cot(α1 /2)/10. Let αθ1 be the\\nsmallest positive eigenphase of the modified search algorithm T(U1 , U2 , θ).\\nLemma 4.7 (proof in Appendix C) proves that αθ1 is of the same order as the principal eigenphase α1 of\\nthe original algorithm U2 U1 . Lemma 4.8 (proof in Appendix D) is the final piece in our argument. It relates\\nthe norm of the principal eigenvectors of the modified walk to the norm of the original ones. Both lemmas\\nextend corresponding results by Tulsi in the case of the 2D grid.\\n\\nLemma 4.7. There is a unique eigenvalue phase αθ1 of the operator T(U1 , U2 , θ) in (0, α1 ]. Moreover,\\ncot(αθ1 /2) ≤ 1.01 × cot(α1 /2). Therefore if 0 ≤ α1 ≤ π/4, then 0.78 × α1 ≤ αθ1 ≤ α1 .\\nLemma 4.8. kw±α1 ,θ k ≤ (3 cos θ) × kw±α1 k.\\nWe have all the ingredients for the main result of this section.\\nTheorem 4.9. Let ε > 0 be any constant. Suppose that the squared length of the projection of the state |φe0 i\\nonto the principal eigenspace of U2 U1 is bounded below by 1 − ε. Then, for every T ≥ QHTε (U2 , |φe0 i)/0.78,\\nthe procedure Rotate(T(U1 , U2 , θ), 1/T, 1/4) maps |φe0,θ i to a state with constant overlap with the target\\nstate |1i|µi.\\nProof. First we prove that T = QHTε (U2θ , |1i|µi) is of the order of QHTε (U2 , |µi). Let l = 2a20 (cot2 α21 )/kwα1 k2 .\\nWe know that l ≥ 1 − ε. Using Lemma 4.1 we get that QHTε (U2 , |µi) = 1/α1 . Moreover, by definition,\\nT ≤ 1/αθ1 . By Lemma 4.7, T ≤ 1/(0.78α1) = QHTε (U2 , |µi)/0.78. We now get our conclusion by applying\\nCorollary 4.6, Lemmata 4.7 and 4.8, and Theorem 3.9.\\nWe combine the above theorem with Lemma 3.12 to get our final result.\\n12\\n\\n\\x0cCorollary 4.10. Let P be a state-transitive reversible ergodic Markov chain, and let z be any state. Set\\n|µi = |zi|pz i, U1 = I − 2|µihµ|, and let U2 be the quantum analogue of P . Then for every ε ≤ 1/2 and\\nT ≥ QHTε (U2 , |φe0 i)/0.78, the procedure Rotate(T(U1 , U2 , θ), 1/T, 1/4) maps |φe0,θ i to a state with constant\\noverlap with the target state |1i|µi.\\nProof. The proof is direct once we realize that the conclusions of Lemma 3.12 for a particular element z\\nremain valid for any element because of the state-transitivity of P .\\n\\nReferences\\n[AA05]\\n\\nS. Aaronson and A. Ambainis. Quantum search of spatial regions. Theory of Computing,\\n1(4):47–79, 2005.\\n\\n[AAKV01] D. Aharonov, A. Ambainis, J. Kempe, and U. Vazirani. Quantum walks on graphs. Proceedings\\nof the 33rd Annual ACM Symposium on Theory of Computing, pages 50–59, 2001.\\n[ABN+ 01] A. Ambainis, E. Bach, A. Nayak, A. Vishwanath, and J. Watrous. One-dimensional quantum\\nwalks. Proceedings of the 33rd Annual ACM Symposium on Theory of Computing, pages 37–49,\\n2001.\\n[AKR05]\\n\\nA. Ambainis, J. Kempe, and A. Rivosh. Coins make quantum walks faster. In Proceedings of\\nthe Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1099–1108, 2005.\\n\\n[Amb03]\\n\\nA. Ambainis. Quantum walks and their algorithmic applications. International Journal of\\nQuantum Information, 1:507–518, 2003.\\n\\n[Amb04]\\n\\nA. Ambainis. Quantum walk algorithm for Element Distinctness. Proceedings of the 45th Symposium on Foundations of Computer Science, pages 22–31, 2004.\\n\\n[BŠ06]\\n\\nH. Buhrman and R. Špalek. Quantum verification of matrix products. Proceedings of the\\nSeventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 880–889, 2006.\\n\\n[CEMM98] R. Cleve, A. Ekert, C. Macchiavello, and M. Mosca. Quantum algorithms revisited. Proceedings\\nof the Royal Society of London, Series A, 454:339–354, 1998.\\n[CG04]\\n\\nA. Childs and J. Goldstone. Spatial search and the Dirac equation. Physical Review A, 70:042312,\\n2004.\\n\\n[Gro96]\\n\\nL. Grover. A fast quantum mechanical algorithm for database search. Proceedings of the TwentyEighth Annual ACM Symposium on the Theory of Computing, pages 212–219, 1996.\\n\\n[Kit95]\\n\\nA. Kitaev. Quantum measurements and the Abelian stabilizer problem. ECCC technical report\\n96-003 and arXiv.org e-print quant-ph/9511026, 1995.\\n\\n[MN07]\\n\\nF. Magniez and A. Nayak. Quantum complexity of testing group commutativity. Algorithmica,\\n48(3):221–232, 2007.\\n\\n[MNRS07] F. Magniez, A. Nayak, J. Roland, and M. Santha. Search via quantum walk. Proceedings of the\\n39th ACM Symposium on Theory of Computing, pages 575–584, 2007.\\n[MSS07]\\n\\nF. Magniez, M. Santha, and M. Szegedy. Quantum algorithms for the triangle problem. SIAM\\nJournal on Computing, 37(2):611–629, 2007.\\n\\n[NV00]\\n\\nA. Nayak and A. Vishwanath. Quantum walk on the line. Technical Report quant-ph/0010117,\\narXiv, 2000.\\n\\n13\\n\\n\\x0c[San08]\\n\\nM. Santha. Quantum walk based search algorithms. In Proceedings of the 5th Annual Conference\\non Theory and Applications of Models of Computation, volume 4978, pages 31–46. LNCS, 2008.\\n\\n[SKW03]\\n\\nN. Shenvi, J. Kempe, and K. Whaley. A quantum random walk search algorithm. Physical\\nReview A, 67:052307, 2003.\\n\\n[Sze04]\\n\\nM. Szegedy. Quantum speed-up of Markov chain based algorithms. Proceedings of the 45th\\nSymposium on Foundations of Computer Science, pages 32–41, 2004.\\n\\n[Tul08]\\n\\nA. Tulsi. Faster quantum walk algorithm for the two dimensional spatial search. Physical Review\\nA, 2008. To appear.\\n\\nA\\n\\nProof of Theorem 2.2\\n\\n\\x01\\nProof. For the first statement, we set k = 4 ln 2ε HTε/2 (P, z), and we denote by sk the probability that\\nz is not reached in P\\nthe first k steps. The claim follows if we show that sk ≤ ε. It is not hard to see that\\n†\\nk\\nsk = π−z\\nP−z\\nu−z = j νj2 (cos θj )k . We bound sk from above as follows:\\nX\\nX\\nsk ≤\\nνj2 +\\nνj2 (cos θj )k\\nj:1/θj2 >HTε/2 (P,z)\\n\\n\\x12\\n≤ ε/2 + 1 −\\n\\nj:1/θj2 ≤HTε/2 (P,z)\\n\\n1\\n4 HTε/2 (P, z)\\n\\n\\x13k\\n\\n.\\n\\nThe first summation is at most ε/2 by the definition of HTε/2 (P, z), and the bound for the second summation\\nis justified since 1/θj2 ≤ HTε/2 (P, z) implies cos θj ≤ 1 − 1/(4HTε/2 (P, z)). Thus the second term is also at\\nmost ε/2 by the choice of k.\\nFor the second statement, set k = 21 hε/3 (P, z). Then using the definition of hε/3 (P, z) and bounding\\n(1 − 1/2k)2k from below by 1/3, we get\\n\\x12\\n\\x132k\\nX\\nX\\nX\\n1\\nε\\n1\\n2\\n2\\n2k\\nνj 1 −\\nνj2 .\\nνj (cos θj )\\n>\\n≥\\n≥ s2k =\\n3\\n2k\\n3\\n1\\n1\\nj\\nj:cos θj >1− 2k\\n\\nj:cos θj >1− 2k\\n\\n1\\nNow observe that if 1/θj2 > k, then cos θj > 1− 2k\\n. Therefore Pr[Hz > k] ≤ ε, and the statement follows.\\n\\nB\\n\\nProof of Lemma 3.12\\n\\nProof. We know from Lemma 3.11 that if\\n|µz i\\nthen\\na20,z\\n\\nα\\ncot\\n2\\n\\n=\\n\\n=\\n\\n−\\n\\na0,z |φ0 i +\\nX\\n\\na2j,z\\n\\nj\\n\\nX\\nj\\n\\n\\x01\\n−\\naj,z |φ+\\nj i + |φj i + a−1,z |φi,\\n\\n\\x12 \\x12\\n\\x13\\n\\x12\\n\\x13\\x13\\nα + θj\\nα\\nα − θj\\ncot\\n+ cot\\n+ a2−1,z tan ,\\n2\\n2\\n2\\n\\nwhere 0 < αz < θ1 ≤ θ2 ≤ . . . < π. Since\\n\\x13\\n\\x12\\n\\x13\\x13\\n\\x12 \\x12\\nα − θj\\nα + θj\\n+ cot\\n− cot\\n2\\n2\\n\\n=\\n\\n2 sin α\\n,\\ncos α − cos θj\\n\\nthis is equivalent to\\na20,z cot\\n\\nα\\n2\\n\\n=\\n\\nX\\nj\\n\\na2j,z\\n\\n2 sin α\\nα\\n+ a2−1,z tan .\\ncos α − cos θj\\n2\\n14\\n\\n(4)\\n\\n\\x0cWe first claim that there exists z such that |a0,z |2 ≤ |a1,z |2 . Indeed, since |φ0 i belongs to Span(M ),\\nX\\nX\\nX\\nX\\na20,z =\\nhµz |φ0 i = 1 =\\nhµz |φ+\\n=\\na21,z .\\n1i\\nz\\n\\nz\\n\\nz\\n\\nz\\n\\nFix now arbitrarily such a z. To simplify the notation, from now on we refer to αz and to the coefficients\\naj,z without the subscript z. Since all of the terms on the right hand side of Eq. (4) are positive, it follows\\nthat\\n2 sin α\\nα\\n.\\n(5)\\n≥\\ncot\\n2\\ncos α − cos θ1\\nSince the right hand side decreases if θ1 is replaced by some θ1 < θ ≤ π, we obtain for every j > 1,\\ncot\\nand\\ncot\\n\\nα\\n2\\n\\n≥\\n\\nα\\n2\\n\\n2 sin α\\n,\\ncos α − cos θj\\n\\n≥\\n\\n2 sin α\\ncos α − cos π\\n\\n=\\n\\n2 tan\\n\\nα\\n2\\n\\n(6)\\n\\n>\\n\\ntan\\n\\nα\\n.\\n2\\n\\n(7)\\n\\n′\\nWe also know that the eigenvectors |w±α i = |µi + i|w±α\\ni corresponding to the eigenvalues e±iα satisfy\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n\\x13\\nX \\x12\\n±α − θj\\n±α\\n±α + θj\\n±α\\n+\\n−\\n′\\naj cot\\n|φj i + cot\\n|φj i − a−1 tan\\n|φ0 i +\\n|φi.\\n|w±α i = a0 cot\\n2\\n2\\n2\\n2\\nj\\n\\nLet us now define the vector |si in the two dimensional space generated by |w±α i by\\n|si\\n\\n|wα i − |w−α i\\n.\\ni\\n\\n=\\n\\nObserve that |µi is orthogonal to |w±α i and therefore to |si. Then the length of the projection of |φe0 i to\\nthe subspace is the same as the one of |φ0 i. This is at least |hs|φ0 i|/ksk, which we now bound from below.\\nSince the functions tan and cot are odd, we get\\n\\x12\\n\\x13\\x13\\nX \\x12 \\x12 α − θj \\x13\\n\\x01\\nα\\nα + θj\\nα\\n−\\n|si = 2a0 cot |φ0 i +\\naj cot\\n+ cot\\n|φ+\\n|φi,\\nj i + |φj i − 2a−1 tan\\n2\\n2\\n2\\n2\\nj\\nand therefore\\nksk2\\n\\n=\\n\\n4a20 cot2\\n\\nX\\nsin2 α\\nα\\nα\\na2j\\n+ 4a2−1 tan2 .\\n+8\\n2\\n2\\n(cos\\nα\\n−\\ncos\\nθ\\n)\\n2\\nj\\nj\\n\\nFrom (4), (5), (6), and (7) it follows that\\na20 cot2\\n\\nα\\n2\\n\\n≥\\n\\n2\\n\\nX\\nj\\n\\na2j\\n\\nsin2 α\\nα\\n+ a2−1 tan2\\n2\\n(cos α − cos θj )\\n2\\n\\nand therefore\\nksk2\\n\\n8a20 cot2\\n\\n≤\\n\\nα\\n.\\n2\\n\\nSince hs|φ0 i = 2a0 cot α2 , we can indeed conclude that\\n|hs|φ0 i|\\nksk\\n\\n≥\\n\\n15\\n\\n1\\n√ .\\n2\\n\\n\\x0cC\\n\\nProof of Lemma 4.7\\n\\nProof. By the definition of α1 (Lemma 3.11),\\n\\x12\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\x13\\nα1 X 2\\nα1 + θj\\nα1 − θj\\na20 cot\\naj cot\\n+ cot\\n+\\n2\\n2\\n2\\nj\\n\\n=\\n\\n0.\\n\\n(8)\\n\\nFix any θ ≥ 0 and define the monotonically decreasing and continuous function f : (0, θ1 ) 7→ R:\\n\\x12\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\x13\\nx X 2\\nx + θj\\nx\\nx − θj\\nf (x) = a20 cot +\\naj cot\\n+ cot\\n− tan2 θ tan .\\n2\\n2\\n2\\n2\\nj\\nWe know that limx→0+ f (x) = +∞, limx→θ1 − f (x) = −∞, and f (α1 ) ≤ 0. Therefore there is a unique\\nαθ1 ∈ (0, α1 ] such that f (αθ1 ) = 0.\\nFrom the monotonicity of cot, for x ∈ (0, α1 ], we have\\n\\x13\\n\\x12\\n\\x13\\x13\\n\\x12\\n\\x13\\x13\\n\\x12\\nX \\x12 \\x12 α + θj \\x13\\nX \\x12\\nx − θj\\nα − θj\\nx + θj\\n2\\n2\\naj cot\\n+ cot\\n≥\\n+ cot\\n.\\naj cot\\n2\\n2\\n2\\n2\\nj\\nj\\nUsing this inequality together with Eq. (8) and the monotonicity of tan, we bound the function f from above\\nas follows:\\nf (x)\\n\\nx\\nα\\nx\\n− a20 cot − tan2 θ tan\\n2\\n2\\n2\\nα\\nα\\nx\\n2\\n2\\n2\\n≥ a0 cot − a0 cot − tan θ tan\\n2\\n2\\n2\\nx\\nα\\n2\\n2\\n≥ a0 cot − 1.01 × a0 cot ,\\n2\\n2\\n\\n≥ a20 cot\\n\\nwhere the last inequality comes from the hypothesis 0 ≤ tan θ ≤ a0 cot(α1 /2)/10. Since f (αθ1 ) = 0, we get\\nthat cot(αθ1 /2) ≤ 1.01 × cot(α1 /2).\\nWe now prove that f (0.78 × α1 ) ≤ 0, which concludes the proof. In the rest of the proof,\\n√ we restrict the\\nvariable x to the interval [0.78 × α1 , α1 ]. Let β be the solution of tan(β/2) = tan(α1 /2)/ 1.01 in [0, π/2).\\nThen f (β) ≥ 0, and therefore αθ1 ≥ β.\\nSince α1 ≥ 0, we have tan(α1 /2) ≥ α1 /2 and\\nβ\\n2\\n\\ntan\\n\\n=\\n\\ntan( α21 )\\n√\\n1.01\\n\\n≥\\n\\nα1\\n√\\n.\\n2 1.01\\n\\nMoreover since 0 ≤ β ≤ α1 ≤ π/4, we have tan(β/2) ≤ 2β/π, and therefore\\nβ\\n\\nD\\n\\n≥\\n\\nπ\\n√\\n×α\\n4 1.01\\n\\n≥\\n\\n0.78 × α1 .\\n\\nProof of Lemma 4.8\\n\\nProof. Since the two vectors |w±α1 ,θ i (respectively, |w±α1 i) have the same norm and are orthogonal, it\\nsuffices to upper bound the following squared norm:\\n\\uf8ee\\n\\uf8f9\\n\\x132\\nθ\\nθ\\nX \\x12\\nα\\nsin α1\\n+ tan2 θ tan2 1 \\uf8fb\\na2j\\nkwα1 ,θ − w−α1 ,θ k2 = 4 cos2 θ \\uf8f0a20 cot2 αθ1 /2 + 2\\n(9)\\nθ − cos θ\\n2\\ncos\\nα\\nj\\n1\\nj\\n16\\n\\n\\x0cBy Lemma 4.7,\\na20 cot2 αθ1 /2\\nand\\ntan2 θ tan2\\n\\nαθ1\\n2\\n\\n=\\n\\n1.01 a20 cot2 α1 /2\\n\\n≤\\n\\n0.01 a20 cot2\\n\\nα1\\nαθ\\ntan2 1\\n2\\n2\\n\\n≤\\n\\n0.01 a20\\n\\nby the choice of tan θ = a0 cot(α1 /2)/10 and the monotonicity of cot2 = 1/ tan2 on (0, π/2). Since\\nX\\nj\\n\\na2j\\n\\n\\x12\\n\\nsin αθ1\\ncos αθ1 − cos θj\\n\\n\\x132\\n\\n≤\\n\\nX\\nj\\n\\na2j\\n\\n\\x12\\n\\nsin α1\\ncos α1 − cos θj\\n\\n\\x132\\n\\nthe quantity inside of the square brackets of (9) is at most:\\n\\uf8eb\\n\\uf8f6\\n\\x132\\nX \\x12\\nsin α1\\na2j\\n2 \\uf8eda20 cot2 α1 /2 + 2\\n+ a20 \\uf8f8 .\\ncos\\nα\\n−\\ncos\\nθ\\n1\\nj\\nj\\nHence,\\nkwα1 ,θ − w−α1 ,θ k2\\n\\n≤\\n\\n(8 cos2 θ) × kwα1 − w−α1 k2 .\\n\\n17\\n\\n,\\n\\n(10)\\n\\n\\x0c', 'arXiv:0811.1254v1 [math.CO] 8 Nov 2008\\n\\nChapter 1\\nCoding theory and algebraic combinatorics\\n\\nMichael Huber∗\\nInstitut für Mathematik, Technische Universität Berlin,\\nStraße des 17. Juni 136, D-10623 Berlin, Germany,\\nmhuber@math.tu-berlin.de\\nThis chapter introduces and elaborates on the fruitful interplay of coding theory\\nand algebraic combinatorics, with most of the focus on the interaction of codes\\nwith combinatorial designs, finite geometries, simple groups, sphere packings,\\nkissing numbers, lattices, and association schemes. In particular, special interest\\nis devoted to the relationship between codes and combinatorial designs. We\\ndescribe and recapitulate important results in the development of the state of the\\nart. In addition, we give illustrative examples and constructions, and highlight\\nrecent advances. Finally, we provide a collection of significant open problems and\\nchallenges concerning future research.\\n\\n1.1. Introduction\\nThe classical publications “A mathematical theory of communication” by\\nC. E. Shannon [1] and “Error detecting and error correcting codes” by R. W. Hamming [2] gave birth to the twin disciplines of information theory and coding theory.\\nSince their inceptions the interactions of information and coding theory with many\\nmathematical branches have continually deepened. This is in particular true for the\\nclose connection between coding theory and algebraic combinatorics.\\nThis chapter introduces and elaborates on this fruitful interplay of coding theory and algebraic combinatorics, with most of the focus on the interaction of codes\\nwith combinatorial designs, finite geometries, simple groups, sphere packings, kissing numbers, lattices, and association schemes. In particular, special interest is\\ndevoted to the relationship between codes and combinatorial designs. Since we\\ndo not assume the reader is familiar with the theory of combinatorial designs, an\\naccessible and reasonably self-contained exposition is provided. Subsequently, we\\ndescribe and recapitulate important results in the development of the state of the\\nart, provide illustrative examples and constructions, and highlight recent advances.\\nFurthermore, we give a collection of significant open problems and challenges concerning future research.\\n∗ The\\n\\nauthor gratefully acknowledges support by the Deutsche Forschungsgemeinschaft (DFG).\\n1\\n\\n\\x0c2\\n\\nMichael Huber\\n\\nThe chapter is organized as follows. In Sec. 1.2, we give a brief account of\\nbasic notions of algebraic coding theory. Section 1.3 consists of the main part of\\nthe chapter: After an introduction to finite projective planes and combinatorial\\ndesigns, a subsection on basic connections between codes and combinatorial designs\\nfollows. The next subsection is on perfect codes and designs, and addresses further\\nrelated concepts. Subsection 1.3.4 deals with the classical Assmus-Mattson Theorem\\nand various analogues. A subsection on codes and finite geometries follows the\\ndiscussion on the non-existence of a projective plane of order 10. In Subsection 1.3.6,\\ninterrelations between the Golay codes, the Mathieu-Witt designs, and the Mathieu\\ngroups are studied. Subsection 1.3.7 deals with the Golay codes and the Leech\\nlattice, as well as recent milestones concerning kissing numbers and sphere packings.\\nThe last topic of this section considers codes and association schemes. The chapter\\nconcludes with sections on directions for further research as well as conclusions and\\nexercises.\\n1.2. Background\\nFor our further purposes, we give a short account of basic notions of algebraic coding\\ntheory. For additional information on the subject of algebraic coding theory, the\\nreader is referred to [3–13]. For some historical notes on its origins, see [14] and [6,\\nChap. 1], as well as [15] for a historical survey on coding theory and information\\ntheory.\\nWe denote by Fn the set of all n-tuples from a q-symbol alphabet. If q is a\\nprime power, we take the finite field F = Fq with q elements, and interpret Fn as\\nan n-dimensional vector space Fnq over Fq . The elements of Fn are called vectors\\n(or words) and will be denoted by bold symbols.\\nThe (Hamming) distance between two codewords x, y ∈ Fn is defined by the\\nnumber of coordinate positions in which they differ, i.e.\\nd(x, y) := |{i | 1 ≤ i ≤ n, xi 6= yi }| .\\nThe weight w(x) of a codeword x is defined by\\nw(x) := d(x, 0),\\nwhenever 0 is an element of F.\\nA subset C ⊆ Fn is called a (q-ary) code of length n (binary if q = 2, ternary if\\nq = 3). The elements of C are called codewords. A linear code (or [n, k] code) over\\nthe field Fq is a k-dimensional linear subspace C of the vector space Fnq . We note\\nthat large parts of coding theory are concerned with linear codes. In particular,\\nas many combinatorial configurations can be described by their incidence matrices,\\ncoding theorists have started in the early 1960’s to consider as codes the vector\\nspaces spanned by the rows of the respective incidence matrices over some given\\nfield.\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n3\\n\\nThe minimum distance d of a code C is defined as\\nd := min {d(x, y) | x, y ∈ C, x 6= y}.\\nClearly, the minimum distance of a linear code is equal to its minimum weight , i.e.\\nthe minimum of the weights of all non-zero codewords. An [n, k, d] code is an [n, k]\\ncode with minimum distance d.\\nThe minimum distance of a (not necessarily linear) code C determines the errorcorrecting capability of C: If d = 2e + 1, then C is called an e-error-correcting code.\\nDefining by\\nSe (x) := {y ∈ Fn | d(x, y) ≤ e}\\nthe sphere (or ball ) of radius e around a codeword x of C, this implies that the\\nspheres of radius e around distinct codewords are disjoint.\\nCounting the number of codewords in a sphere of radius e yields to the subsequent sphere packing (or Hamming) Bound .\\nTheorem 1.1. Let C be a q-ary code of length n and minimum distance d = 2e + 1.\\nThen\\ne \\x12 \\x13\\nX\\nn\\n(q − 1)i ≤ q n .\\n|C| ·\\ni\\ni=0\\nIf equality holds, then C is called a perfect code. Equivalently, C is perfect if\\nthe spheres of radius e around all codewords cover the whole space Fn . Certainly,\\nperfect codes are combinatorially interesting objects, however, they are extremely\\nrare.\\nWe will call two codes (permutation) equivalent if one is obtained from the other\\nby applying a fixed permutation to the coordinate positions for all codewords. A\\ngenerator matrix G for an [n, k] code C is a (k × n)-matrix for which the rows are\\na basis of C. We say that G is in standard form if G = (Ik , P ), where Ik is the\\n(k × k) identity matrix.\\nFor an [n, k] code C, let\\n\\nC ⊥ := {x ∈ Fnq | ∀y∈C [hx, yi = 0]}\\ndenote the dual code of C, where hx, yi is the standard inner (or dot) product in\\nFnq . The code C ⊥ is an [n, n − k] code. If H is a generator matrix for C ⊥ , then\\nclearly\\nC = {x ∈ Fnq | xH T = 0},\\nand H is called a parity check matrix for the code C. If G = (Ik , P ) is a generator\\nmatrix of C, then H = (−P T , In−k ) is a parity check matrix of C. A code C is\\ncalled self-dual if C = C ⊥ . If C ⊂ C ⊥ , then C is called self-orthogonal .\\n\\n\\x0c4\\n\\nMichael Huber\\n\\nIf C is a linear code of length n over Fq , then\\nC := {(c1 , . . . , cn , cn+1 ) | (c1 , . . . , cn ) ∈ C,\\n\\nn+1\\nX\\n\\nci = 0}\\n\\ni=1\\n\\ndefines the extended code corresponding to C. The symbol cn+1 is called the overall\\nparity check symbol . Conversely, C is the punctured (or shortened ) code of C.\\nThe weight distribution of a linear code C of length n is the sequence {Ai }ni=0 ,\\nwhere Ai denotes the number of codewords in C of weight i. The polynomial\\nA(x) :=\\n\\nn\\nX\\n\\nAi xi\\n\\ni=0\\n\\nis the weight enumerator of C.\\nThe weight enumerators of a liner code and its dual code are related, as shown\\nby the following theorem, which is one of the most important results in the theory\\nof error-correcting codes.\\nTheorem 1.2. (MacWilliams [16]). Let C be an [n, k] code over Fq with weight\\nenumerator A(x) and let A⊥ (x) be the weight enumerator of the dual code C ⊥ .\\nThen\\n\\x01\\n1−x\\nA⊥ (x) = q −k (1 + (q − 1)x)n A\\n.\\n1 + (q − 1)x\\nWe note that the concept of the weight enumerator can be generalized to nonlinear codes (so-called distance enumerator, cf. [17, 18] and Subsection 1.3.8).\\nAn [n, k] code C over Fq is called cyclic if\\n∀(c0 ,c1 ,...,cn−1 )∈C [(cn−1 , c0 , . . . , cn−2 ) ∈ C],\\ni.e. any cyclic shift of a codeword is again a codeword. We adopt the usual convention for cyclic codes that n and q are coprime. Using the isomorphism\\n(a0 , a1 , . . . , an−1 ) ⇄ a0 + a1 x + . . . + an−1 xn−1\\nbetween Fnq and the residue class ring Fq [x]/(xn − 1), it follows that a cyclic code\\ncorresponds to an ideal in Fq [x]/(xn − 1).\\n1.3. Thoughts for Practitioners\\nIn the following, we introduce and elaborate on the fruitful interplay of coding\\ntheory and algebraic combinatorics, with most of the focus on the interaction of\\ncodes with combinatorial designs, finite geometries, simple groups, sphere packings,\\nkissing numbers, lattices, and association schemes. In particular, special interest is\\ndevoted to the relationship between codes and combinatorial designs. We give an\\naccessible and reasonably self-contained exposition in the first subsection as we do\\nnot assume the reader is familiar with the theory of combinatorial designs. In what\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n5\\n\\nfollows, we describe and recapitulate important results in the development of the\\nstate of the art. In addition, we give illustrative examples and constructions, and\\nhighlight recent achievements.\\n1.3.1. Introduction to finite projective planes and combinatorial designs\\nCombinatorial design theory is a subject of considerable interest in discrete mathematics. We give in this subsection an introduction to the topic, with emphasis on\\nthe construction of some important designs. For a more general treatment of combinatorial designs, the reader is referred to [19–24]. In particular, [19, 21] provide\\nencyclopedias on key results.\\nBesides coding theory, there are many interesting connections of design theory\\nto other fields. We mention in our context especially its links to finite geometries [25], incidence geometry [26], group theory [27–30], graph theory [4, 31], cryptography [32–34], as well as classification algorithms [35]. In addition to that, we\\nrecommend [22, 36–39] for the reader interested in the broad area of combinatorics\\nin general.\\nWe start by introducing several notions.\\nDefinition 1.1. A projective plane of order n is a pair of points and lines such\\nthat the following properties hold:\\n(i) any two distinct points are on a unique line,\\n(ii) any two distinct lines intersect in a unique point,\\n(iii) there exists a quadrangle, i.e. four points no three of which are on a common\\nline,\\n(iv) there are n + 1 points on each line, n + 1 lines through each point and the total\\nnumber of points, respectively lines, is n2 + n + 1.\\nIt follows easily from (i), (ii), and (iii) that the number of points on a line is a\\nconstant. When setting this constant equal to n + 1, then (iv) is a consequence of\\n(i) and (iii).\\nCombinatorial designs can be regarded as generalizations of projective planes:\\nDefinition 1.2. For positive integers t ≤ k ≤ v and λ, we define a t-design, or\\nmore precisely a t-(v, k, λ) design, to be a pair D = (X, B), where X is a finite set\\nof points, and B a set of k-element subsets of X called blocks, with the property\\nthat any t points are contained in precisely λ blocks.\\nWe will denote points by lower-case and blocks by upper-case Latin letters. Via\\nconvention, we set v := |X| and b := |B|. Throughout this chapter, ‘repeated blocks’\\nare not allowed, that is, the same k-element subset of points may not occur twice\\nas a block. If t < k < v holds, then we speak of a non-trivial t-design.\\n\\n\\x0c6\\n\\nMichael Huber\\n\\nDesigns may be represented algebraically in terms of incidence matrices: Let\\nD = (X, B) be a t-design, and let the points be labeled {x1 , . . . , xv } and the blocks\\nbe labeled {B1 , . . . , Bb }. Then, the (b × v)-matrix A = (aij ) (1 ≤ i ≤ b, 1 ≤ j ≤ v)\\ndefined by\\n\\x1a\\n1, if xj ∈ Bi\\naij :=\\n0, otherwise\\nis called an incidence matrix of D. Clearly, A depends on the respective labeling,\\nhowever, it is unique up to column and row permutation.\\nIf D1 = (X1 , B1 ) and D2 = (X2 , B2 ) are two t-designs, then a bijective map\\n:\\nα X1 −→ X2 is called an isomorphism of D1 onto D2 , if\\nB ∈ B1 ⇐⇒ α(B) ∈ B2 .\\nIn this case, the designs D1 and D2 are isomorphic. An isomorphism of a design D\\nonto itself is called an automorphism of D. Evidently, the set of all automorphisms\\nof a design D form a group under composition, the full automorphism group of D.\\nAny subgroup of it will be called an automorphism group of D.\\n\\nIf D = (X, B) is a t-(v, k, λ) design with t ≥ 2, and x ∈ X arbitrary,\\nthen the derived design with respect to x is Dx = (Xx , Bx ), where Xx = X\\\\{x},\\nBx = {B\\\\{x} | x ∈ B ∈ B}. In this case, D is also called an extension of Dx . Obviously, Dx is a (t − 1)-(v − 1, k − 1, λ) design. The complementary design D is\\nobtained by replacing each block of D by its complement.\\nFor historical reasons, a t-(v, k, λ) design with λ = 1 is called a Steiner t-design.\\nSometimes this is also known as a Steiner system if the parameter t is clearly given\\nfrom the context.\\nThe special case of a Steiner design with parameters t = 2 and k = 3 is called\\na Steiner triple system of order v (briefly ST S(v)). The question regarding their\\nexistence was posed in the classical “Combinatorische Aufgabe” (1853) of the nineteenth century geometer Jakob Steiner [40]:\\n“Welche Zahl, N , von Elementen hat die Eigenschaft, dass sich die Elemente\\nso zu dreien ordnen lassen, dass je zwei in einer, aber nur in einer Verbindung\\nvorkommen?”\\n\\nHowever, there had been earlier work on these particular designs going back to,\\nin particular, J. Plücker, W. S. B. Woolhouse, and most notably T. P. Kirkman.\\nFor an account on the early history of designs, see [21, Chap. I.2] and [41].\\nA Steiner design with parameters t = 3 and k = 4 is called a Steiner quadruple\\nsystem of order v (briefly SQS(v)).\\nIf a 2-design has equally many points and blocks, i.e. v = b, then we speak of\\na square design (as its incidence matrix is square). By tradition, square designs\\nare often called symmetric designs, although here the term does not imply any\\nsymmetry of the design. For more on these interesting designs, see, e.g., [42].\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n7\\n\\nWe give some illustrative examples of finite projective planes and combinatorial\\ndesigns. We assume that q is always a prime power.\\nExample 1.1. Let us choose as point set\\nX = {1, 2, 3, 4, 5, 6, 7}\\nand as block set\\nB = {{1, 2, 4}, {2, 3, 5}, {3, 4, 6}, {4, 5, 7}, {1, 5, 6}, {2, 6, 7}, {1, 3, 7}}.\\nThis gives a 2-(7, 3, 1) design, the well-known Fano plane, the smallest design arising\\nfrom a projective geometry, which is unique up to isomorphism. We give the usual\\nrepresentation of this projective plane of order 2 by the following diagram:\\n2\\n\\n4\\n\\n6\\n\\n1\\n\\n5\\n\\n7\\nFig. 1.1.\\n\\n3\\n\\nFano plane\\n\\nExample 1.2. We take as point set\\nX = {1, 2, 3, 4, 5, 6, 7, 8, 9}\\nand as block set\\nB = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}, {1, 4, 7}, {2, 5, 8}, {3, 6, 9},\\n{1, 5, 9}, {2, 6, 7}, {3, 4, 8}, {1, 6, 8}, {2, 4, 9}, {3, 5, 7}}.\\nThis gives a 2-(9, 3, 1) design, the smallest non-trivial design arising from an affine\\ngeometry, which is again unique up to isomorphism. This affine plane of order 3\\ncan be constructed from the array\\n123\\n456\\n789\\nas shown in Figure 1.2.\\n\\n\\x0c8\\n\\nMichael Huber\\n\\n7\\n\\n8\\n5\\n\\n4\\n1\\n\\nFig. 1.2.\\n\\n9\\n\\n2\\n\\n6\\n3\\n\\nAffine plane of order 3\\n\\nMore generally, we obtain:\\nExample 1.3. We choose as point set X the set of 1-dimensional subspaces of\\na vector space V = V (d, q) of dimension d ≥ 3 over Fq . As block set B we take\\nthe set of 2-dimensional subspaces of V . Then there are v = (q d − 1)/(q − 1)\\npoints and each block B ∈ B contains k = q + 1 points. Since obviously any\\ntwo 1-dimensional subspaces span a single 2-dimensional subspace, any two distinct\\npoints are contained in a unique block. Thus, the projective space P G(d − 1, q)\\nd\\n−1\\nis an example of a 2-( qq−1\\n, q + 1, 1) design. For d = 3, the particular designs are\\nprojective planes of order q, which are square designs. More generally, for any fixed\\ni with 1 ≤ i ≤ d − 2, the points and i-dimensional subspaces of P G(d − 1, q) (i.e.\\nthe (i + 1)-dimensional subspaces of V ) yield a 2-design.\\nExample 1.4. We take as point set X the set of elements of a vector space\\nV = V (d, q) of dimension d ≥ 2 over Fq . As block set B we choose the set of affine\\nlines of V (i.e. the translates of 1-dimensional subspaces of V ). Then there are\\nv = q d points and each block B ∈ B contains k = q points. As clearly any two\\ndistinct points lie on exactly one line, they are contained in a unique block. Hence,\\nwe obtain the affine space AG(d, q) as an example of a 2-(q d , q, 1) design. When\\nd = 2, these designs are affine planes of order q. More generally, for any fixed i with\\n1 ≤ i ≤ d − 1, the points and i-dimensional subspaces of AG(d, q) form a 2-design.\\nRemark 1.1. It is well-established that both affine and projective planes of order\\nn exist whenever n is a prime power. The conjecture that no such planes exist\\nwith orders other than prime powers is unresolved so far. The classical result of\\nR. H. Bruck and H. J. Ryser [43] still remains the only general statement: If n ≡ 1\\nor 2 (mod 4) and n is not equal to the sum of two squares of integers, then n does\\nnot occur as the order of a finite projective plane. The smallest integer that is not a\\nprime power and not covered by the Bruck-Ryser Theorem is 10. Using substantial\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n9\\n\\ncomputer analysis, C. W. H. Lam, L. Thiel, and S. Swiercz [44] proved the nonexistence of a projective plane of order 10 (cf. Remark 1.10). The next smallest\\nnumber to consider is 12, for which neither a positive nor a negative answer has\\nbeen proved.\\nNeedless to mention that — apart from the existence problem — the question on\\nthe number of different isomorphism types (when existent) is fundamental. There\\nare, for example, precisely four non-isomorphic projective planes of order 9. For a\\nfurther discussion, in particular of the rich history of affine and projective planes,\\nwe refer, e.g., to [25, 45–49].\\nExample 1.5. We take as points the vertices of a 3-dimensional cube. As illustrated\\nin Figure 1.3, we can choose three types of blocks:\\n(i) a face (six of these),\\n(ii) two opposite edges (six of these),\\n(iii) an inscribed regular tetrahedron (two of these).\\nThis gives a 3-(8, 4, 1) design, which is unique up to isomorphism.\\n\\nFig. 1.3.\\n\\nSteiner quadruple system of order 8\\n\\nWe have more generally:\\nExample 1.6. In AG(d, q) any three distinct points define a plane unless they are\\ncollinear (that is, lie on the same line). If the underlying field is F2 , then the lines\\ncontain only two points and hence any three points cannot be collinear. Therefore,\\nthe points and planes of the affine space AG(d, 2) form a 3-(2d , 4, 1) design. More\\ngenerally, for any fixed i with 2 ≤ i ≤ d − 1, the points and i-dimensional subspaces\\nof AG(d, 2) form a 3-design.\\nExample 1.7. The unique 2-(9, 3, 1) design whose points and blocks are the points\\nand lines of the affine plane AG(2, 3) can be extended precisely three times to the\\nfollowing designs which are also unique up to isomorphism: the 3-(10, 4, 1) design\\nwhich is the Möbius plane of order 3 with P Γ L(2, 9) as full automorphism group,\\nand the two Mathieu-Witt designs 4-(11, 5, 1) and 5-(12, 6, 1) with the sporadic\\nMathieu groups M11 and M12 as point 4-transitive and point 5-transitive full automorphism groups, respectively.\\n\\n\\x0c10\\n\\nMichael Huber\\n\\nTo construct the ‘large’ Mathieu-Witt designs one starts with the\\n2-(21, 5, 1) design whose points and blocks are the points and lines of the projective plane P G(2, 4). This can be extended also exactly three times to the following unique designs: the Mathieu-Witt design 3-(22, 6, 1) with Aut(M22 ) as point\\n3-transitive full automorphism group as well as the Mathieu-Witt designs 4-(23, 7, 1)\\nand 5-(24, 8, 1) with M23 and M24 as point 4-transitive and point 5-transitive full\\nautomorphism groups, respectively.\\nThe five Mathieu groups were the first sporadic simple groups and were discovered by E. Mathieu [50, 51] over one hundred years ago. They are the only\\nfinite 4- and 5-transitive permutation groups apart from the symmetric or alternating groups. The Steiner designs associated with the Mathieu groups were first\\nconstructed by both R. D. Carmichael [28] and E. Witt [52], and their uniqueness\\nestablished up to isomorphism by Witt [53]. From the meanwhile various alternative constructions, we mention especially those of H. Lüneburg [54] and M. Aschbacher [55, Chap. 6]. However, the easiest way to construct and prove uniqueness\\nof the Mathieu-Witt designs is via coding theory, using the related binary and\\nternary Golay codes (see Subsection 1.3.6).\\nRemark 1.2. By classifying Steiner designs which admit automorphism groups\\nwith sufficiently strong symmetry properties, specific characterizations of the\\nMathieu-Witt designs with their related Mathieu groups were obtained (see,\\ne.g., [56–61] and [62, Chap. 5] for a survey).\\nRemark 1.3. We mention that, in general, for t = 2 and 3, there are many infinite\\nclasses of Steiner t-designs, but for t = 4 and 5 only a finite number are known.\\nAlthough L. Teirlinck [63] has shown that non-trivial t-designs exist for all values\\nof t, no Steiner t-designs have been constructed for t ≥ 6 so far.\\nIn what follows, we need some helpful combinatorial tools:\\nA standard combinatorial double counting argument gives the following assertions.\\nLemma 1.1. Let D = (X, B) be a t-(v, k, λ) design. For a positive integer s ≤ t,\\nlet S ⊆ X with |S| = s. Then the total number λs of blocks containing all the points\\nof S is given by\\n\\x01\\nv−s\\nλs = λ\\n\\nt−s\\n\\x01.\\nk−s\\nt−s\\n\\nIn particular, for t ≥ 2, a t-(v, k, λ) design is also an s-(v, k, λs ) design.\\nFor historical reasons, it is customary to set r := λ1 to be the total number of\\nblocks containing a given point (referring to the ‘replication number’ from statistical\\ndesign of experiments, one of the origins of designs theory).\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n11\\n\\nLemma 1.2. Let D = (X, B) be a t-(v, k, λ) design. Then the following holds:\\n(a) bk = vr.\\n\\x12 \\x13\\n\\x12 \\x13\\nv\\nk\\n(b)\\nλ=b\\n.\\nt\\nt\\n(c) r(k − 1) = λ2 (v − 1) for t ≥ 2.\\nSince in Lemma 1.1 each λs must be an integer, we have moreover the subsequent\\nnecessary arithmetic conditions.\\nLemma 1.3. Let D = (X, B) be a t-(v, k, λ) design. Then\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\nv−s\\nk−s\\nλ\\n≡ 0 (mod\\n)\\nt−s\\nt−s\\nfor each positive integer s ≤ t.\\nThe following theorem is an important result in the theory of designs, generally\\nknown as Fisher’s Inequality.\\nTheorem 1.3. (Fisher [64]). If D = (X, B) is a non-trivial t-(v, k, λ) design with\\nt ≥ 2, then we have b ≥ v, that is, there are at least as many blocks as points in D.\\nWe remark that equality holds exactly for square designs when t = 2. Obviously,\\nthe equality v = b implies r = k by Lemma 1.2 (a).\\n1.3.2. Basic connections between codes and combinatorial designs\\nThere is a rich and fruitful interplay between coding theory and design theory. In\\nparticular, many t-designs have been found in the last decades by considering the\\ncodewords of fixed weight in some special, often linear codes. As we will see in the\\nsequel, these codes typically exhibit a high degree of regularity. There is an amount\\nof literature [4, 7, 13, 31, 65–72] discussing to some extent in more detail various\\nrelations between codes and designs.\\nFor a codeword x ∈ Fn , the set\\nsupp(x) := {i | xi 6= 0}\\nof all coordinate positions with non-zero coordinates is called the support of x. We\\nshall often form a t-design of a code in the following way: Given a (usually linear)\\ncode of length n, which contains the zero vector, and non-zero weight w, we choose\\nas point set X the set of n coordinate positions of the code and as block set B the\\nsupports of all codewords of weight w.\\nSince we do not allow repeated blocks, clearly only distinct representatives of\\nsupports for codewords with the same supports are taken in the non-binary case.\\n\\n\\x0c12\\n\\nMichael Huber\\n\\nWe give some elementary examples.\\nExample 1.8. The matrix\\n11\\n\\uf8ec0 1\\nG=\\uf8ec\\n\\uf8ed0 0\\n00\\n\\uf8eb\\n\\n01\\n10\\n11\\n01\\n\\n\\uf8f6\\n000\\n1 0 0\\uf8f7\\n\\uf8f7\\n0 1 0\\uf8f8\\n101\\n\\nis a generator matrix of a binary [7, 4, 3] Hamming code, which is the smallest\\nnon-trivial Hamming code (see also Example 1.12). This code is a perfect singleerror-correcting code with weight distribution A0 = A7 = 1, A3 = A4 = 7. The\\nseven codewords of weight 3 are precisely the seven rows of the incidence matrix\\n\\uf8f6\\n\\uf8eb\\n1101000\\n\\uf8ec0 1 1 0 1 0 0\\uf8f7\\n\\uf8f7\\n\\uf8ec\\n\\uf8ec0 0 1 1 0 1 0\\uf8f7\\n\\uf8f7\\n\\uf8ec\\n\\uf8f7\\n\\uf8ec\\n\\uf8ec0 0 0 1 1 0 1\\uf8f7\\n\\uf8f7\\n\\uf8ec\\n\\uf8ec1 0 0 0 1 1 0\\uf8f7\\n\\uf8f7\\n\\uf8ec\\n\\uf8ed0 1 0 0 0 1 1\\uf8f8\\n1010001\\nof the Fano plane P G(2, 2) of Fig. 1.1. The supports of the seven codewords of\\nweight 4 yield the complementary 2-(7, 4, 2) design, i.e. the biplane of order 2.\\nExample 1.9. The matrix (I4 , J4 −I4 ), where J4 denotes the (4×4) all-one matrix,\\ngenerates the extended binary [8, 4, 4] Hamming code. This code is self-dual and\\nhas weight distribution A0 = A8 = 1, A4 = 14. As any two codewords of weight\\n4 have distance at least 4, they have at most two 1’s in common, and hence no\\ncodeword of weight 3 can\\n\\x01 appear as a subword of more than one codeword. On the\\nother hand, there are 83 = 56 words of weight 3 and each codeword of weight 4 has\\nfour subwords of weight 3. Hence each codeword of weight 3 is a subword of exactly\\none codeword of weight 4. Therefore, the supports of the fourteen codewords of\\nweight 4 form a 3-(8, 4, 1) design, which is the unique SQS(8) (cf. Example 1.5).\\nWe give also a basic example of a non-linear code constructed from design theory.\\nExample 1.10. We take the rows of an incidence matrix of the (unique) Hadamard\\n2-(11, 5, 2) design, and adjoin the all-one codeword. Then, the twelve codewords\\nhave mutual distance 6, and if we delete a coordinate, we get a binary non-linear\\ncode of length 10 and minimum distance 5.\\nFor a detailed description of the connection between non-linear codes and design\\ntheory as well as the application of design theory in the area of (majority-logic)\\ndecoding, the reader is referred, e.g., to [13, 71, 72].\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n13\\n\\nUsing highly transitive permutation groups, a further construction of designs\\nfrom codes can be described (see, e.g., [31]).\\nTheorem 1.4. Let C be a code which admits an automorphism group acting\\nt-homogeneously (in particular, t-transitively) on the set of coordinates. Then the\\nsupports of the codewords of any non-zero weight form a t-design.\\nExample 1.11. The\\x01 r-th order Reed-Muller (RM) code RM(r, m) of length 2m is a\\nPr\\nm−r\\nbinary [2m , i=0 m\\n] code with its codewords the value-vectors of all Boolean\\ni ,2\\nfunctions in m variables of degree at most r. These codes were first considered by\\nD. E. Muller [73] and I. S. Reed [74] in 1954. The dual of the Reed-Muller code\\nRM(r, m) is RM(m − r − 1, m). Clearly, the extended binary [8, 4, 4] Hamming code\\nin Example 1.9 is RM(1, 3).\\nAlternatively, a codeword in RM(r, m) can be viewed as the sum of characteristic\\nfunctions of subspaces of dimension at least m−r of the affine space AG(m, 2). Thus,\\nthe full automorphism group of RM(r, m) contains the 3-transitive group AGL(m, 2)\\nof all affine transformations, and hence the codewords of any fixed non-zero weight\\nyield a 3-design.\\n1.3.3. Perfect codes and designs\\nThe interplay between coding theory and combinatorial designs is most evidently\\nseen in the relationship between perfect codes and t-designs.\\nTheorem 1.5. (Assmus and Mattson [75]). A linear e-error-correcting code of\\nlength n over Fq is perfect if and only if the supports of the codewords of minimum\\nweight d = 2e + 1 form an (e + 1)-(n, d, (q − 1)e ) design.\\nThe question\\n“Does every Steiner triple system on n points extend to a Steiner quadruple\\nsystem on n + 1 points?”\\n\\nwhich goes also back to Jakob Steiner [40], is still unresolved in general. However,\\nin terms of binary e-error-correcting codes, there is a positive answer.\\nTheorem 1.6. (Assmus and Mattson [75]). Let C be a (not necessarily linear)\\nbinary e-error correcting code of length n, which contains the zero vector. Then C\\nis perfect if and only if the supports of the codewords of minimum weight d = 2e + 1\\nform a Steiner (e + 1)-(n, d, 1) design. Moreover, the supports of the minimum\\ncodewords in the extended code C form a Steiner (e + 2)-(n + 1, d + 1, 1) design.\\nWe have seen in Example 1.8 and Example 1.9 that the supports of the seven\\ncodewords of weight 3 in the binary [7, 4, 3] Hamming code form a ST S(7), while\\nthe supports of the fourteen codewords of weight 4 in the extended [8, 4, 4] Hamming\\n\\n\\x0c14\\n\\nMichael Huber\\n\\ncode yield a SQS(8). In view of the above theorems, we get more generally:\\nExample 1.12. Let n := (q m − 1)/(q − 1). We consider a (m × n)-matrix H over\\nFq such that no two columns of H are linearly dependent. Then H clearly is a\\nparity check matrix of an [n, n − m, 3] code, which is the Hamming code over Fq .\\nThe number of its codewords is q n−m , and for any codeword x, we have S1 (x) =\\n1 + n(q − 1) = q m . Hence, by the Sphere Packing Bound (Theorem 1.1), this code is\\nperfect, and the supports of codewords of minimum weight 3 form a 2-(n, 3, q − 1)\\ndesign. Furthermore, in a binary [2m − 1, 2m − 1 − m, 3] Hamming code the supports\\nof codewords of weight 3 form a ST S(2m − 1), and the supports of the codewords\\nof weight 4 in the extended code yield a SQS(2m).\\nNote. The Hamming codes were developed by R. W. Hamming [2] in the mid\\n1940’s, who was employed at Bell Laboratories, and addressed a need for error\\ncorrection in his work on the primitive computers of the time. We remark that\\nthe extended binary [2m , 2m − m − 1, 4] Hamming code is the Reed-Muller code\\nRM(m − 2, m).\\nExample 1.13. The binary Golay code is a [23, 12, 7] code, while the ternary Golay\\ncode is a [11, 6, 5] code. For both codes, the parameters imply equality in the\\nSphere Packing Bound, and hence these codes are perfect. We will discuss later\\nvarious constructions of these some of the most famous codes (see Example 1.14 and\\nConstruction 1.12). By the above theorems, the supports of codewords of minimum\\nweight 7 in the binary [23, 12, 7] Golay code form a Steiner 4-(23, 7, 1) design, and\\nthe supports of the codewords of weight 8 in the extended binary [24, 12, 8] Golay\\ncode give a Steiner 5-(24, 8, 1) design. The supports of codewords of minimum\\nweight 5 in the ternary [11, 6, 5] Golay code yield a 3-(11, 5, 4) design. It can be\\nshown (e.g., via Theorem 1.4) that this is indeed a Steiner 4-(11, 5, 1) design. We\\nwill see in Example 1.15 that the supports of the codewords of weight 6 in the\\nextended ternary [12, 6, 6] Golay code give a Steiner 5-(12, 6, 1) design; thus the\\nabove results are not best possible.\\nNote. The Golay codes were discovered by M. J. E. Golay [76] in 1949 in the\\nprocess of extending Hamming’s construction. They have numerous practical realworld applications, e.g., the use of the extended binary Golay code in the Voyager\\nspacecraft program during the early 1980’s or in contemporary standard Automatic\\nLink Establishment (ALE) in High Frequency (HF) data communication for Forward Error Correction (FEC).\\nRemark 1.4. It is easily seen from their construction that the Hamming codes are\\nunique (up to equivalence). It was shown by V. Pless [77] that this is also true for the\\nGolay codes. Moreover, the binary and ternary Golay codes are the only non-trivial\\nperfect e-error-correcting codes with e > 1 over any field Fq . Using integral roots of\\nthe Lloyd polynomial, this remarkable fact was proven by A. Tietäväinen [78] and\\n\\n\\x0c15\\n\\nCoding theory and algebraic combinatorics\\n\\nJ. H. van Lint [79], and independently by V. A. Zinov’ev and V. K. Leont’ev [80].\\nM. R. Best [81] and Y. Hong [82] extended this result to arbitrary alphabets for\\ne > 2. For a thorough account of perfect codes, we refer to [83] and [84, Chap. 11].\\nAs trivial perfect codes can only form trivial designs, we have (up to equivalence)\\na complete list of non-trivial linear perfect codes with their associated designs:\\nCode\\nHamming code\\nbinary Golay code\\nternary Golay code\\n\\nCode parameters\\nm\\n−1 q m −1\\n[ qq−1\\n, q−1\\n\\n[23, 12, 7]\\n[11, 6, 5]\\n\\nDesign parameters\\nm\\n\\n− m, 3]\\n\\nq any prime power\\nq=2\\nq=3\\n\\n−1\\n2-( qq−1\\n, 3, q − 1)\\n4-(23, 7, 1)\\n4-(11, 5, 1)\\n\\nThere are various constructions of non-linear single-error-correcting perfect\\ncodes. For more details, see, e.g., [9, 13, 71, 72, 85] and references therein. However,\\na classification of these codes seems out of reach at present, although some progress\\nhas been made recently, see, for instance [86–88].\\nRemark 1.5. The long-standing question whether every Steiner triple system of\\norder 2m − 1 occurs in a perfect code has been answered recently in the negative.\\nRelying on the classification [89] of the Steiner quadruple systems of order 16, it was\\nshown in [90] that the unique anti-Pasch Steiner triple system of order 15 provides\\na counterexample.\\nRemark 1.6. Due to the close relationship between perfect codes and some of\\nthe most interesting designs, several natural extensions of perfect codes have been\\nexamined in this respect: Nearly perfect codes [91], and the more general class\\nof uniformly packed codes [92, 93], were studied extensively and eventually lead\\nto t-designs. H. C. A. van Tilborg [94] showed that e-error correcting uniformly\\npacked codes do not exist for e > 3, and classified those for e ≤ 3. For more\\ndetails, see [4, 10, 13, 94]. The concept of diameter perfect codes [95, 96] is related\\nparticularly to Steiner designs. For further generalizations of perfect codes, see\\ne.g., [84, Chap. 11] and [13, Chap. 6].\\n1.3.4. The Assmus-Mattson Theorem and analogues\\nWe consider in this subsection one of the most fundamental results in the interplay\\nof coding theory and design theory. We start by introducing two important classes\\nof codes.\\nLet q be an odd prime power. We define a function χ (the so-called Legendresymbol ) on Fq by\\n\\uf8f1\\n\\uf8f2 0, if x = 0\\nχ(x) := 1, if x is a non-zero square\\n\\uf8f3\\n−1, otherwise.\\n\\n\\x0c16\\n\\nMichael Huber\\n\\nWe note that χ is a character on the multiplicative group of Fq . Using the elements\\nof Fq as row and column labels ai and aj (0 ≤ i, j < q), respectively, a matrix\\nQ = (qij ) of order q can be defined by\\nqij := χ(aj − ai ).\\n\\n(1.1)\\n\\nIf q is a prime, then Q is a circulant matrix. We call a matrix\\n\\uf8f6\\n\\uf8eb\\n0\\n1 ··· 1\\n\\uf8f7\\n\\uf8ec χ(−1)\\n\\uf8f7\\n\\uf8ec\\nCq+1 := \\uf8ec .\\n\\uf8f7\\n.\\n\\uf8ed .\\nQ \\uf8f8\\nχ(−1)\\nof order q + 1 a Paley matrix . These matrices were constructed by R. A. Paley in\\n1933 and are a specific type of conference matrices, which have their origin in the\\napplication to conference telephone circuits.\\nConstruction 1.7. Let n be an odd prime, and q be a quadratic residue (mod n),\\ni.e. q (n−1)/2 ≡ 1 (mod n). The quadratic residue code (or QR code) of length n\\n√\\nover Fq is a [n, (n + 1)/2] code with minimum weight d ≥ n (so-called Square\\nRoot Bound ). It can be generated by the (0, 1)-circulant matrix of order n with top\\nrow the incidence vector of the non-zero quadratic residues (mod n). These codes\\nare a special class of cyclic codes and were first constructed by A. M. Gleason in\\n1964. For n ≡ 3 (mod 4), the extended quadratic residue code is self-dual. We note\\nfor the important binary case that q is a quadratic residue (mod n) if and only if\\nn ≡ ±1 (mod 8).\\nNote. By a theorem of A. M. Gleason and E. Prange, the full automorphism group\\nof an extended quadratic residue code of length n contains the group P SL(2, n) of\\nall linear fractional transformations whose determinant is a non-zero square.\\nExample 1.14. The binary [7, 4, 3] Hamming code is a quadratic residue code of\\nlength 7 over F2 . The binary [23, 12, 7] Golay code is a quadratic residue code of\\nlength 23 over F2 , while the ternary [11, 6, 5] Golay code is a quadratic residue code\\nof length 11 over F3 .\\nConstruction 1.8. For q ≡ −1 (mod 6) a prime power, the Pless symmetry code\\nSym2(q+1) of dimension q + 1 is a ternary [2(q + 1), q + 1] code with generator matrix\\nT\\n= −Iq+1\\nG2(q+1) := (Iq+1 , Cq+1 ), where Cq+1 is a Paley matrix. Since Cq+1 Cq+1\\n(over F3 ) for q ≡ −1 (mod 3), the code Sym2(q+1) is self-dual. This infinite family\\nof cyclic codes were introduced by V. Pless [97, 98] in 1972. We note that the first\\nsymmetry code S12 is equivalent to the extended [12, 6, 6] Golay code.\\nThe celebrated Assmus-Mattson Theorem gives a sufficient condition for the\\ncodewords of constant weight in a linear code to form a t-design.\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n17\\n\\nTheorem 1.9. (Assmus and Mattson [99]). Let C be an [n, k, d] code over Fq and\\n+q−2\\n<\\nC ⊥ be the [n, n−k, e] dual code. Let n0 be the largest integer such that n0 − n0q−1\\n⊥\\nd, and define m0 similarly for the dual code C , whereas if q = 2, we assume that\\nn0 = m0 = n. For some integer t with 0 < t < d, let us suppose that there are\\nat most d − t non-zero weights w in C ⊥ with w ≤ n − t. Then, for any weight\\nv with d ≤ v ≤ n0 , the supports of codewords of weight v in C form a t-design.\\nFurthermore, for any weight w with e ≤ w ≤ min{n − t, m0 }, the support of the\\ncodewords w in C ⊥ also form a t-design.\\nThe proof of the theorem involves a clever use of the MacWilliams relations\\n(Theorem 1.2). Along with these, Lemma 1.1 and the immediate observation that\\ncodewords of weight less than n0 with the same support must be scalar multiples of\\neach other, form the basis of the proof (for a detailed proof, see, e.g., [4, Chap. 14]).\\nRemark 1.7. Until this result by E. F. Assmus, Jr. and H. F. Mattson, Jr. in\\n1969, only very few 5-designs were known: the Mathieu-Witt designs 5-(12, 6, 1)\\nand 5-(24, 8, 1), the 5-(24, 8, 48) design formed by the codewords of weight 12 (the\\ndodecads) in the extended binary Golay code, as well as 5-(12, 6, 2) and 5-(24, 8, 2)\\ndesigns which had been found without using coding theory. However, by using the\\nAssmus-Mattson Theorem, it was possible to find a number of new 5-designs. In\\nparticular, the theorem is most useful when the dual code has relatively few nonzero weights. Nevertheless, it has not been possible to detect t-designs for t > 5 by\\nthe Assmus-Mattson Theorem.\\nWe illustrate in the following examples some applications of the theorem.\\nExample 1.15. The extended binary [24, 12, 8] Golay code is self-dual (cf. Construction 1.7) and has codewords of weight 0, 8, 12, 16, and 24 in view of Theorem 1.2. For t = 5, we obtain the Steiner 5-(24, 8, 1) design as in Example 1.13. In\\nthe self-dual extended ternary [12, 6, 6] Golay code all codewords are divisible by\\n3, and hence for t = 5, the supports of the codewords of weight 6 form a Steiner\\n5-(12, 6, 1) design.\\nExample 1.16. The extended quadratic residue code of length 48 over F2 is selfdual with minimum distance 12. By Theorem 1.2, it has codewords of weight\\n0, 12, 16, 20, 24, 28, 32, 36, and 48. For t = 5, each of the values v = 12, 16, 20, or 24\\nyields a different 5-design and its complementary design.\\nExample 1.17. The Pless symmetry code Sym36 of dimension 18 is self-dual\\n(cf. Construction 1.8) with minimum distance 12. The supports of codewords of\\nweight 12, 15, 18, and 21 yield 5-designs together with their complementary designs.\\nRemark 1.8. We give an overview of the state of knowledge concerning codes over\\nFq with their associated 5-designs (cf. also the tables in [13, Chap. 16], [65, 71, 72]).\\nIn fact, these codes are all self-dual. Trivial designs as well as complementary\\ndesigns are omitted.\\n\\n\\x0c18\\n\\nMichael Huber\\n\\nCode\\n\\nCode parameters\\n\\nDesign parameters\\n\\nRef.\\n\\nExtended cyclic code\\n\\n[18, 9, 8]\\n\\nq=4\\n\\n[100]\\n\\nExtended binary Golay code\\n\\n[24, 12, 8]\\n\\nq=2\\n\\nExtended ternary Golay code\\nLifted Golay code over Z4\\n\\n[12, 6, 6]\\n[24, 12]\\n\\nq=3\\nZ4\\n\\nExtended quadric residue codes\\n\\n[24, 12, 9]\\n\\nq=3\\n\\n[30, 15, 12]\\n\\nq=4\\n\\n[48, 24, 12]\\n\\nq=2\\n\\n[48, 24, 15]\\n\\nq=3\\n\\n[60, 30, 18]\\n\\nq=3\\n\\n[24, 12, 9]\\n\\nq=3\\n\\n[36, 18, 12]\\n\\nq=3\\n\\n[48, 24, 15]\\n\\nq=3\\n\\n[60, 30, 18]\\n\\nq=3\\n\\n5-(18, 8, 6)\\n5-(18, 10, 180)\\n5-(24, 8, 1)\\n5-(24, 12, 48)\\n5-(12, 6, 1)\\n5-(24, 10, 36)\\n5-(24, 11, 336)\\n5-(24, 12, 1584)\\n5-(24, 12, 1632)\\n5-(24, 9, 6)\\n5-(24, 12, 576)\\n5-(24, 15, 8580)\\n5-(30, 12, 220)\\n5-(30, 14, 5390)\\n5-(30, 16, 123000)\\n5-(48, 12, 8)\\n5-(48, 16, 1365)\\n5-(48, 20, 36176)\\n5-(48, 24, 190680)\\n5-(48, 12, 364)\\n5-(48, 18, 50456)\\n5-(48, 21, 2957388)\\n5-(48, 24, 71307600)\\n5-(48, 27, 749999640)\\n5-(60, 18, 3060)\\n5-(60, 21, 449820)\\n5-(60, 24, 34337160)\\n5-(60, 27, 1271766600)\\n5-(60, 30, 24140500956)\\n5-(60, 33, 239329029060)\\n5-(24, 9, 6)\\n5-(24, 12, 576)\\n5-(24, 15, 8580)\\n5-(36, 12, 45)\\n5-(36, 15, 5577)\\n5-(36, 18, 209685)\\n5-(36, 21, 2438973)\\n5-(48, 12, 364)\\n5-(48, 18, 50456)\\n5-(48, 21, 2957388)\\n5-(48, 24, 71307600)\\n5-(48, 27, 749999640)\\n5-(60, 18, 3060)\\n5-(60, 21, 449820)\\n5-(60, 24, 34337160)\\n5-(60, 27, 1271766600)\\n5-(60, 30, 24140500956)\\n5-(60, 33, 239329029060)\\n\\nPless symmetry codes\\n\\n[101]\\n\\n[102, 103]\\n[102]\\n[102]\\n[102]\\n[65, 99]\\n\\n[65, 99]\\n\\n[65, 99]\\n\\n[65, 99]\\n\\n[65, 99]\\n\\n[98]\\n\\n[98]\\n\\n[98]\\n\\n[97, 98]\\n\\nNote. The lifted Golay code over Z4 is defined in [104] as the extended Hensel\\nlifted quadric residue code of length 24. The supports of the codewords of Hamming\\nweight 10 in the lifted Golay code and certain extremal double circulant Type II\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n19\\n\\ncodes of length 24 yield (non-isomorphic) 5-(24, 10, 36) designs. We further note\\nthat the quadratic residue codes and the Pless symmetry codes listed in the table\\nwith the same parameters are not equivalent as shown in [98] by inspecting specific\\nelements of the automorphism group P SL(2, q).\\nRemark 1.9. The concept of the weight enumerator can be generalized to nonlinear codes (so-called distance enumerator ), which leads to an analog of the\\nMacWilliams relations as well as to similar results to the Assmus-Mattson Theorem for non-linear codes (see [17, 18, 105] and Subsection 1.3.8). The question\\nwhether there is an analogous result to the Assmus-Mattson theorem for codes over\\nZ4 proposed in [102] was answered in the affirmative in [106]. Further generalizations of the Assmus-Mattson Theorem are known, see in particular [107–113].\\n1.3.5. Codes and finite geometries\\nLet A be an incidence matrix of a projective plane P G(2, n) of order n. When we\\n2\\nconsider the subspace C of F2n +n+1 spanned by the rows of A, we obtain for odd n\\nonly the [n2 + n + 1, n2 + n, 2] code consisting of all codewords of even weight. The\\ncase for even n is more interesting, in particular if n ≡ 2 (mod 4).\\nTheorem 1.10. For n ≡ 2 (mod 4), the rows of an incidence matrix of a projective\\nplane P G(2, n) of order n generate a binary code C of dimension (n2 + n + 2)/2,\\nand the extended code C is self-dual.\\nIn a projective plane P G(2, n) of even order n, there exist sets of n + 2 points,\\nno three of which are collinear, and which are called hyperovals (sometimes just\\novals, cf. [46]). This gives furthermore\\nTheorem 1.11. The code C has minimum weight n+1. Moreover, the codewords of\\nminimum weight correspond to the lines and those of weight n + 2 to the hyperovals\\nof P G(2, n).\\nRemark 1.10. The above two theorems arose in the context of the examination of\\nthe existence of a projective plane of order 10 (cf. Remark 1.1; for detailed proofs\\nsee, e.g., [4, Chapt. 13]). Assuming the existence of such a plane, the obtained\\nproperties of the corresponding code lead to very extensive computer searches. For\\nexample, in an early crucial step, it was shown [114] that this code could not have\\ncodewords of weight 15. On the various attempts to attack the problem and the final\\nverification of the non-existence, we refer to [44, 115, 116] as well as [22, Chap. 17]\\nand [35, Chap. 12].\\nNote. We note that at present the Fano plane is the only known projective plane\\nwith order n ≡ 2 (mod 4).\\n\\n\\x0c20\\n\\nMichael Huber\\n\\nFor further accounts on codes and finite geometries, the reader is referred, e.g.,\\nto [66, Chap. 5 and 6] and [3, 4, 67, 116–119], as well as [120] from a more grouptheoretical perspective and [121] with an emphasis on quadratic forms over F2 .\\n1.3.6. Golay codes, Mathieu-Witt designs, and Mathieu groups\\nWe highlight some of the remarkable and natural interrelations between the Golay\\ncodes, the Mathieu-Witt designs, and the Mathieu groups.\\nThere are various different constructions for the Golay codes besides the description as quadratic residue codes in Example 1.14. We briefly illustrate some\\nexemplary constructions. For further details and more constructions, we refer\\nto [122], [13, Chap. 20], [4, Chap. 11], and [123, Chap. 11].\\nConstruction 1.12.\\n• Starting with the zero vector in F24\\n2 , a linear code of length 24 can be obtained\\nby successively taking the lexicographically least binary codeword which has\\nnot been used and which has distance at least 8 to any predecessor. At the end\\nof this process, we have 4096 codewords which form the extended binary Golay\\ncode. This construction is due to J. H. Conway and N. J. A. Sloane [124].\\n• Let A be an incidence matrix of the (unique) 2-(11, 6, 3) design. Then G :=\\n(I12 , P ) with\\n\\uf8eb\\n\\uf8f6\\n0 1 ··· 1\\n\\uf8ec1\\n\\uf8f7\\n\\uf8ec\\n\\uf8f7\\nP := \\uf8ec .\\n\\uf8f7\\n\\uf8ed ..\\nA \\uf8f8\\n1\\n\\nis a (12 × 24)-matrix in which each row (except the top row) has eight 1’s, and\\ngenerates the extended binary Golay code.\\n• Let N be an (12 × 12)-adjacency matrix of the graph formed by the vertices\\nand edges of the regular icosahedron. Then G := (I12 , J12 − N ) is a generator\\nmatrix for the extended binary Golay code.\\n• We recall that F4 = {0, 1, ω, ω 2} is the field of four elements with ω 2 = ω + 1.\\nThe hexacode is the [6, 3, 4] code over F4 generated by the matrix G := (I3 , P )\\nwith\\n\\uf8eb\\n\\uf8f6\\n1 ω2 ω\\nP := \\uf8ed 1 ω ω 2 \\uf8f8 .\\n1 1 1\\nThe extended binary Golay code can be defined by identifying each codeword\\nwith a binary (4 × 6)-matrix M (with rows m0 , m1 , m2 , m3 ), satisfying\\n(i) each column of M has the same parity as the first row m0 ,\\n(ii) the sum m1 + ωm2 + ω 2 m3 lies in the hexacode.\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n21\\n\\nThis description is essentially equivalent to the computational tool MOG (Miracle Octad Generator) of R. T. Curtis [125]. The construction via the hexacode\\nis by Conway, see, e.g., [123, Chap. 11].\\n• Let Q be the circulant matrix of order 5 defined by Eq. (1.1). Then G := (I6 , P ),\\nwhere P is the matrix Q bordered on top with a row of 1’s, is a generator matrix\\nof the ternary Golay code.\\nRemark 1.11. Referring to Example 1.7, we note that the automorphism groups\\nof the Golay codes are isomorphic to the particular Mathieu groups, as was first\\npointed out in [101, 126]. Moreover, the Golay codes are related in a particularly\\ndeep and interesting way to a larger family of sporadic finite simple groups (cf.,\\ne.g., [55]).\\nRemark 1.12. We have seen in Example 1.13 that the supports of the codewords\\nof weight 8 in the extended binary [24, 12, 8] Golay code form a Steiner 5-(24, 8, 1)\\ndesign. The uniqueness of the large Mathieu-Witt design (up to isomorphism) can\\nbe established easily via coding theory (cf. Example 1.7). The main part is to show\\nthat any binary code of 4096 codewords, including the zero vector, of length 24 and\\nminimum distance 8, is linear and can be determined uniquely (up to equivalence).\\nFor further details, in particular for a uniqueness proof of the small Mathieu-Witt\\ndesigns, see, e.g., [70, 122] and [4, Chap. 11].\\n1.3.7. Golay codes, Leech lattice, kissing numbers, and sphere packings\\nSphere packings closely connect mathematics and information theory via the sampling theorem as observed by C. E. Shannon [1] in his classical article of 1948.\\nRephrased in a more geometric language, this can be expressed as follows:\\n“Nearly equal signals are represented by neighboring points, so to keep the signals\\ndistinct, Shannon represents them by n-dimensional ‘billiard balls’, and is therefore led to ask: what is the best way to pack ‘billiard balls’ in n dimensions?” [127]\\n\\nOne of the most remarkable lattices, the Leech lattice in R24 , plays a crucial role\\nin classical sphere packings. We recall that a lattice in Rn is a discrete subgroup\\nof Rn of rank n. The extended binary Golay code led to the discovery by John\\nLeech [128] of the 24-dimensional Euclidean lattice named after him. There are\\nvarious constructions besides the usual ones from the binary and ternary Golay\\ncodes in the meantime, see, e.g., [129], [123, Chap. 24]. We outline some of the\\nfundamental connections between sphere packings and the Leech lattice.\\nThe Kissing Number Problem deals with the maximal number τn of equal size\\nnon-overlapping spheres in the n-dimensional Euclidean space Rn that can touch\\na given sphere of the same size. Only a few of these numbers are actually known.\\nFor dimensions n = 1, 2, 3, the classical solutions are: τ1 = 2, τ2 = 6, τ3 = 12.\\nThe number τ3 was the subject of a famous controversy between Isaac Newton and\\n\\n\\x0c22\\n\\nMichael Huber\\n\\nDavid Gregory in 1694, and was finally verified only in 1953 by K. Schütte and\\nB. L. van der Waerden [130]. Using an approach initiated by P. Delsarte [18, 131]\\nin the early 1970’s which gives linear programming upper bounds for binary errorcorrecting codes and for spherical codes [132] (cf. Subsection 1.3.8), A. M. Odlyzko\\nand N. J. A. Sloane [133], and independently V. I. Levenshtein [134], proved that\\nτ8 = 240 and τ24 = 196560. These exact solutions are the number of non-zero\\nvectors of minimal length in the root lattice E8 and in the Leech lattice, respectively.\\nBy extending and improving Delsarte’s method, O. R. Musin [135] verified in 2003\\nthat τ4 = 24, which is the number of non-zero vectors of minimal length in the root\\nlattice D4 .\\nThe Sphere Packing Problem asks for the maximal density of a packing of equal\\nsize non-overlapping spheres in the n-dimensional Euclidean space Rn . A sphere\\npacking is called a lattice packing if the centers of the spheres form a lattice in Rn .\\nThe Leech lattice is the unique densest lattice packing (up to scaling and isometries)\\nin R24 , as was shown by H. Cohn and A. Kumar [136, 137] recently in 2004, again\\nby a modification of Delsarte’s method. Moreover, they showed that the density of\\nany sphere packing in R24 cannot exceed the one given by the Leech lattice by a\\nfactor of more than 1 + 1.65 · 10−30 (via a computer calculation). The proof is based\\non the work [138] by Cohn and N. D. Elkies in 2003 in which linear programming\\nbounds for the Sphere Packing Problem are introduced and new upper bounds on\\nthe density of sphere packings in Rn with dimension n ≤ 36 are proven.\\nFor further details on the Kissing Number Problem and the Sphere Packing\\nProblem, see [123, Chap. 1], [127, 139], [14], as well as the survey articles [140–142].\\nFor an on-line database on lattices, see [143].\\n1.3.8. Codes and association schemes\\nAny finite nonempty subset of the unit sphere S n−1 in the n-dimensional Euclidean\\nspace Rn is called a spherical code. These codes have many practical applications,\\ne.g., in the design of signals for data transmission and storage. As a special class of\\nspherical codes, spherical designs were introduced by P. Delsarte, J.-M. Goethals\\nand J. Seidel [132] in 1977 as analogs on S n−1 of the classical combinatorial designs.\\nFor example, in S 2 the tetrahedron is a spherical 2-design; the octahedron and the\\ncube are spherical 3-designs, and the icosahedron and the dodecahedron are spherical 5-designs. In order to obtain the linear programming upper bound mentioned\\nin the previous subsection, Krawtchouk polynomials were involved in the case of\\nbinary error-correcting codes and Gegenbauer polynomials in the case of spherical\\ncodes.\\nHowever, Delsarte’s approach was indeed much more general and far-reaching.\\nHe developed for association schemes, which have their origin in the statistical\\ntheory of design of experiments, a theory to unify many of the objects we have\\nbeen addressing in this chapter. We give a formal definition of association schemes\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n23\\n\\nin the sense of Delsarte [18] as well as introduce the Hamming and the Johnson\\nschemes as important examples of the two fundamental classes of P -polynomial and\\nQ-polynomial association schemes.\\nDefinition 1.3. A d-class association scheme is a finite point set X together with\\nd + 1 relations Ri on X, satisfying\\n(i) {R0 , R1 , . . . , Rd } is a partition of X × X,\\n(ii) R0 = {(x, x) | x ∈ X},\\n(iii) for each i with 0 ≤ i ≤ d, there exists a j with 0 ≤ j ≤ d such that (x, y) ∈ Ri\\nimplies (y, x) ∈ Rj ,\\n(iv) for any (x, y) ∈ Rk , the number pkij of points z ∈ X with (x, z) ∈ Ri and\\n(z, y) ∈ Rj depends only on i, j and k,\\n(v) pkij = pkji for all i, j and k.\\nThe numbers pkij are called the intersection numbers of the association scheme. Two\\npoints x, y ∈ X are called i-th associates if {x, y} ∈ Ri .\\nExample 1.18. The Hamming scheme H(n, q) has as point set X the set Fn of all\\nn-tuples from a q-symbol alphabet; two n-tuples are i-th associates if their Hamming\\ndistance is i. The Johnson scheme J(v, k), with k ≤ 12 v, has as point set X the\\nset of all k-element subsets of a set of size v; two k-element subset S1 , S2 are i-th\\nassociates if |S1 ∩ S2 | = k − i.\\nDelsarte introduced the Hamming and Johnson schemes as settings for the classical concept of error-correcting codes and combinatorial t-designs, respectively. In\\nthis manner, certain results become formally dual, like the Sphere Packing Bound\\n(Theorem 1.1) and Fisher’s Inequality (Theorem 1.3).\\nFor a more extended treatment of association schemes, the reader is referred,\\ne.g., to [144–148], [4, Chap. 17], [13, Chap. 21], and in particular to [149, 150] with an\\nemphasis on the close connection between coding theory and associations schemes.\\nFor a survey on spherical designs, see [21, Chap. VI.54].\\n1.4. Directions for further research\\nWe present in this section a collection of significant open problems and challenges\\nconcerning future research.\\nProblem 1.1. (cf. [40]). Does every Steiner triple system on n points extend to a\\nSteiner quadruple system on n + 1 points?\\nProblem 1.2. Does there exist any non-trivial Steiner 6-design?\\nProblem 1.3. (cf. [13, p. 180]). Find all non-linear single-error-correcting perfect\\ncodes over Fq .\\n\\n\\x0c24\\n\\nMichael Huber\\n\\nProblem 1.4. (cf. [6, p. 106]). Characterize codes where all codewords of the same\\nweight (or of minimum weight) form a non-trivial design.\\nProblem 1.5. (cf. [6, p. 116]). Find a proof of the non-existence of a projective\\nplane of order 10 without the help of a computer or with an easily reproducible\\ncomputer program.\\nProblem 1.6. Does there exist any finite projective plane of order 12, or of any\\nother order that is neither a prime power nor covered by the Bruck-Ryser Theorem\\n(cf. Remark 1.1)?\\nProblem 1.7. Does the root lattice D4 give the unique kissing number configuration in R4 ?\\nProblem 1.8. Solve the Kissing Number Problem in n dimensions for any n > 4\\napart from n = 8 and 24. For presently known lower and upper bounds, we refer\\nto [151] and [152], respectively. Also any improvements of these bounds would be\\ndesirable.\\nProblem 1.9. (cf. [138, Conj. 8.1]). Verify the conjecture that the Leech lattice is\\nthe unique densest sphere packing in R24 .\\n1.5. Conclusions\\nOver the last sixty years a substantial amount of research has been inspired by\\nthe various interactions of coding theory and algebraic combinatorics. The fruitful\\ninterplay often reveals the high degree of regularity of both the codes and the\\ncombinatorial structures. This has lead to a vivid area of research connecting closely\\nmathematics with information and coding theory. The emerging methods can be\\napplied sometimes surprisingly effectively, e.g., in view of the recent advances on\\nkissing numbers and sphere packings.\\nA further development of this beautiful interplay as well as its application to\\nconcrete problems would be desirable, certainly also in view of the various still open\\nand long-standing problems.\\n\\n1.6. Terminologies/Keywords\\nError-correcting codes, combinatorial designs, perfect codes and related concepts,\\nAssmus-Mattson Theorem and analogues, projective geometries, non-existence of\\na projective plane of order 10, Golay codes, Leech lattice, kissing numbers, sphere\\npackings, spherical codes, association schemes.\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n25\\n\\n1.7. Exercises\\n(1) Verify (numerically) that the Steiner quadruple system SQS(8) of order 8\\n(cf. Example 1.5) has 14 blocks, and that the Mathieu-Witt design 5-(24, 8, 1)\\n(cf. Example 1.7) has 759 blocks.\\n(2) What are the parameters of the 2-design consisting of the points and hyperplanes (i.e. the (d−2)-dimensional projective subspaces) of the projective space\\nP G(d − 1, q)?\\n\\n(3) Does there exist a self-dual [8, 4] code over the finite field F2 ?\\n\\n(4) Show that the ternary [11, 6, 5] Golay code has 132 codewords of weight 5.\\n(5) Compute the weight distribution of the binary [23, 12, 7] Golay code.\\n(6) Show that any binary code of 4096 codewords, including the zero vector, of\\nlength 24 and minimum distance 8 is linear.\\n(7) Give a proof for the Sphere Packing Bound (cf. Theorem 1.1).\\n(8) Give a proof for Fisher’s Inequality (cf. Theorem 1.3).\\n(9) Show that a binary code generated by the rows of an incidence matrix of any\\nprojective plane P G(2, n) of even order n has dimension at most (n2 + n + 2)/2\\n(cf. Theorem 1.10).\\n(10) (Todd’s Lemma). In the Mathieu-Witt design 5-(24, 8, 1), if B1 and B2 are\\nblocks (octads) meeting in four points, then B1 + B2 is also a block.\\nSolutions:\\nad (1): By Lemma 1.2 (b), we have to calculate b = 8·7·6\\n4·3·2 = 14 in the case of the\\n24·23·22·21·20\\n= 759 in the case of\\nSteiner quadruple system SQS(8), and b =\\n8·7·6·5·4\\nthe Mathieu-Witt design 5-(24, 8, 1).\\nad (2): Starting from Example 1.3, we obtain via counting arguments (or by\\nusing the transitivity properties of the general linear group) that the\\npoints and hyperplanes of the projective space P G(d − 1, q) form a\\nd\\n−1 qd−1 −1 qd−2 −1\\n, q−1 , q−1 ) design.\\n2-( qq−1\\nad (3): Yes, the extended binary [8, 4, 4] Hamming code is self-dual (cf. Example 1.9).\\nad (4): Since the ternary [11, 6, 5] Golay code is perfect (cf. Example 1.13), every word\\n11\\nof weight\\n\\x01 53\\x01 in F3 has distance 2 to a codeword of weight 5. Thus A5 =\\n11\\n3\\n2 · 3 / 2 = 132.\\nad (5): The binary [23, 12, 7] Golay code contains the zero vector and is perfect. This\\ndetermines the weight distribution as follows A0 = A23 = 1, A7 = A16 = 253,\\nA8 = A15 = 506, A11 = A12 = 1288.\\n\\nad (6): Let C denote a binary code of 4096 codewords, including the zero vector, of\\nlength 24 and minimum distance 8. Deleting any coordinate leads to a code\\n\\n\\x0c26\\n\\nMichael Huber\\n\\nwhich has the same weight distribution as the code given in Exercise (5). Hence,\\nthe code C only has codewords of weight 0, 8, 12, 16 and 24. This is still true\\nif the code C is translated by any codeword (i.e. C + x for any x ∈ C). Thus,\\nthe distances between pairs of codewords are also 0, 8, 12, 16 and 24. Therefore,\\nthe standard inner product hx, yi vanishes for any two codewords x, y ∈ C,\\nand hence C is self-orthogonal. For cardinality reasons, we conclude that C is\\nself-dual and hence in particular linear.\\n\\x01\\nPe\\nad (7): The sum i=0 ni (q − 1)i counts the number of words in a sphere of radius\\ne. As the spheres\\nof radius e about distinct codewords are disjoint, we obtain\\n\\x01\\nPe\\n|C| · i=0 ni (q − 1)i words. Clearly, this number cannot exceed the total\\nnumber q n of words, and the claim follows.\\nad (8): As a non-trivial t-design with t ≥ 2 is also a non-trivial 2-design by Lemma 1.1,\\nit is sufficient to prove the assertion for an arbitrary non-trivial 2-(v, k, λ) design\\nD. Let A be an incidence matrix of D as defined in Subsection 1.3.1. Clearly,\\nthe (i, k)-th entry\\n(AAt )ik =\\n\\nb\\nX\\nj=1\\n\\n(A)ij (At )jk =\\n\\nb\\nX\\n\\naij akj\\n\\nj=1\\n\\nof the (v × v)-matrix AAt is the total number of blocks containing both xi and\\nxk , and is thus equal to r if i = k, and to λ if i 6= k. Hence\\nAAt = (r − λ)I + λJ,\\n\\nwhere I denotes the (v × v)-unit matrix and J the (v × v)-matrix with all entries\\nequal to 1. Using elementary row and column operations, it follows easily that\\ndet(AAt ) = rk(r − λ)v−1 .\\n\\nThus AAt is non-singular (i.e. its determinant is non-zero) as r = λ would\\nimply v = k by Lemma 1.1, yielding that the design is trivial. Therefore, the\\nmatrix AAt has rank(A) = v. But, if b < v, then rank(A) ≤ b < v, and thus\\nrank(AAt ) < v, a contradiction. It follows that b ≥ v, proving the claim.\\n\\nad (9): Let C denote a binary code generated by the rows of an incidence matrix of\\nP G(2, n). By assumption n is even, and hence the extended code C must be\\nself-orthogonal. Therefore, the dimension of C is at most n2 + n + 2/2.\\n\\nad (10): For given blocks B1 = {01, 02, 03, 04, 05, 06, 07, 08} and B2 = {01, 02, 03, 04,\\n09, 10, 11, 12} in the Mathieu-Witt design 5-(24, 8, 1), let us assume that B1 +B2\\nis not a block. The block B3 which contains {05, 06, 07, 08, 09} must contain\\njust one more point of B2 , say B3 = {05, 06, 07, 08, 09, 10, 13, 14}. Similarly,\\nB4 = {05, 06, 07, 08, 11, 12, 15, 16} is the block containing {05, 06, 07, 08, 11}.\\nBut hence, it is impossible to find a block which contains {05, 06, 07, 09, 11}\\nand intersects with Bi , 1 ≤ i ≤ 4, in 0, 2 or 4 points. Therefore, we obtain\\na contradiction as there must be a block containing any five points by Definition 1.2.\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n27\\n\\nReferences\\n[1] C. E. Shannon, A mathematical theory of communication, Bell Syst. Tech. J. 27,\\n379–423 and 623–656, (1948).\\n[2] R. W. Hamming, Error detecting and error correcting codes, Bell Syst. Tech. J. 29,\\n147–160, (1950).\\n[3] E. R. Berlekamp, Algebraic Coding Theory. (McGraw-Hill, New York, 1968; Revised\\nedition: Aegean Park Press 1984).\\n[4] P. J. Cameron and J. H. van Lint, Designs, Graphs, Codes and their Links. (Cambridge Univ. Press, Cambridge, 1991).\\n[5] R. Hill, A First Course in Coding Theory. (Clarendon Press, Oxford, 1986).\\n[6] W. C. Huffman and V. Pless, Eds., Handbook of Coding Theory. vol. I and II, (NorthHolland, Amsterdam, New York, Oxford, 1998).\\n[7] W. C. Huffman and V. Pless, Fundamentals of Error-Correcting Codes. (Cambridge\\nUniv. Press, Cambridge, 2003).\\n[8] A. Betten, M. Braun, H. Fripertinger, A. Kerber, A. Kohnert, and A. Wassermann,\\nError-Correcting Linear Codes. (Springer, Berlin, Heidelberg, New York, 2006).\\n[9] J. H. van Lint, Codes, In eds. R. L. Graham, M. Grötschel, and L. Lovász, Handbook of Combinatorics, vol. I, pp. 773–807. North-Holland, Amsterdam, New York,\\nOxford, (1995).\\n[10] J. H. van Lint, Introduction to Coding Theory. (Springer, Berlin, Heidelberg, New\\nYork, 1999), 3rd edition.\\n[11] W. W. Peterson and E. J. Weldon, Jr., Error-Correcting Codes. (MIT Press, Cambridge, 1972), 2nd edition.\\n[12] R. Roth, Introduction to Coding Theory. (Cambridge Univ. Press, Cambridge, 2006).\\n[13] F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting Codes.\\n(North-Holland, Amsterdam, New York, Oxford, 1977; 12. impression 2006).\\n[14] T. M. Thompson, From Error-Correcting Codes through Sphere Packings to Simple\\nGroups. (Carus Math. Monograph 21, 1983).\\n[15] A. R. Calderbank, The art of signaling: fifty years of coding theory, IEEE Trans.\\nInform. Theory. 44, 2561–2595, (1998).\\n[16] F. J. MacWilliams, A theorem on the distribution of weights in a systematic code,\\nBell Syst. Tech. J. 42, 79–94, (1963).\\n[17] F. J. MacWilliams, N. J. A. Sloane, and J.-M. Goethals, The MacWilliams identities\\nfor non-linear codes, Bell System Tech. J. 51, 803–819, (1972).\\n[18] P. Delsarte, An algebraic approach to the association schemes of coding theory,\\nPhilips Res. Reports Suppl. 10, (1973).\\n[19] T. Beth, D. Jungnickel, and H. Lenz, Design Theory. vol. I and II, Encyclopedia of\\nMath. and Its Applications 69/78, (Cambridge Univ. Press, Cambridge, 1999).\\n[20] P. J. Cameron, Parallelisms of Complete Designs. (Cambridge Univ. Press, Cambridge, 1976).\\n[21] C. J. Colbourn and J. H. Dinitz, Eds., Handbook of Combinatorial Designs. (CRC\\nPress, Boca Raton, 2006), 2nd edition.\\n[22] M. Hall, Jr., Combinatorial Theory. (J. Wiley, New York, 1986), 2nd edition.\\n[23] D. R. Hughes and F. C. Piper, Design Theory. (Cambridge Univ. Press, Cambridge,\\n1985).\\n[24] D. R. Stinson, Combinatorial Designs: Constructions and Analysis. (Springer,\\nBerlin, Heidelberg, New York, 2004).\\n[25] P. Dembowski, Finite Geometries. (Springer, Berlin, Heidelberg, New York, 1968;\\nReprint 1997).\\n\\n\\x0c28\\n\\nMichael Huber\\n\\n[26] F. Buekenhout, Ed., Handbook of Incidence Geometry. (North-Holland, Amsterdam,\\nNew York, Oxford, 1995).\\n[27] P. J. Cameron, Permutation Groups. (Cambridge Univ. Press, Cambridge, 1999).\\n[28] R. D. Carmichael, Introduction to the Theory of Groups of Finite Order. (Ginn,\\nBoston, 1937; Reprint: Dover Publications, New York, 1956).\\n[29] J. D. Dixon and B. Mortimer, Permutation Groups. (Springer, Berlin, Heidelberg,\\nNew York, 1996).\\n[30] H. Wielandt, Finite Permutation Groups. (Academic Press, New York, 1964).\\n[31] V. D. Tonchev, Combinatorial Configurations: Designs, Codes, Graphs. (Longman,\\nHarlow, 1988).\\n[32] D. Pei, Authentication Codes and Combinatorial Designs. (CRC Press, Boca Raton,\\n2006).\\n[33] D. R. Stinson, Combinatorial designs and cryptography, In ed. K. Walker, Surveys\\nin Combinatorics, 1993, pp. 257–287. Cambridge Univ. Press, Cambridge, (1993).\\n[34] C. J. Colbourn, J. H. Dinitz, and D. R. Stinson, Applications of combinatorial\\ndesigns to communications, cryptography, and networking, In eds. J. D. Lamb and\\nD. A. Preece, Surveys in Combinatorics, 1999, pp. 37–100. Cambridge Univ. Press,\\nCambridge, (1999).\\n[35] P. Kaski and P. R. J. Östergård, Classification Algorithms for Codes and Designs.\\n(Springer, Berlin, Heidelberg, New York, 2006).\\n[36] P. J. Cameron, Combinatorics: Topics, Techniques, Algorithms. (Cambridge Univ.\\nPress, Cambridge, 1994; Reprint 1996).\\n[37] J. H. van Lint and R. M. Wilson, A Course in Combinatorics. (Cambridge Univ.\\nPress, Cambridge, 2001), 2nd edition.\\n[38] H. J. Ryser, Ed., Combinatorial Mathematics. (Math. Assoc. Amer., Buffalo, NY,\\n1963).\\n[39] R. L. Graham, M. Grötschel, and L. Lovász, Eds., Handbook of Combinatorics. vol.\\nI and II, (North-Holland, Amsterdam, New York, Oxford, 1995).\\n[40] J. Steiner, Combinatorische Aufgabe, J. Reine Angew. Math. 45, 181–182, (1853).\\n[41] R. J. Wilson, The early history of block designs, Rend. Sem. Mat. Messina Ser. II.\\n9, 267–276, (2003).\\n[42] Y. J. Ionin and M. S. Shrikhande, Combinatorics of Symmetric Designs. (Cambridge\\nUniv. Press, Cambridge, 2006).\\n[43] R. H. Bruck and H. J. Ryser, The non-existence of certain finite projective planes,\\nCanad. J. Math. 1, 88–93, (1949).\\n[44] C. W. H. Lam, L. Thiel, and S. Swiercz, The non-existence of finite projective planes\\nof order 10, Canad. J. Math. 41, 1117–1123, (1989).\\n[45] A. Beutelspacher, Projective Planes, In ed. F. Buekenhout, Handbook of Incidence\\nGeometry, pp. 101–136. North-Holland, Amsterdam, New York, Oxford, (1995).\\n[46] J. W. P. Hirschfeld, Projective Geometries over Finite Fields. (Clarendon Press,\\nOxford, 1998), 2nd edition.\\n[47] D. R. Hughes and F. C. Piper, Projective Planes. (Springer, Berlin, Heidelberg, New\\nYork, 1982), 2nd edition.\\n[48] H. Lüneburg, Translation Planes. (Springer, Berlin, Heidelberg, New York, 1980).\\n[49] G. Pickert, Projektive Ebenen. (Springer, Berlin, Heidelberg, New York, 1975), 2nd\\nedition.\\n[50] E. Mathieu, Mémoire sur l’étude des fonctions de plusieurs quantitiés, J. Math.\\nPures Appl. 6, 241–323, (1861).\\n[51] E. Mathieu, Sur la fonction cinq fois transitive de 24 quantités, J. Math. Pures Appl.\\n18, 25–46, (1873).\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n29\\n\\n[52] E. Witt, Die 5-fach transitiven Gruppen von Mathieu, Abh. Math. Sem. Univ. Hamburg. 12, 256–264, (1938).\\n[53] E. Witt, Über Steinersche Systeme, Abh. Math. Sem. Univ. Hamburg. 12, 265–275,\\n(1938).\\n[54] H. Lüneburg, Transitive Erweiterungen endlicher Permutationsgruppen. (Springer,\\nBerlin, Heidelberg, New York, 1969).\\n[55] M. Aschbacher, Sporadic Groups. (Cambridge Univ. Press, Cambridge, 1994).\\n[56] J. Tits, Sur les systèmes de Steiner associés aux trois “grands” groupes de Mathieu,\\nRendic. Math. 23, 166–184, (1964).\\n[57] H. Lüneburg, Fahnenhomogene Quadrupelsysteme, Math. Z. 89, 82–90, (1965).\\n[58] M. Huber, Classification of flag-transitive Steiner quadruple systems, J. Combin.\\nTheory, Series A. 94, 180–190, (2001).\\n[59] M. Huber, The classification of flag-transitive Steiner 3-designs, Adv. Geom. 5,\\n195–221, (2005).\\n[60] M. Huber, The classification of flag-transitive Steiner 4-designs, J. Algebr. Comb.\\n26, 183–207, (2007).\\n[61] M. Huber, A census of highly symmetric combinatorial designs, J. Algebr. Comb.\\n26, 453–476, (2007).\\n[62] M. Huber, Flag-transitive Steiner Designs. (Birkhäuser, Basel, Berlin, Boston, to\\nappear).\\n[63] L. Teirlinck, Non-trivial t-designs without repeated blocks exist for all t, Discrete\\nMath. 65, 301–311, (1987).\\n[64] R. A. Fisher, An examination of the different possible solutions of a problem in\\nincomplete blocks, Ann. Eugenics. 10, 52–75, (1940).\\n[65] E. F. Assmus, Jr. and H. F. Mattson, Jr., Coding and combinatorics, SIAM Rev.\\n16, 349–388, (1974).\\n[66] E. F. Assmus, Jr. and J. D. Key, Designs and their Codes. (Cambridge Univ. Press,\\nCambridge, 1993).\\n[67] E. F. Assmus, Jr. and J. D. Key, Designs and codes: an update, Des. Codes Cryptography. 9, 7–27, (1996).\\n[68] I. F. Blake, Codes and designs, Math. Mag. 52, 81–95, (1979).\\n[69] J. H. van Lint, Codes and designs, In ed. M. Aigner, Higher Combinatorics: Proc.\\nNATO Adv. Study Inst. (Berlin), pp. 241–256. Reidel, Dordrecht, Boston, (1977).\\n[70] J. H. van Lint, Codes and combinatorial designs, In eds. D. Jungnickel and S. A.\\nVanstone, Proc. Marshall Hall conference on coding theory, design theory, group\\ntheory (Burlington, VT), pp. 31–39. J. Wiley, New York, (1993).\\n[71] V. D. Tonchev, Codes and designs, In eds. W. C. Huffman and V. Pless, Handbook\\nof Coding Theory, vol. II, pp. 1229–1267. North-Holland, Amsterdam, New York,\\nOxford, (1998).\\n[72] V. D. Tonchev, Codes, In eds. C. J. Colbourn and J. H. Dinitz, Handbook of Combinatorial Designs, pp. 677–702. CRC Press, Boca Raton, 2nd edition, (2006).\\n[73] D. E. Muller, Application of boolean algebra to switching circuit design and to error\\ncorrection, IEEE Trans. Comp. 3, 6–12, (1954).\\n[74] I. S. Reed, A class of multiple-error-correcting codes and the decoding scheme, IEEE\\nTrans. Inform. Theory. 4, 38–49, (1954).\\n[75] E. F. Assmus, Jr. and H. F. Mattson, Jr., On tactical configurations and errorcorrecting codes, J. Combin. Theory, Series A. 2, 243–257, (1967).\\n[76] M. J. E. Golay, Notes on digital coding, Proc. IRE. 37, 657, (1949).\\n[77] V. Pless, On the uniqueness of the Golay codes, J. Combin. Theory, Series A. 5,\\n215–228, (1968).\\n\\n\\x0c30\\n\\nMichael Huber\\n\\n[78] A. Tietäväinen, On the nonexistence of perfect codes over finite fields, SIAM J.\\nAppl. Math. 24, 88–96, (1973).\\n[79] J. H. van Lint, Nonexistence theorems for perfect error correcting codes, In eds.\\nG. Birkhoff and M. Hall, Jr., Computers in Algebraic Number Theory, vol. IV, pp.\\n89–95. SIAM-AMS Proc., Providence, RI, (1971).\\n[80] V. A. Zinov’ev and V. K. Leont’ev, The nonexistence of perfect codes over Galois\\nfields, Probl. Control and Inform. Theory. 2, 123–132, (1973).\\n[81] M. R. Best. A contribution to the nonexistence of perfect codes. PhD thesis, University of Amsterdam, (1982).\\n[82] Y. Hong, On the nonexistence of unknown perfect 6- and 8-codes in Hamming\\nschemes H(n, q) with q arbitrary, Osaka J. Math. 21, 687–700, (1984).\\n[83] J. H. van Lint, A survey of perfect codes, Rocky Mountain J. Math. 5, 189–224,\\n(1975).\\n[84] G. Cohen, I. Honkala, S. Litsyn, and A. Lobstein, Covering Codes. (North-Holland,\\nAmsterdam, New York, Oxford, 1997).\\n[85] A. M. Romanov, A survey of methods for constructing nonlinear perfect binary\\ncodes, Diskretn. Anal. Issled. Oper. Ser. 1. 13, 60–88, (2006).\\n[86] S. V. Avgustinovich, O. Heden, and F. I. Solov’eva, The classification of some perfect\\ncodes, Des. Codes Cryptography. 31, 313–318, (2004).\\n[87] O. Heden and M. Hessler, On the classification of perfect codes: side class structures,\\nDes. Codes Cryptography. 40, 319–333, (2006).\\n[88] K. T. Phelps, J. Rifà, and M. Villanueva, Kernels and p-kernels of pr -ary 1-perfect\\ncodes, Des. Codes Cryptography. 37, 243–261, (2005).\\n[89] P. Kaski, P. R. J. Östergård, and O. Pottonen, The Steiner quadruple systems of\\norder 16, J. Combin. Theory, Series A. 113, 1764–1770, (2006).\\n[90] P. R. J. Östergård and O. Pottonen, There exist Steiner triple systems of order 15\\nthat do not occur in a perfect binary one-error-correcting code, J. Combin. Des. 15,\\n465–468, (2007).\\n[91] J.-M. Goethals and S. Snover, Nearly perfect binary codes, Discrete Math. 2, 65–88,\\n(1972).\\n[92] N. V. Semakov, V. A. Zinov’ev, and G. V. Zaitsev, Uniformly packed codes, Probl.\\nPeredaci Inform. 7, 38–50, (1971).\\n[93] J.-M. Goethals and H. C. A. van Tilborg, Uniformly packed codes, Philips Res.\\nReports. 30, 9–36, (1975).\\n[94] H. C. A. van Tilborg. Uniformly packed codes. PhD thesis, Technology University\\nEindhoven, (1976).\\n[95] R. Ahlswede, H. K. Aydinian, and L. H. Khachatrian, On perfect codes and related\\nconcepts, Des. Codes Cryptography. 22, 221–237, (2001).\\n[96] M. Schwartz and T. Etzion, Codes and anticodes in the Grassman graph, J. Combin.\\nTheory, Series A. 97, 27–42, (2002).\\n[97] V. Pless, The weight of the symmetry code for p = 29 and the 5-designs obtained\\ntherein, Ann. New York Acad. Sci. 175, 310–313, (1970).\\n[98] V. Pless, Symmetry codes over GF (3) and new five-designs, J. Combin. Theory,\\nSeries A. 12, 119–142, (1972).\\n[99] E. F. Assmus, Jr. and H. F. Mattson, Jr., New 5-designs, J. Combin. Theory, Series\\nA. 6, 122–151, (1969).\\n[100] F. J. MacWilliams, A. M. Odlyzko, N. J. A. Sloane, and H. N. Ward, Self-dual codes\\nover GF (4), J. Combin. Theory, Series A. 25, 288–318, (1978).\\n[101] L. J. Paige, A note on the Mathieu groups, Canad. J. Math. 9, 15–18, (1956).\\n[102] M. Harada, New 5-designs constructed from the lifted Golay code over Z4 , J. Com-\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n31\\n\\nbin. Des. 6, 225–229, (1998).\\n[103] T. A. Gulliver and M. Harada, Extremal double circulant type II codes over Z4 and\\nconstruction of 5-(24, 10, 36) designs, Discrete Math. 194, 129–137, (1999).\\n[104] A. Bonnecaze, P. Solé, and A. R. Calderbank, Quaternary quadratic residue codes\\nand unimodular lattices, IEEE Trans. Inform. Theory. 41, 366–377, (1995).\\n[105] P. Delsarte, Four fundamental parameters of a code and their combinatorial significance, Information and Control. 23, 407–438, (1973).\\n[106] K. Tanabe, A criterion for designs in Z4 -codes on the symmetrized weight enumerator, Des. Codes Cryptography. 30, 169–185, (2003).\\n[107] A. R. Calderbank, P. Delsarte, and N. J. A. Sloane, A strengthening of the AssmusMattson theorem, IEEE Trans. Inform. Theory. 37, 1261–1268, (1991).\\n[108] G. T. Kennedy and V. Pless, On designs and formally self-dual codes, Des. Codes\\nCryptography. 4, 43–55, (1994).\\n[109] J. Simonis, MacWilliams identities and coordinate partitions, Linear Algebra Appl.\\n216, 81–91, (1995).\\n[110] C. Bachoc, On harmonic weight enumerators of binary codes, Des. Codes Cryptography. 18, 11–28, (1999).\\n[111] J.-L. Kim and V. Pless, Designs in additive codes over GF (4), Des. Codes Cryptography. 30, 187–199, (2003).\\n[112] D.-J. Shin, P. V. Kumar, and T. Helleseth, An Assmus-Mattson-type approach for\\nidentifying 3-designs from linear codes over Z4 , Des. Codes Cryptography. 31, 75–92,\\n(2004).\\n[113] T. Britz and K. Shiromoto, Designs from subcode supports of linear codes, Des.\\nCodes Cryptography. 46, 175–189, (2008).\\n[114] F. J. MacWilliams, N. J. A. Sloane, and J. G. Thompson, On the existence of a\\nprojective plane of order 10, J. Combin. Theory, Series A. 14, 66–78, (1973).\\n[115] C. W. H. Lam, The search for a finite projective plane of order 10, Amer. Math.\\nMonthly. 98, 305–318, (1991).\\n[116] P. J. Cameron, Finite Geometries, In eds. R. L. Graham, M. Grötschel, and\\nL. Lovász, Handbook of Combinatorics, vol. I, pp. 647–692. North-Holland, Amsterdam, New York, Oxford, (1995).\\n[117] E. F. Assmus, Jr. and J. D. Key, Polynomial codes and finite geometries, In eds.\\nW. C. Huffman and V. Pless, Handbook of Coding Theory, vol. II, pp. 1269–1343.\\nNorth-Holland, Amsterdam, New York, Oxford, (1998).\\n[118] J. A. Thas, Finite geometries, varieties and codes, Doc. Math. J. DMV, Extra Vol.\\nICM III. pp. 397–408, (1998).\\n[119] L. Storme. Projective geometry and coding theory, COM2 MAC Lect. Note Series\\n9. Combin. and Comput. Math., Center Pohang Univ. of Science and Technology,\\n(2003).\\n[120] C. Hering. On codes and projective designs. Technical Report 344, Kyoto Univ.,\\nMath. Research Inst. Seminar Notes, (1979).\\n[121] P. J. Cameron. Finite geometry and coding theory, Socrates Course Notes, (1999).\\nPublished electronically at http://dwispc8.vub.ac.be/Potenza/lectnotes.html.\\n[122] T. Beth and D. Jungnickel, Mathieu groups, Witt designs, and Golay codes, In eds.\\nM. Aigner and D. Jungnickel, Geometries and Groups, pp. 157–179. Springer, Berlin,\\nHeidelberg, New York, (1981).\\n[123] J. H. Conway and N. J. A. Sloane, Sphere Packings, Lattices and Groups. (Springer,\\nBerlin, Heidelberg, New York, 1998), 3rd edition.\\n[124] J. H. Conway and N. J. A. Sloane, Lexicographic codes: error-correcting codes from\\ngame theory, IEEE Trans. Inform. Theory. 32, 337–348, (1986).\\n\\n\\x0c32\\n\\nMichael Huber\\n\\n[125] R. T. Curtis, A new combinatorial approach to M24 , Math. Proc. Cambridge Philos.\\nSoc. 79, 25–42, (1976).\\n[126] E. F. Assmus, Jr. and H. F. Mattson, Jr., Perfect codes and the Mathieu groups,\\nArch. Math. 17, 121–135, (1966).\\n[127] N. J. A. Sloane, The Sphere Packing Problem, Doc. Math. J. DMV, Extra Vol. ICM\\nIII. pp. 387–396, (1998).\\n[128] J. Leech, Some sphere packings in higher space, Canad. J. Math. 16, 657–682,\\n(1964).\\n[129] J. H. Conway and N. J. A. Sloane, Twenty-three constructions for the Leech lattice,\\nProc. Roy. Soc. Lond., Series A. 381, 275–283, (1982).\\n[130] K. Schütte and B. L. van der Waerden, Das Problem der dreizehn Kugeln, Math.\\nAnn. 125, 325–334, (1953).\\n[131] P. Delsarte, Bounds for unrestricted codes, by linear programming, Philips Res.\\nReports. 27, 272–289, (1972).\\n[132] P. Delsarte, J.-M. Goethals, and J. Seidel, Spherical codes and designs, Geom. Dedicata. 6, 363–388, (1977).\\n[133] A. M. Odlyzko and N. J. A. Sloane, New bounds on the number of unit spheres that\\ncan touch a unit sphere in n dimensions, J. Combin. Theory, Series A. 26, 210–214,\\n(1979).\\n[134] V. I. Levenshtein, On bounds for packings in n-dimensional Euclidean space, Sov.\\nMath. Doklady. 20, 417–421, (1979).\\n[135] O. R. Musin, The kissing number in four dimensions, Ann. Math. (to appear).\\nPreprint published electronically at http://arxiv.org/abs/math.MG/0309430.\\n[136] H. Cohn and A. Kumar, The densest lattice in twenty-four dimensions, Electron.\\nRes. Announc. Amer. Math. Soc. 10, 58–67, (2004).\\n[137] H. Cohn and A. Kumar, Optimality and uniqueness of the Leech lattice\\namong lattices, Ann. Math. (to appear). Preprint published electronically at\\nhttp://arxiv.org/abs/math.MG/0403263.\\n[138] H. Cohn and N. D. Elkies, New upper bounds on sphere packings, Ann. Math. 157,\\n689–714, (2003).\\n[139] O. R. Musin. An extension of Delsarte’s method. The kissing problem in three and\\nfour dimensions. In Proc. COE Workshop on Sphere Packings (Kyushu Univ. 2004),\\npp. 1–25, (2005).\\n[140] N. D. Elkies, Lattices, linear codes, and invariants, Notices Amer. Math. Soc. 47,\\n1238–1245 and 1382–1391, (2000).\\n[141] F. Pfender and G. M. Ziegler, Kissing numbers, sphere packings and some unexpected proofs, Notices Amer. Math. Soc. 51, 873–883, (2004).\\n[142] K. Bezdek, Sphere packings revisited, Europ. J. Comb. 27, 864–883, (2006).\\n[143] G. Nebe and N. J. A. Sloane. A catalogue of lattices. Published electronically at\\nhttp://www.research.att.com/~ njas/lattices/.\\n[144] R. C. Bose and T. Shimamoto, Classification and analysis of partially balanced\\nincomplete block designs with two associate classes, J. Amer. Statist. Assoc. 47,\\n151–184, (1952).\\n[145] R. C. Bose and D. M. Mesner, On linear associative algebras corresponding to association schemes of partially balanced designs, Ann. Math. Statist. 30, 21–38, (1959).\\n[146] E. Bannai and T. Ito, Algebraic Combinatorics I: Association Schemes. (Benjamin,\\nNew York, 1984).\\n[147] A. E. Brouwer, A. M. Cohen, and A. Neumaier, Distance-Regular Graphs. (Springer,\\nBerlin, Heidelberg, New York, 1989).\\n[148] A. E. Brouwer and W. H. Haemers, Association Schemes, In eds. R. L. Graham,\\n\\n\\x0cCoding theory and algebraic combinatorics\\n\\n[149]\\n\\n[150]\\n[151]\\n[152]\\n\\n33\\n\\nM. Grötschel, and L. Lovász, Handbook of Combinatorics, vol. I, pp. 747–771. NorthHolland, Amsterdam, New York, Oxford, (1995).\\nP. Camion, Codes and association schemes: Basic properties of association schemes\\nrelevant to coding, In eds. W. C. Huffman and V. Pless, Handbook of Coding Theory,\\nvol. II, pp. 1441–1567. North-Holland, Amsterdam, New York, Oxford, (1998).\\nP. Delsarte and V. Levensthein, Association schemes and coding theory, IEEE Trans.\\nInform. Theory. 44, 2477–2504, (1998).\\nG. Nebe and N. J. A. Sloane. Table of the highest kissing numbers presently known.\\nPublished electronically at http://www.research.att.com/~ njas/lattices/.\\nC.\\nBachoc\\nand\\nF.\\nVallentin,\\nNew upper bounds for kissing numbers from semidefinite programming. Preprint\\npublished electronically at http://arxiv.org/abs/math.MG/0608426.\\n\\n\\x0c34\\n\\nMichael Huber\\n\\n\\x0cIndex\\n\\n[n, k] code, 2\\nt-design, 5\\n\\ngenerator matrix, 3\\ngraph theory, 5\\ngroup theory, 5\\n\\naffine geometry, 8\\naffine plane, 8\\naffine space, 8\\nalgebraic combinatorics, 1\\nassociation scheme, 22\\nautomorphism, 6\\nautomorphism group, 6\\n\\nHamming Bound, 3\\nHamming code, 14\\nHamming distance, 2\\nHamming scheme, 23\\nhexacode, 20\\nhyperoval, 19\\n\\nball, 3\\nbinary Golay code, 14\\nblock, 5\\nBruck-Ryser Theorem, 8\\n\\nincidence geometry, 5\\nincidence matrix, 6\\ninformation theory, 21\\nisomorphism, 6\\n\\nclassification algorithm, 5\\ncoding theory, 1, 5\\ncollinear, 9\\ncombinatorial design theory, 5\\ncombinatorics, 1\\ncryptography, 5\\ncube, 9\\ncyclic code, 4\\n\\nJohnson scheme, 23\\n\\nderived design, 6\\ndesign, 5\\ndiameter perfect code, 15\\ndodecad, 17\\ndual code, 3\\n\\nMathieu groups, 9\\nMathieu-Witt designs, 9\\nminimum distance, 3\\nminimum weight, 3\\nMiracle Octad Generator, 21\\n\\nerror-correcting code, 3\\nextended code, 4\\nextension, 6\\n\\nnearly perfect code, 15\\nnon-trivial design, 5\\n\\nKissing Number Problem, 21\\nlattice, 21\\nlattice packing, 22\\nLeech lattice, 21\\nlinear code, 2\\n\\noctad, 25\\noval, 19\\noverall parity check symbol, 4\\n\\nFano plane, 7\\nfinite geometries, 5\\nFisher’s Inequality, 11\\nfull automorphism group, 6\\n\\nPaley matrix, 16\\n35\\n\\n\\x0c36\\n\\nparity check matrix, 3\\nperfect code, 3\\npermutation equivalent, 3\\nPless symmetry code, 16\\npoint, 5\\nprojective geometry, 8\\nprojective plane, 5, 8\\nprojective space, 8\\npunctured code, 4\\nQR code, 16\\nquadrangle, 5\\nquadratic residue code, 16\\nReed-Muller (RM) code, 13\\nregular tetrahedron, 9\\nself-dual code, 3\\nself-orthogonal code, 3\\nShannon’s Sampling Theorem, 21\\nsphere, 3\\nsphere packing bound, 3\\nSphere Packing Problem, 22\\nspherical code, 22\\nspherical design, 22\\nsporadic simple groups, 10\\nsquare design, 6\\nSquare Root Bound, 16\\nstandard form, 3\\nstatistical design of experiments, 10\\nSteiner t-design, 6\\nSteiner design, 6\\nSteiner quadruple system, 6\\nSteiner system, 6\\nSteiner triple system, 6, 7\\nsupport, 11\\nsymmetric design, 6\\nternary Golay code, 14\\ntrivial design, 5\\nuniformly packed codes, 15\\nweight, 2\\nweight distribution, 4\\nweight enumerator, 4\\nword, 2\\n\\nIndex\\n\\n\\x0c', 'Generating Random Networks Without Short Cycles\\nMohsen Bayati\\nGraduate School of Business, Stanford University, Stanford, CA 94305, bayati@stanford.edu\\n\\nAndrea Montanari\\nDepartments of Electrical Engineering and Statistics, Stanford, CA 94305, montanar@stanford.edu\\n\\narXiv:0811.2853v2 [cs.DS] 29 Dec 2017\\n\\nAmin Saberi\\nDepartments of Management Science and Engineering and Institute for Computational and Mathematical Engineering,\\nStanford, CA 94305, saberi@stanford.edu\\n\\nRandom graph generation is an important tool for studying large complex networks. Despite abundance\\nof random graph models, constructing models with application-driven constraints is poorly understood. In\\norder to advance state-of-the-art in this area, we focus on random graphs without short cycles as a stylized\\nfamily of graphs, and propose the RandGraph algorithm for randomly generating them. For any constant k,\\nwhen m = O(n1+1/[2k(k+3)] ), RandGraph generates an asymptotically uniform random graph with n vertices,\\nm edges, and no cycle of length at most k using O(n2 m) operations. We also characterize the approximation\\nerror for finite values of n. To the best of our knowledge, this is the first polynomial-time algorithm for the\\nproblem. RandGraph works by sequentially adding m edges to an empty graph with n vertices. Recently,\\nsuch sequential algorithms have been successful for random sampling problems. Our main contributions\\nto this line of research includes introducing a new approach for sequentially approximating edge-specific\\nprobabilities at each step of the algorithm, and providing a new method for analyzing such algorithms.\\nKey words : Network models, Poisson approximation, Random graphs\\n\\n1. Introduction\\nRecently, a common objective in many application areas has been extracting information from data\\nsets that contain a network structure. Examples of such data are the Internet, social networks,\\nbiological networks, or healthcare networks such as network of physician referrals. In the last\\nexample, consider the question “how is the network of physician referrals formed?”. Answering this\\nquestion could allow policy makers to influence the formation of the network with the objective of\\nimproving quality of care. This could be achieved by rewarding referrals to higher quality physicians\\nand penalizing referrals to lower performing physicians. Unfortunately, empirical analysis of such\\nnetwork related questions is challenging since in most cases researchers have access to a single\\nnetwork or a few snapshots of it over time. Specifically, the small number of samples renders the\\nestimation part of any parametric network formation model unreliable (Chandrasekhar 2015).\\nA popular approach in statistical data analysis, when facing small number of observations, is\\nbootstrap (Efron 1979) which increases the number of observations by creating random re-samples\\n1\\n\\n\\x0c2\\n\\nof the original data. However, creating random copies of networks can be computationally expensive. For example, if the aim is to create a random copy of the physician referral network while\\nkeeping the number of neighbors (degree) of each node fixed, the problem becomes NP hard in\\ngeneral (Wormald 1999). The property of fixing the number of neighbors is relevant when it is\\ndesired to control for variations in abilities of the physicians to form working relationships. Similarly, one could be interested in creating random copies of a network when certain sub-structures\\nshould be preserved or avoided. This problem in general is unsolved from a theoretical point of\\nview except for few examples where efficient algorithms are proposed (Wormald 1999). Therefore,\\npractitioners use non-rigorous heuristic models of random networks which may lead to incorrect\\n(biased) estimates, see (Milo et al. 2002) for such a heuristic.\\nThe objective of this paper is to advance state-of-the-art in this line of research by proposing a\\nnew algorithm and analysis technique. We present the approach for a stylized subclass of problems,\\ngenerating random graphs without short cycles, and leave extensions to other substructures for\\nfuture research. While our emphasis in this paper is on advancing the methodology, and the family\\nof graphs without short cycles is selected as an example of open problems in this area, we note that\\nrandomly generating graphs from this family has practical implications in information theory. Such\\ngraphs are used in designing low density parity check (LDPC) codes that can achieve Shannon\\ncapacity for transmitting messages in a noisy environment (Richardson and Urbanke 2008).\\n1.1. Contributions\\nWe present a simple and efficient algorithm, RandGraph, for randomly generating simple graphs\\nwithout short cycles. For any constant k, α ≤ 1/[2k(k + 3)], and m = O(n1+α ), RandGraph generates\\nan asymptotically uniform random graph with n vertices, m edges, and no cycle of length k or\\nsmaller. RandGraph uses O(n2 m) operations in expectation. In addition, for finite values of n, we\\ncalculate the approximation error. To the best of our knowledge, this is the first polynomial-time\\nalgorithm for the problem.\\nRandGraph starts with an empty graph and sequentially adds m edges between pairs of nonadjacent vertices. In every step, two distinct vertices i, j with distance at least k are selected with\\nprobability pij , and the edge (ij) is added to the graph. The most crucial step, computing pij ,\\nis obtained by finding a sharp estimate for the number of extensions of the partially constructed\\ngraph, Gt , that contain (ij) and have no cycle of length at most k. This estimation is done by\\ncomputing the expected number of small cycles produced if the rest of the edges are added uniformly\\nat random, using a Poisson approximation.\\nOur analysis of RandGraph involes three approximation steps. First we approximate random\\ngraphs that have m edges and n vertices with Erdös-Rényi (ER) graphs where each edge appears\\n\\n\\x0c3\\n\\nindependently with probability m/\\n\\nn\\n2\\n\\n\\x01\\n\\n. The second approximation uses Janson inequality (Janson\\n\\n1990) for estimating the probability1 that random ER graphs have no cycle of length at most k.\\nThese two approximations provide us with an estimate for the uniform distribution on the family of\\ngraphs without cycles of length at most k. In the final and third step, we approximate Gt with ER\\ngraphs with edge density t/m to estimate the output distribution of RandGraph, and to show that\\nit is asymptotically equal to the uniform distribution. We emphasize that these approximations\\nare easy when m = O(n), and our main contribution is to show that they are sharp even when the\\nnumber of edges is super-linear in n, namely when m = O(n1+α ) for small values of α.\\nWe also provide a theoretical and empirical comparison between RandGraph and the well-known\\ntriangle-free process that has recently been shown to produce triangle-free graphs (our problem\\nwhen k = 3) with an almost uniform distribution (Pontiveros et al. 2013, Bohman and Keevash\\n2013). The comparison shows that the output distribution of RandGraph is much closer to the\\nuniform distribution.\\n1.2. Organization of the Paper\\nThe rest of the paper is organized as follows. §2 discusses related research. Description of RandGraph\\nand the main result are presented in §3. §4 provides the main idea behind RandGraph followed by\\nits analysis in §5. An efficient implementation of RandGraph is presented in §6 and a comparison\\nwith the triangle-free process is given in §7. Finallly, an extension of RandGraph to bipartite graphs\\nwith given degrees is discussed in §8.\\n\\n2. Related Literature\\nRandom graph models have been used in a wide variety of research areas. For example they are\\nused in determining the effect of having overweight friends in adolescent obesity (Valente et al.\\n2009), in studying social networks that result from uncoordinated random connections created\\nby individuals (Jackson and Watts 2002), in modeling emergence of the world wide web as an\\nendogenous phenomena (Papadimitriou 2001) with certain topological properties (Kleinberg 2000,\\nNewman 2003), and in simulating networking protocols on the Internet topology (Tangmunarunkit\\net al. 2002, Faloutsos et al. 1999, Medina et al. 2000, Bu and Towsley 2002).\\nIn information theory, random graphs are used to construct LDPC codes that can approach\\nShannon capacity (Richardson and Urbanke 2008), specifically, when the graphs representing the\\ncodes are selected uniformly at random from the set of bipartite graphs with given degree sequences\\n1\\n\\nWe note that using the Poisson approximation method in §6.2 of (Janson et al. 2000) one can estimate this probability\\nwith an additive error that converges to 0 with a rate that is inversely polynomial in n. However, here we require a\\nstronger approximation since we need a multiplicative error that converges to 1. This would require the additive error\\nto converge to zero faster than the probability of the event itself which is exponentially small in n when m = O(n1+α ).\\n\\n\\x0c4\\n\\n(Amraoui et al. 2007, Chung et al. 2001, Luby et al. 1997). While these random graphs guarantee\\noptimal performances asymptotically, in practice the LDPC graph has between 103 and 105 nodes\\nwhere it is shown that the existence of a small number of subgraphs spoil the code performances (Di\\net al. 2002, Richardson 2003, Koetter and Vontobel 2003). The present paper studies a specific class\\nof such subgraphs (short cycles), but we expect our approach to be applicable to other subgraphs\\nas well. In addition, for the sake of simplicity, we present the relevant proofs only for the problem\\nof generating random graphs without short cycles (not necessarily bipartite nor with prescribed\\ndegrees). Then we will adapt the algorithm for generating random bipartite graphs with given\\ndegree sequences that have no short cycles2 . Generalizing proofs to this case is cumbersome but\\nwe expect that to be conceptually straightforward.\\nRandom graph generation has also been studied extensively as an important theoretical problem\\n(Wormald 1999, Ioannides 2006). From a theoretical perspective, our work is related to the following\\nproblem. Consider a graph property P that is preserved by removal of any edge from the graph. It\\nis a standard problem in extremal graph theory to determine the largest m such that there exists\\na graph with n vertices and m edges having property P . Lower bounds on m can be obtained\\nthrough the analysis of greedy algorithms. Such algorithms proceed by sequentially choosing an\\nedge uniformly from edges whose inclusion would not destroy property P , adding that to the\\ngraph, and repeating the procedure until no further edge can be added. The resulting graph is a\\nrandom maximal P -graph. The question of finding the number of edges of a random maximal P graph for several properties P has attracted considerable attention (Rucinski and Wormald 1992,\\nErdős et al. 1995, Spencer 1995, Bollobás and Riordan 2000, Osthus and Taraz 2001, Bohman and\\nKeevash 2010, Wolfovitz 2011, Pontiveros et al. 2013, Bohman and Keevash 2013, Warnke 2014).\\nIn particular, when P is the property that the graph has no cycles of length k, the above process of\\nsequentially growing the graph is called Ck -free process. Bohman and Keevash (2010) showed that\\nthe process asymptotically leads to graphs with at least some constant times n(n log n)1/(k−1) edges\\nwhich improved earlier results of Bollobás and Riordan (2000) and Osthus and Taraz (2001). For\\nthe case of k = 3, Pontiveros et al. (2013), Bohman and Keevash (2013) proved a sharper result that\\np\\nwith high probability (as n goes to ∞) the number of edges m would be [1 + o(1)]n n log(n)/8\\nwhich is of order n1.5 up to logarithmic factors.\\nIn addition to the bound on m, and related to the topic of this paper, the analyses by Pontiveros\\net al. (2013), Bohman and Keevash (2013) show that certain graph parameters in the C3 -free\\nprocess (also known as triangle-free process) concentrate around their value in uniformly random\\nC3 -free graphs. But these papers do not provide any formal statement on closeness of the two\\n2\\n\\nImplementation details of the application to LDPC codes can be found in this conference paper (Bayati et al. 2009a).\\n\\n\\x0c5\\n\\ndistributions. In contrast, we prove that RandGraph with k = 3, which is a variant of the C3 -free\\nprocess, generates graphs with a distribution that converges in total-variation distance to uniform\\nC3 -free graphs, early in the process; i.e., when m is of order n1+1/36 . We also provide the rate of\\nthis convergence. We note that this range of m is a small subset of the range studied by (Pontiveros\\net al. 2013, Bohman and Keevash 2013), but in §7 we show that our convergence results are sharper\\nand provide stronger concentration for the graph parameters. In §7, we also emprically demonstrate\\nthat the output distribution of RandGraph is much closer to uniform than the C3 -process.\\nHowever, we believe the value of RandGraph and its analysis is when the objective is a more\\ngeneral problem; generating graphs with a given degree sequence that do not have small cycles. In\\nthis setting we expect the natural extension of the Ck -free process would lead to Ck -free graphs with\\na highly non-uniform distribution. This is motivated by (Bayati et al. 2010) that showed, when the\\ndegree sequence is irregular, the process of adding edges uniformly at random in the configuration\\nmodel, while avoiding creation of double-edges or self-loops, generates graphs with a distribution\\nthat is asymptotically equal to the uniform distribution multiplied by an exponentially large bias3 .\\nHowever, providing such a rigorous analysis, when the constraint of avoiding small cycles is added,\\nis still an open problem. We view the present paper as a first step in this direction since it suggests\\na design approach for the problem (see §4 for details). But to simplify the presentation, we focus\\nthe rigorous analysis to the case where the degree sequence constraint is relaxed to just having a\\nfixed number of edges. And in §8, we demonstrate how the approach translates to an algorithm\\nwhen the degree sequence is prescribed and the graph is bipartite.\\nThis paper is also closely related to the literature on designing sequential algorithms for counting\\nand generating random graphs with given degrees (Chen et al. 2005, Blitzstein and Diaconis 2010,\\nSteger and Wormald 1999, Kim and Vu 2007, Bayati et al. 2010, Blanchet 2009). In fact, the current\\npaper builds on this line of research and develops two mainly new techniques: (1) for obtaining\\nprobabilities pij , instead of starting from a biased algorithm, characterizing its bias, and selecting\\npij that can cancel the bias, we use Poisson approximation to directly estimate correct probabilities\\npij that leads to an unbiased algorithm, and (2) for the analysis, we use graph approximation\\nmethods, Janson inequality, and a combinatorial argument to track the accumulated error from\\nsequentially approximating pij in each round.\\nFinally, we note that a preliminary and weaker version of our main result has appeared in\\nproceedings of annual ACM-SIAM Symposium on Discrete Algorithms (Bayati et al. 2009b). In\\nparticular, Theorem 3.1 of Bayati et al. (2009b) only shows that the total variation distance\\nbetween the output distribution (for a different version) of RandGraph and the uniform distribution\\n3\\n\\nFor regular graphs (Steger and Wormald 1999, Kim and Vu 2007, Bayati et al. 2010)\\n√ provide a positive result; the\\noutput distribution becomes asymptotically uniform when the degrees of are order n.\\n\\n\\x0c6\\n\\nconverges to 0 as size of the graphs goes to ∞. But here, we characterize size of the total variation\\ndistance for any finite n, that is of order n−1/2+k(k+3)α . In addition, the aforementioned discussion\\non Ck -free process and its comparison with RandGraph, in §7, are new.\\n\\n3. Algorithm RandGraph and Main Result\\nIn this section we start by introducing some notation and then present our algorithm (RandGraph)\\nfollowed by the main theorem on its asymptotic performance.\\nThe girth of a graph G is defined to be the length of its shortest cycle. Let Gn,m denote the set\\nof all simple graphs with m edges over n vertices and let Gn,m,k be the subset of graphs in Gn,m\\nwith girth greater than k. Throughout the paper k is a constant and is independent of n and m.\\nFor any positive integer s, the set of integers 1, 2, . . . , s is denoted by [s]. The complete graph with\\nvertex set [n] is denoted by Kn . For a graph G with n vertices, we label its vertices by integers in\\n[n]. For each pair of distinct integers i, j ∈ [n], an edge that connects node i to node j is denoted\\nby (ij). All graphs considered in this paper are undirected which means (ij) and (ji) refer to the\\nsame edge.\\nRandGraph starts with an empty graph G0 on n vertices and at each step t, t ∈ {0, 1, . . . , m − 1},\\nan edge (ij) is added to Gt from Q(Gt ), the set of edges that their addition to Gt does not create a\\ncycle of length at most k. Then Gt+1 will be Gt ∪ (ij). If Q(Gt ) is the empty set for some t < m then\\nRandGraph reports FAIL and terminates. The main technical step in RandGraph is that the edge\\n(ij) is selected randomly from Q(Gt ), according to a carefully constructed probability distribution\\nthat is denoted by p(ij |Gt ) and is given by\\np(ij |Gt ) ≡\\nHere Z(Gt ) ≡\\n\\nP\\n\\n(ij)∈Q(Gt ) e\\n\\n−Ek (Gt ,ij)\\n\\n1\\ne−Ek (Gt ,ij) .\\nZ(Gt )\\n\\n(1)\\n\\nis a normalizing term,\\n\\nEk (Gt , ij) ≡\\n\\nk X\\nr−2\\nX\\n\\nGt ,ij r−1−`\\nNr,`\\nqt\\n,\\n\\nr=3 `=0\\nGt ,ij\\nm−t\\n, and Nr,`\\nis the number of simple cycles (cycles that do not repeat a vertex) in Kn that\\n(n2 )−t\\nhave length r, include (ij), and include exactly ` edges of Gt . We will provide the intuition behind\\n\\nqt ≡\\n\\nthis complex-looking formula in §4. In addition, in §6 we will provide an efficient way of calculating\\np(ij |Gt ) using sparse matrix multiplication. Throughout the paper, to simplify the notation, in\\nmathematical formula we will refer to RandGraph by the short notation RG.\\nBy construction, if RandGraph outputs a graph G then G is a member of Gn,m,k . If RandGraph\\noutputs FAIL the algorithm will be repeated till it produces a graph. We will show later that the\\nprobability of FAIL output vanishes asymptotically. Let PRG (G) be the probability that RandGraph\\n\\n\\x0c7\\n\\nAlgorithm 1 RandGraph.\\nInput: n, m, k\\nOutput: An element of Gn,m,k or FAIL\\nset G0 to be a graph over vertex set [n] and with no edges\\nfor each t in {0, . . . , m − 1} do\\nif |Q(Gt )| = 0 then\\nstop and return FAIL\\nelse\\nsample an edge (ij) with probability p(ij |Gt ), defined by Eq. (1)\\nset Gt+1 = Gt ∪ (ij)\\nend if\\nend for\\nif the algorithm does not FAIL before t = m − 1 then\\nreturn Gm\\nend if\\ndoes not FAIL and returns graph G. Let also PU be the uniform probability on the set Gn,m,k ; that\\nis PU (G) = 1/|Gn,m,k |. Our goal is to show that PRG (G) and PU (G) are very close in total variation\\ndistance. The total variation distance between two probability measures P and Q on a set X is\\nn\\no\\ndefined by dT V (P, Q) ≡ sup |P(A) − Q(A)| : A ⊂ X . Now, we are ready to state the main result\\nof the paper. Its proof is provided in §5.\\nTheorem 1. For m = O(n1+α ), m ≥ n, and a constant k ≥ 3 such that α ≤ 1/[2k(k + 3)], the\\nfailure probability of RandGraph asymptotically vanishes and the graphs generated by RandGraph\\nare approximately uniform. In particular,\\nPRG (FAIL) = O(n−1/2+k(k+3)α )\\n\\nand\\n\\ndT V (PRG , PU ) = O(n−1/2+k(k+3)α ) .\\n\\nThe next result shows a run-time guarantee for RandGraph and is proved in §6.\\nTheorem 2. Let n, m, and k satisfy the conditions of Theorem 1. For all n large enough, there\\nexist an implementation of RandGraph that uses asymptotically O(n2 m) operations in expectation.\\n\\n4. The Intuition Behind RandGraph\\nIn order to understand RandGraph, and in particular the calculations for [p(ij |Gt )], it is instructive\\nto examine the execution tree T of a simpler version of RandGraph that sequentially adds m random\\nedges to an empty graph on n vertices to obtain an element of Gn,m (without any attention to\\nwhether a short cycle is generated or not). Consider a rooted m-level tree where the root (the vertex\\nin level zero) corresponds to the empty graph at the beginning of this sequential algorithm and\\nlevel t vertices correspond to all pairs (Gt , πt ) where Gt is a partial graph that can be constructed\\nafter t steps, and πt is an ordering of its t edges. There is a link (edge) in T between a partial\\ngraph (Gt , πt ) from level t to a partial graph (Gt+1 , πt+1 ) from level t + 1 if Gt ⊂ Gt+1 and the first\\n\\n\\x0c8\\n\\nt edges of πt and πt+1 are equal. Any path from the root to a leaf at level m of T corresponds to\\none possible way of sequentially generating a random graph in Gn,m .\\nLet us denote those partial graphs Gt that have girth greater than k by valid graphs. Our goal\\nis to reach a valid leaf in T, uniformly at random, by starting from the root and going down the\\ntree. A myopic approach could be repeating the above sequential algorithm many times until its\\noutput in step m is a valid leaf of T. However, when m = O(n1+α ), the fraction of valid leaves is of\\nα\\n\\norder e−n (see §5 for details). Therefore, this myopic approach has an exponentially small chance\\nof success. Note that the myopic approach works well when m = O(n) since a constant fraction of\\nleaves of T are valid. Therefore, our focus is when m is super linear in n.\\nIn contrast to the myopic approach, RandGraph is designed based on a general strategy for\\nuniformly randomly generating valid leaves of T (Sinclair 1993); at any step t, it chooses (ij) with\\nprobability proportional to the number of valid leaves of T among descendant of (Gt+1 , πt+1 ) where\\nGt+1 = Gt ∪ (ij). Denote this probability by ptrue (Gt+1 , πt+1 ). The main challenge for implementing\\nthis strategy is calculating ptrue (Gt+1 , πt+1 ). In RandGraph we will approximate ptrue (Gt+1 , πt+1 ) with\\np(Gt+1 , πt+1 ) as follows. Let nk (Gt+1 , πt+1 ) denote the number of cycles of length at most k in a leaf\\nchosen uniformly at random among descendants of (Gt+1 , πt+1 ) in T. Note that ptrue (Gt+1 , πt+1 ) is\\nby definition equal to P {nk (Gt+1 , πt+1 ) = 0}. Using Poisson approximation, see (Alon and Spencer\\n1992) for details, one expects the distribution of nk (Gt+1 , πt+1 ) to be approximately Poisson. In\\nparticular,\\nP{nk (Gt+1 , πt+1 ) = 0} ≈ exp (−E[nk (Gt+1 , πt+1 )]) .\\n\\n(2)\\n\\nTherefore, our approximation p(Gt+1 , πt+1 ) will be chosen to be proportional to the right hand side\\nof Eq. (2). This is the main intuition behind Eq. (1).\\nA crucial step in the analysis of RandGraph, provided in §5, is to control the accumulated error\\nm−1\\nY\\x14\\nt=0\\n\\n\\x15\\np(Gt+1 , πt+1 )\\n.\\nptrue (Gt+1 , πt+1 )\\n\\nPrior work (Kim and Vu 2007, Bayati et al. 2010) used sharp concentration inequalities to find a\\nseparate upper bound, for each t, on the error term [p(Gt+1 , πt+1 )/ptrue (Gt+1 , πt+1 )]. Instead, in this\\nQm−1\\npaper we simplify the final product t=0 [p(Gt+1 , πt+1 )/ptrue (Gt+1 , πt+1 )] and will approximate it\\ndirectly which leads to a tighter bound.\\n\\n5. Analysis of RandGraph and Proof of Theorem 1\\nThe aim of this section is to prove Theorem 1. The core of the proof is to show that PRG (G),\\nprobability of generating a graph G by RandGraph, is asymptotically larger than PU (G), the uniform\\nprobability over Gn,m,k . After this result is stated in Lemma 1, it is used to prove Theorem 1. The\\n\\n\\x0c9\\n\\nrest of the section is divided to four subsections. In particular, §5.1 describes the main steps for\\nproving Lemma 1 which rely on auxiliary Lemmas 2 and 3. These auxiliary lemmas are stated in\\n§5.1 and proved in §5.2 and §5.3 respectively. Throughout this section we will introduce a large\\n\\nnumber of new notations. For convenience, we have repeated all notations with their definition in\\nTable 1 of Appendix B.\\nLemma 1. There exist positive constants c1 and c2 such that\\n\\x02\\n\\x03\\nPRG (G) ≥ 1 − c1 n−1/2+k(k+3)α PU (G) ,\\nfor every n, m, k satisfying the conditions of Theorem 1, and all G ∈ Gn,m,k except for a subset of\\ngraphs in Gn,m,k of size c2 exp(−nkα )|Gn,m,k |.\\nIn other words, Lemma 1 shows that for all but o(|Gn,m,k |) graphs G in Gn,m,k inequality PRG (G) ≥\\n[1 − o(1)]PU (G), holds where the term o(1) goes to zero as n goes to infinity uniformly in the graph\\nG. Next, we prove Theorem 1 using Lemma 1.\\nProof of Theorem 1\\n\\nFrom the definition of dT V (PRG , PU ), using triangle inequality, we obtain\\nX\\n\\ndT V (PRG , PU ) ≤\\n\\n|PRG (G) − PU (G)| .\\n\\nG∈Gn,m,k\\n\\nThen, depending on whether PRG (G) ≥ PU (G) or PRG (G) < [1 − c1 n−1/2+k(k+3)α ]PU (G) we bound the\\nterm |PRG (G) − PU (G)| differently. Let Bn,m,k ⊂ Gn,m,k be the set of all graphs G with PRG (G) ≤\\nPU (G) and let the subset Dn,m,k ⊆ Bn,m,k to be those graphs G in Bn,m,k with PRG (G) < [1 −\\nc1 n−1/2+k(k+3)α ]PU (G). To simplify the notation, for the rest of the proof we drop the subscripts\\nn, m, k from Bn,m,k , Dn,m,k and Gn,m,k . Assuming Lemma 1 holds then |D| = c2 e−n\\n\\nkα\\n\\n|G| and for\\n\\nG ∈ B\\\\D\\n|PRG (G) − PU (G)| = PU (G) − PRG (G) ≤ c1 n−1/2+k(k+3)α PU (G) .\\n\\n(3)\\n\\nTherefore,\\n\\x0c\\n\\x0c Xh\\ni\\nX \\x0c\\x0c\\nX \\x0c\\x0c\\n\\x0c\\n\\x0c\\nPRG (G) − PU (G) + 2\\n\\x0cPRG (G) − PU (G)\\x0c =\\n\\x0cPRG (G) − PU (G)\\x0c\\nG∈G\\n\\nG∈G\\n\\n=\\n\\nXh\\n\\n\\x0c\\n\\x0c\\nX \\x0c\\x0c\\nX \\x0c\\x0c\\n\\x0c\\n\\x0c\\nPRG (G) − PU (G) + 2\\n\\x0cPRG (G) − PU (G)\\x0c\\n\\x0cPRG (G) − PU (G)\\x0c + 2\\ni\\n\\nG∈G\\n(a)\\n\\n≤\\n\\nX\\nG∈G\\n\\n(4)\\n\\nG∈B\\n\\nG∈D\\n\\nG∈B\\\\D\\n\\nPRG (G) −\\n\\nX\\n\\nPU (G) + 2c1 n−1/2+k(k+3)α\\n\\nG∈G\\n\\nG∈B\\\\D\\n\\n|D|\\n≤ 1 − PRG (FAIL) − 1 + 2c1 n−1/2+k(k+3)α + 4\\n|G|\\n≤ 2c1 n−1/2+k(k+3)α + 4c2 e−n\\n\\nX\\n\\nkα\\n\\n− PRG (FAIL) ,\\n\\nPU (G) + 4\\n\\nX\\nG∈D\\n\\nPU (G)\\n\\n\\x0c10\\n\\nwhere (a) uses Eq. (3) and triangle inequality. Also, PRG (FAIL) is the probability of failure of\\nRandGraph. In summary, we proved\\ndT V (PRG , PU ) + PRG (FAIL) ≤\\n\\nX\\n\\n|PRG (G) − PU (G)| + PRG (FAIL) = O(n−1/2+k(k+3)α ) ,\\n\\nG∈G\\n\\nwhich finishes the proof \\x03\\nThroughout the rest of this section our focus will be on proving Lemma 1.\\n5.1. Lower Bound For PRG (G): Proof of Lemma 1\\nWe break proof of Lemma 1 into four main steps. Two of these steps (steps 1 and 3 below) will be\\nmajor and involve proving additional Lemmas that will be later proved in §5.2 and §5.3.\\nStep 1 in Proof of Lemma 1: Approximating PU via Jansen inequality.\\n\\nSince PU = 1/|Gn,m,k |,\\n\\nwe will find an asymptotic estimate for |Gn,m,k | using Janson inequality (Janson 1990) that shows\\nthe number of cycles of constant length in Gn,m is approximately a Poisson random variable. The\\nresult is summarized in the following lemma that is proved in §5.2. Before stating the lemma, we\\ndefine Cr to be the set of all simple cycles of length r in Kn and introduce notation N for total\\n\\x01\\nnumber of edges in Kn which is equal to n2 .\\nLemma 2. Let m = O(n1+α ) with α < 1/(2k − 1), k ≥ 3, and m ≥ n, then\\nPU (G)\\nn \\x01\\nh P\\nk\\nN\\nexp − r=3 |Cr |\\nm\\n\\n\\x01\\nm r\\nN\\n\\nio−1 = e\\n\\n\\x12 3kα−1 \\x13\\nO n 2\\n\\n.\\n\\n(5)\\n\\nIn other words, the number of graphs with n vertices, m edges, and no cycle of length up to k is\\n\\x01\\nPk\\n3kα−1\\nN\\n(1 + o(1)) m\\nexp[− r=3 |Cr |(m/N )r ] where the o(1) term is of order n 2 .\\nThe remaining steps will provide necessary approximations and algebraic simplifications to find an\\nasymptotic lower bound for PRG which will be equal to the denominator term in Eq. (5).\\nStep 2 in Proof of Lemma 1: Using convexity and Jensen Inequality.\\n\\nLet us start by writing\\n\\nan expression for PRG (G) when G is a fixed element of Gn,m,k . Note that RandGraph sequentially\\nadds edges to an empty graph to produce a graph with m edges. Hence for the fixed graph G, there\\nare m! permutations of the edges of G that can be generated by RandGraph and each permutation\\ncan be output with a different probability. Let π be any permutation of edges of G (i.e. a one-to-one\\nmapping from {1, . . . , m} to the edges of G), and let Gπt be the graph having [n] as vertex set and\\n{π(1), . . . , π(t)} as edge set. This is the partial graph that is generated after t steps of RandGraph\\n\\nconditioned on having π as output. Now we can write\\nPRG (G) =\\n\\nX m−1\\nY\\nπ\\n\\nt=0\\n\\np(π(t + 1)|Gπt ) .\\n\\n\\x0c11\\n\\nAdditionally, consider the uniform distribution on the set of all m! permutations π. Then,\\n\\nP\\nπ\\n\\ncan\\n\\nbe replaced by m! Eπ where Eπ is expectation with respect to a random permutation π. Hence,\\n(m−1\\n)\\n(m−1\\n)\\nY\\nX\\nπ\\nπ\\nPRG (G) = m! Eπ\\np(π(t + 1)|Gt ) = m! Eπ exp\\nlog p(π(t + 1)|Gt )\\nt=0\\n\\n(m−1t=0\\n)\\nX\\nπ\\n≥ m! exp\\nEπ log p(π(t + 1)|Gt ) ,\\n\\n(6)\\n\\nt=0\\n\\nwhere the inequality is by Jensen inequality for the convex function ex . Next, applying the definition\\nof p(π(t + 1)|Gt ) from Eq. (1) we get\\n\" m−1\\n#\\nm−1\\nX\\nX\\nPRG (G) ≥ m! exp −\\nEπ Ek (Gπt , π(t + 1)) −\\nEπ log Z(Gπt ) .\\nt=0\\n\\n(7)\\n\\nt=0\\n\\nNow, we define F (Gπt ) to be the set of all forbidden pairs at step t, pairs of nodes i and j that\\nadding (ij) to Gπt creates a cycle of length at most k, and set Z0 (Gπt ) ≡ N − t − |F (Gπt )|. Note that,\\nZ(Gπt )\\nlog Z(Gπt ) = log Z0 (Gπt ) + log\\nZ0 (Gπt )\\n\\x14\\n\\x15\\n|F (Gπt )|\\nZ(Gπt )\\n= log (N − t)(1 −\\n) + log\\nN −t\\nZ0 (Gπt )\\n|F (Gπt )|\\nZ(Gπt )\\n≤ log(N − t) −\\n+ log\\n,\\nN −t\\nZ0 (Gπt )\\n\\n(8)\\n\\nusing inequality log(1 − x) ≤ −x for x ∈ (−∞, 1] that holds since |F (Gπt )| ≤ N − t. Combining Eqs.\\n(7) and (8) and using 1/(N − t) ≥ 1/N , we arrive at the following modified lower bound for PRG (G)\\n\\uf8fc\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\" m−1\\n#\\uf8f4\\n# \" m−1\\n# \" m−1\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f2\\nπ\\nX\\nX\\nX\\n1\\nZ(G\\n)\\n1\\nt\\nπ\\nπ\\n−\\nEπ log\\n.\\nPRG (G) ≥ N \\x01 exp\\nEπ Ek (Gt , π(t + 1)) +\\nEπ |F (Gt )| + −\\n\\uf8f4\\nN t=0\\nZ0 (Gπt ) \\uf8f4\\n\\uf8f4\\n\\uf8f4\\nm\\nt=0\\nt=0\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n{z\\n} |\\n{z\\n} |\\n{z\\n}\\uf8f4\\n\\uf8fe\\n\\uf8f3|\\nS1 (G)\\n\\nS2 (G)\\n\\nS3 (G)\\n\\n(9)\\nThe next step is the most important part of our effort in the journey to prove Lemma 1.\\nStep 3 in Proof of Lemma 1: Simplifying S1 (G) + S2 (G) + S3 (G).\\n\\nThis step shows the main\\n\\nbenefit of deferring the calculation of approximation errors for p(ij |Gπt ) to the final step. We will\\nshow that even though the terms Si (G) for i = 1, 2, 3 can be large and dependent on G, many\\nterms in their combined sum cancel out and the resulting expression will be independent of G. In\\nparticular, we will show that the only negative term4 , S1 (G), will completely cancel S2 (G) and all\\ngraph dependent parts of S3 (G). Throughout the rest, since G is fixed, we often drop the references\\nto G in Si : i = 1, 2, 3.\\nThe main result of this step is summarized in the following lemma. First we define Cr,` (G) to be\\nthe set of all simple cycles of length r, belonging to Kn , that include exactly ` edges of G.\\n4\\n\\nS3 (G) will be positive since Z(Gπt ) < Z0 (Gπt ).\\n\\n\\x0c12\\n\\nLemma 3. Let m be larger than n and also satisfy m = O(n1+α ) where α ≤ 1/[2k(k + 3)] for a\\nkα\\n\\nconstant k ≥ 3. Then for all but O(e−n ) fraction of graphs G in Gn,m,k the three inequalities below\\nhold. In other words, the number of graphs in Gn,m,k that violate at least one of the inequalities has\\nkα\\n\\nsize of order e−n |Gn,m,k |.\\n\\x01r−` R 1 `−1\\n\\x01 Pk Pr−1\\n(a) S1 (G) ≥ −O n(k−1)(k+3)α−1 − r=3 `=1 |Cr,` (G)| m\\n` 0 θ (1 − θ)r−` dθ.\\nN\\n\\x10\\n\\x11 P\\n\\x01 R 1 r−1\\nk\\n(b) S2 (G) ≥ −O nk(k+3)α−1/2 + r=3 |Cr,r−1 (G)| m\\nθ dθ.\\nN\\n0\\nR1\\nPk Pr−2\\n(c) S3 (G) ≥ −O(nk(k+3)α−1/2 ) + r=3 `=0 |Cr,` (G)|( m\\n)r−` (r − `) 0 θ` (1 − θ)r−`−1 dθ.\\nN\\nWe defer proof of Lemma 3 to §5.3.\\nStep 4 and the Final Step in Proof of Lemma 1.\\n\\nNext we will show how the different terms in\\n\\nlower bounds for Si ’s from Lemma 3 cancel each other. The main idea in relating the terms in the\\nlower bounds is the following equation which is obtained using integration by parts for r − 1 ≥ ` > 1,\\nZ 1\\nZ 1\\n`−1\\nr−`\\n`\\nθ (1 − θ) dθ = (r − `)\\nθ` (1 − θ)r−`−1 dθ .\\n(10)\\n0\\n\\n0\\n\\nUsing (10) we can see that, when adding the right hand sides of the three inequalities in Lemma\\n3, all terms in the lower bound for S1 with 1 ≤ ` ≤ r − 2 are canceled with the corresponding terms\\nin the lower bound for S3 . In addition, the ` = r − 1 terms in the lower bound of S1 are canceled\\nwith the lower bound of S2 . Therefore, the uncanceled terms are ` = 0 terms from the lower bound\\nof S3 which we will see below to be asymptotically independent of G. More formally, combining\\nkα\\n\\nEq. (9) and Lemma 3, for all graphs G in Gn,m,k except a subset of size O(e−n |Gn,m,k |),\\nPRG (G) ≥\\n≥\\n\\n=\\n\\n1\\nN\\nm\\n\\n1\\nN\\nm\\n\\n1\\nN\\nm\\n\\n\\x01 exp [S1 (G) + S2 (G) + S3 (G)]\\n\"\\n\\x01 exp −O(nk(k+3)α−1/2 ) +\\n\\nk\\nX\\n\\n|Cr,0 (G)|\\n\\nr=3\\n\\n\"\\n\\x01 exp −O(nk(k+3)α−1/2 ) +\\n\\nk\\nX\\nr=3\\n\\n|Cr,0 (G)|\\n\\n\\x10 m \\x11r\\nN\\n\\x10 m \\x11r\\nN\\n\\n#\\n\\n1\\n\\nZ\\n\\n(1 − θ)r−1 dθ\\n\\nr\\n0\\n\\n#\\n.\\n\\n(11)\\n\\nWe note that even though the equality (10) is just an algebraic fact, it can be viewed as doublecounting a combinatorial quantity using two different approaches. The quantity would be number\\nof times a cycle in Kn would be considered in calculation of probability terms p(π(t + 1)|Gπt ).\\nIn §5.3 we perform both counting arguments and then approximate the result of each counting\\nargument with integration with respect to θ = t/m.\\nComparing (11) and the asymptotic expression for PU (G) given by the denominator in left hand\\nside of Eq. (5), we see that the only difference in the exponent is the use of |Cr,0 (G)| instead of |Cr |\\nand the following lemma, proved in §A, provides the final piece.\\n\\n\\x0c13\\n\\nLemma 4. If m = O(n1+α ) and k is constant then |Cr \\\\Cr,0 (G)|/|Cr | = O(nα−1 ).\\nUsing Lemma 4 we have\\nk\\nX\\nr=3\\n\\n|Cr,0 (G)|\\n\\n\\x10 m \\x11r\\nN\\n\\n≥\\n\\nk\\nX\\n\\nk\\n\\x10 m \\x11r\\nX\\n\\x02\\n\\x03 \\x10 m \\x11r\\n|Cr | 1 − O(nα−1 )\\n≥ −O(n(k+1)α−1 ) +\\n|Cr |\\n,\\nN\\nN\\nr=3\\nr=3\\n\\nwhere the last inequality uses |Cr | = O(nr ) and m = O(n1+α ). Summarizing, using Lemmas 2-3, for\\nkα\\n\\nall graphs G in Gn,m,k except a subset of size O(e−n |Gn,m,k |) we have\\nh\\n\\x01 i\\nPk\\nm r\\nk(k+3)α−1/2\\nexp −O(n\\n) + r=3 |Cr | N\\n\\x01\\nPRG (G) ≥\\nN\\nm\\nk(k+3)α−1/2\\n\\n\\x03\\n≥ exp −O(n\\n) − O(n(3kα−1)/2 ) PU (G)\\n\\x02\\n\\x03\\n= exp −O(nk(k+3)α−1/2 ) PU (G)\\n\\x02\\n\\x03\\n≥ 1 − O(nk(k+3)α−1/2 ) PU (G) .\\n\\x02\\n\\nHere the last inequality uses ex ≥ 1 + x. The above equation means that there is a constant c1\\nwhere PRG (G) ≥ [1 − c1 nk(k+3)α−1/2 )]PU (G) for the same family of graphs which finishes proof of\\nLemma 1. Therefore, all we need now is proving Lemmas 2-3 \\x03\\n5.2. Approximating |Gn,m,k | and Proof of Lemma 2\\nBefore delving into the details, we provide a high-level overview of the proof. The main idea is\\nto look at the random graph model Gn,m and estimate the probability of the event of having\\na graph with girth larger than k using Janson inequality. However, we will do all of this on an\\napproximation to the random graph model Gn,m , namely random graph model Gn,p where each\\nedge on vertices of [n] appears independently randomly with probability p = m/N . This type of\\napproximation is well-known in random graph literature (Janson et al. 2000). Any graph in Gn,p\\nwould have on average m edges, making Gn,p a natural approximation to Gn,m .\\n5.2.1. Approximating Pn,p (Ak ) via Janson Inequality. First we define Janson inequality.\\nDefinition 1 (Janson Inequality). Let I be a set of graphs on the vertex set [n]. Now consider\\na random graph G from Gn,p , for any i ∈ I we define a “bad event” Bi to be when G contains i\\nas a subgraph. Janson inequality aims to estimate the probability that G does not contain any\\n(c) \\x01\\n(c)\\nsubgraph in I, that is equal to P ∩i∈I Bi , when the events {Bi }i∈I are almost independent.\\nMore formally, let η, ξ be real numbers such that and for all i in I,\\nP(Bi ) ≤ η < 1\\n\\nand\\n\\nX\\nBj ∼Bi\\n\\nP(Bi ∩ Bj ) = ξ .\\n\\n\\x0c14\\n\\nHere Bi ∼ Bj means that Bi , Bj are dependent which means the subgraphs i and j have at least\\none common edge. Then Janson inequality is\\nY\\n\\n(c)\\nP(Bi )\\n\\n≤P\\n\\n(c) \\x01\\n∩i∈I Bi\\n\\ni∈I\\n(c) \\x01\\n\\nIn particular, for ξ = o(1) we have P ∩i∈I Bi\\n\\n\\x12\\n\\n\\x13Y\\nξ\\n(c)\\n≤ exp\\nP(Bi ) .\\n2(1 − η) i∈I\\n= (1 + o(1))\\n\\nQ\\n\\n(12)\\n\\n(c)\\ni∈I P(Bi ).\\n\\nRemark 1. Janson inequality is not necessarily about subgraphs of a random graph and is more\\ngeneral. For brevity we stated the inequality in the above form and defer the reader to (Janson\\n1990) or (Alon and Spencer 1992) for the more general version.\\nLet us denote the probability with respect to the randomness in Gn,p and Gn,m by Pn,p and Pn,m\\nrespectively. Let Ak be the event that a random graph, selected from G(n, p) or G(m, n), has girth\\ngreater than k. Our next step is to calculate Pn,p (Ak ).\\nFor every cycle i of length at most k on vertices of [n] we consider a bad event Bi that is the\\nevent that a random graph G from Gn,p contains cycle i. In particular, I = ∪kr=3 Cr . It is not difficult\\nPk Pk\\nto see that P(Bi ) = O(pk ) and ξ = O( r1 =3 r2 =3 nr1 +r2 −2 pr1 +r2 −1 ). And since p = O(nα−1 ) then\\nusing Janson inequality (12),\\nY\\n\\nP(Bi ) ≤ Pn,p (Ak ) ≤ eO(n\\n(c)\\n\\n(2k−1)α−1\\n\\ni∈I\\n\\n)\\n\\nY\\n\\n(c)\\n\\nP(Bi )\\n\\ni∈I\\n\\nwhich gives the following for α < 1/(2k − 1),\\nPn,p (Ak ) = eO(n\\n\\n(2k−1)α−1\\n\\n)\\n\\nY\\n\\nO (n(2k−1)α−1 )\\n\\nY\\n\\n(c)\\n\\nP(Bi )\\n\\ni∈I\\n\\n=e\\n\\n1 − plength(i)\\n\\n\\x01\\n\\ni∈I\\n\\n\"\\n= exp O n\\n\\n(2k−1)α−1\\n\\n\\x01\\n\\n+\\n\\nk\\nX\\n\\n#\\nr\\n\\n|Cr | log(1 − p )\\n\\nr=3\\n\\n\"\\n= exp O n\\n\\n\\x01\\n(2k−1)α−1\\n\\n−\\n\\nk\\nX\\n\\n#\\n|Cr |p\\n\\nr\\n\\n.\\n\\n(13)\\n\\nr=3\\n\\nThe last step uses log(1 − x) = −x + O(x2 ) and |Cr |p2r = O(n2rα−r ) = O(n(2k−1)α−1 ).\\n5.2.2. Approximating Pn,m (Ak ) with Pn,p (Ak ). We start by stating the following result on\\nmonotone properties of Gn,p and Gn,m . However, we only state it for the specific event Ak but it\\napplies to more general events that satisfy the following property. If G is in Ak then any graph G0 ,\\nobtained by removal of an edge from G, would also be contained in Ak . Such events are known as\\nmonotone graph properties.\\n\\n\\x0c15\\n\\nProposition 1 (Lemma 1.10 in (Janson et al. 2000)). For 0 ≤ p ≤ p0 ≤ 1 and 0 ≤ m ≤ m0 ≤\\nN we have Pn,p (Ak ) ≥ Pn,p0 (Ak ) and Pn,m (Ak ) ≥ Pn,m0 (Ak ).\\nProof of Lemma 2.\\n\\nFirst define m(G) to be the number of edges for any graph G. Now we state\\n\\nthe following lemma for comparing Pn,p (Ak ) and Pn,m (Ak ) that is proved in Appendix A.\\nLemma 5. For any 0 < p < 1, 1 < m < N , and the monotone event Ak described above we have\\n\\x10\\n\\x11\\nPn,p (Ak ) ≤ Pn,m (Ak ) + Pn,p m(G) < m ,\\n(14)\\n\\x10\\n\\x11\\nPn,p (Ak ) ≥ Pn,m (Ak ) − Pn,p m(G) > m .\\n(15)\\nNext, we state a lemma, proved in §A using Hoeffding inequality, that provides a sharp upper\\nbound for the probability of the event that a graph G in Gn,p does not have exactly m edges when\\np is close to m/N .\\nLemma 6. For β with 0 < β < 1 if m is large enough and p1 ≡ m−mN\\n\\x10\\n\\x11\\nβ\\nPn,p1 m(G) > m ≤ e−m /8 ,\\n\\x10\\n\\x11\\nβ\\nPn,p2 m(G) < m ≤ e−m /8 .\\n\\n1+β\\n2\\n\\n1+β\\n\\nand p2 ≡\\n\\nm+m 2\\nN\\n\\n, we have\\n(16)\\n(17)\\n\\nNow we can use (15) for m and p1 together with (16) to obtain\\n\\x10\\n\\x11\\nmβ\\nPn,m (Ak ) ≤ Pn,p1 (Ak ) + Pn,p1 m(G) > m ≤ Pn,p1 (Ak ) + e− 8 .\\n\\n(18)\\n\\nSimilarly, (14) for m and p2 combined with (17) gives\\n\\x10\\n\\x11\\nmβ\\nPn,m (Ak ) ≥ Pn,p2 (Ak ) − Pn,p2 m(G) < m ≥ Pn,p2 (Ak ) − e− 8 .\\n\\n(19)\\n\\n5.2.3. Finalizing Proof of Lemma 2. First, to simplify the formulas we introduce new\\nnotation that will only be used in §5.2.3 . Recall from (13) that Pn,p = exp[−H(p) + O(n(2k−1)α−1 )]\\nPk\\nwhere H(p) = r=3 |Cr |pr . Combining (18) and (19) and using this new notation we have,\\neH(p)−H(p2 )+O(n\\n\\n(2k−1)α−1\\n\\n)\\n\\n− e−\\n\\nmβ +H(p)\\n8\\n\\n≤\\n\\n(2k−1)α−1\\nPn,m (Ak )\\nmβ\\n)\\n≤ eH(p)−H(p1 )+O(n\\n+ e− 8 +H(p) . (20)\\nexp[−H(p)]\\n\\nNote that the condition pi = O(nα−1 ) needed for (13) holds since β < 1. Now, using the mean value\\ntheorem, for each i ∈ {1, 2} there is a p∗i between p and pi such that\\n!\\n(1+β)\\n\\x10 (1+α)(1+β)\\n\\x11\\n\\x01\\nm 2\\n0 ∗\\n+(k−1)α−1\\n2\\n|H(p) − H(pi )| = |pi − p| · |H (pi )| = O\\nO n(k−1)α+1 = O n\\n.\\nN\\nNow, for β < (k + 1)α/(1 + α), the right hand side in the above will be O(n(3kα−1)/2 ). On the other\\nβ\\n\\nhand, using H(p) = O(nkα ), when β > kα/(1 + α) the term e−m /8+H(p) will be o(1). Combining\\n\\x10\\n\\x11\\nkα (k+1)α\\n, 1+α\\nwe have\\nthese with Eq. (20), and choosing β in the interval 1+α\\nn\\no\\nn\\no\\n3kα−1\\n3kα−1\\nPn,m (Ak )\\n= exp O(n 2 ) + O(n(2k−1)α−1 ) = exp O(n 2 ) .\\nexp [−H(m/N )]\\n\\n\\x0c16\\n\\nNote that, since α < 1/(2k − 1) then such β would be in (0, 1) which is needed by Lemma 6.\\nTherefore,\\nPU (G) =\\n\\n1\\n|Gn,m,k |\\n\\n=\\n\\n1\\nN\\nm\\n\\n\\x01\\n\\nPn,m (Ak )\\n\\n=\\n\\n\\x01\\nN\\nm\\n\\n1\\nn \\x10 3kα−1 \\x11 P\\nk\\nexp O n 2\\n− r=3 |Cr |\\n\\n\\x01\\nm r\\nN\\n\\no.\\n\\nwhich finishes proof of Lemma 2 \\x03\\n5.3. Proof of Lemma 3\\nBefore going into the details we will provide a high level overview of the proof, focusing on S1 (G).\\n5.3.1. A High-level Overview of the Proof. By definition\\nS1 (G) = −\\n\\nm−1\\nX\\nt=0\\n\\nEπ Ek (Gπt , π(t + 1))\\n\\n=−\\n\\nm−1\\nk X\\nr−2\\nXX\\n\\nh\\ni\\nG\\nEπ Nr,`t,ij qtr−1−` .\\n\\nt=0 r=3 `=0\\n\\nThe first approximation we use is to change the randomness given by π. Since the partial graph Gπt\\nis a uniformly random subgraph of G that has exactly t edges, we can look at Gθ which is a random\\nsubgraph of G that has each edge of G independently with probability θ = t/m. The subgraph\\nGθ has t edges in expectation which makes it a good approximation for Gπt . We use this to show\\nR1\\nPm−1\\nthat − t=0 Eπ Ek (Gπt , π(t + 1)) is approximately equal to −m Eθ 0 Ek (Gθ , (ij)) dθ where (ij) is a\\nuniformly random edge of G. This step is carried out algebraically via Lemma 9. Next, note that\\nEk (Gt , ij) would be approximately sum of the terms qtr−`−1 for all pairs (γ, ij) where γ is in Cr,` (G),\\nand (ij) is an edge in (G\\\\Gθ ) ∩ γ. For any fixed r, ` we will see that the sum of all qtr−`−1 terms\\ncorresponding to such (γ, ij) pair is dominated by the cases where |γ ∩ Gθ | = |γ ∩ G| − 1 = `; in other\\nwords when (ij) is the only edge of G ∩ γ that is not in Gθ . This means each cycle γ ∈ Cr,`+1 (G)\\nwould have a (fixed) contribution of qtr−`−1 which is why a term |Cr,`+1 | appears on the right hand\\nside for S1 in Lemma 3(a) (in fact it is |Cr,` | for a shifted range 1 ≤ ` ≤ r − 1).\\n5.3.2. Additional Definitions and Lemmas. Next, we will state three axillary lemmas\\nthat will be used for the proof. But first we introduce an important subset of Gn,m,k . For any graph\\nG, denote its maximum degree by ∆(G). Also, note that Cr,r (G) counts the number of simple cycles\\nof length r that are contained in G. Define the set of graphs Hn,m,k by,\\n\\x1a \\x0c\\n\\x1b \\x12\\n\\x1a \\x0c\\n\\x1b\\x13\\n\\x0c\\n\\x0c\\n2k−2\\n(k+3)α\\n(2k−1)(k+1)α\\n\\x0c\\n\\x0c\\nHn,m,k ≡ Gn,m,k ∩ G \\x0c ∆(G) ≤ n\\n∩ ∩s=k+1 G \\x0c |Cs,s (G)| ≤ n\\nThe next lemma will show that Hn,m,k contains almost all of Gn,m,k and its proof is given in\\nAppendix A\\nh\\ni\\nkα\\nLemma 7. If m, n, k satisfy conditions of Lemma 3 then |Hn,m,k | ≥ 1 − O(e−n ) |Gn,m,k |.\\n\\n\\x0c17\\nGt ,ij\\nWe also need to state the following useful upper bound, proved in Appendix A, on the terms Nr,`\\n\\nappearing in Si ’s.\\nLemma 8. If m, n, k satisfy conditions of Lemma 3 then for all 3 ≤ r ≤ k and G ∈ Hn,m,k we have\\n\\x01\\nGπ ,ij\\n(a) If 0 ≤ ` < r − 1 then Nr,`t = O (∆(G)` nr−2−` ) = O nr−2−`+`(k+3)α .\\n\\x01\\n(b) If 0 ≤ s < r then |Cr,s (G)| = O (∆(G)s−1 nr−s+α ) = O nr−s+s(k+3)α .\\nBefore stating the last auxiliary lemma we need to define the following.\\nDefinition 2. Let e1 , . . . , es be a set of s edges of G. Define At,π\\ne1 ,...,es to be the event that for all\\n1 ≤ i ≤ s : ei ∈ Gπt . Similarly, define Bet,π\\nto be the event that for all 1 ≤ i ≤ s : ei ∈\\n/ Gπt . Let\\n1 ,...,es\\nalso Cet,π\\nbe the event that π(t + 1) = ei . Also, as a convention (when the index s = 0 is used) the\\ni\\nt,π\\ntwo sets At,π\\ncontain everything hand have probability 1.\\n∅ B∅\\n\\nLemma 9. If m, n, k satisfy conditions of Lemma 3 then for any three integers a, b, c in {0, 1, . . . , k }\\nand any set of edges e1 , e2 , . . . , ea+b+1 of G the following hold\\n\\x11\\nR1 a\\nPm−1 \\x10 t,π\\nt c\\nb+c\\nt,π\\ndθ.\\n∩\\nB\\n(a)\\nP\\nA\\nπ\\nea+1 ,...,ea+b (1 − m ) ≤ O(1) + m 0 θ (1 − θ)\\ne1 ,...,ea\\nt=0\\n\\x10\\n\\x11\\nR\\nPm−1\\n1\\n1\\nt c\\nt,π\\nt,π\\nt,π\\na\\nb+c\\n(b)\\ndθ.\\nt=0 Pπ Ae1 ,...,ea ∩ Bea+1 ,...,ea+b ∩ Cea+b+1 (1 − m ) ≤ O( m ) + 0 θ (1 − θ)\\n\\x10\\n\\x11\\nR\\nPm−1\\n√\\n1\\nt c\\nt,π\\nt,π\\na\\nb+c\\n(c)\\ndθ.\\nt=0 Pπ Ae1 ,...,ea ∩ Bea+1 ,...,ea+b (1 − m ) ≥ −O( m) + m 0 θ (1 − θ)\\nProof of Lemma 9 is provided in Appendix A. Next, we prove Lemma 3.\\n5.3.3. Finalizing Proof of Lemma 3.\\nProof of Lemma 3 (a).\\nGπ\\nt ,π(t+1)\\n\\nNr,`\\n\\nRecall that S1 (G) = −\\n\\nPm−1 Pk\\nt=0\\n\\nr=3\\n\\nPr−2\\n`=0\\n\\nGπ ,π(t+1) r−1−`\\nqt\\n,\\n\\nEπ Nr,`t\\n\\nwhere\\n\\nis number of cycles of length r in Kn that include edge π(t + 1) and have exactly `\\n\\nedges belonging to Gπt . Every such cycle, will contain at least ` + 1 edges of G so it belongs to\\nCr,s (G) for some s with r − 1 ≥ s ≥ ` + 1. This suggests another way to calculate S1 (G). For every\\n\\ncycle that belongs to Cr,s (G) we can calculate its contribution in S1 (G). Precisely, fix a cycle\\nγr,s ∈ Cr,s (G). Let s1 (γr,s ) be sum of all terms in S1 (G) that are contributed by this cycle. Let\\nGπ ,π(t+1)\\n\\n{e1 , . . . , es } be the set of all s edges in γr,s ∩ G. In order for γr,s to be considered in Nr,`t\\n\\nwe\\n\\nneed to have ` + 1 distinct indices i1 , . . . , i`+1 in [s] such that {ei1 , . . . , ei` } ∈ Gπt , ei`+1 = π(t + 1)\\n\\x01\\nand {e1 , . . . , es }\\\\{ei1 , . . . , ei`+1 } ∈ G\\\\(Gπt ∪ {e`+1 }). There are s` ways to pick the first ` indices\\nand (s − `) ways to pick e`+1 from the remaining ones. Therefore,\\ns−1 \\x12 \\x13\\nm−1\\nX\\nX\\ns\\nt,π\\nt,π\\ns1 (γr,s ) = −\\n(s − `)\\nP(At,π\\n∩ B{e\\n)qtr−1−` .\\nei1 ,...,ei ∩ Cei\\n1 ,...,es }\\\\{ei1 ,...,ei`+1 }\\n`\\n`+1\\n`\\nt=0\\n`=0\\nNow, using qt =\\n\\nm−t\\nN −t\\n\\n(21)\\n\\n= ( NN−t )( m\\n)( m−t\\n) ≤ ( NN\\n)( m\\n)( m−t\\n), Eq. (21), Lemma 9(b) for a = `, b =\\nN\\nm\\n−m\\nN\\nm\\n\\ns − (` + 1), c = r − ` − 1, and that N/(N − m) ≥ 1, we have\\n\\x12\\n\\x13r−1 X\\n\\x15\\nZ 1\\ns−1 \\x12 \\x13\\n\\x10 m \\x11r−1−` \\x14 1\\nN\\ns\\n`\\nr+s−2`−2\\ns1 (γr,s ) ≥ −\\n(s − `)\\nO( ) +\\nθ (1 − θ)\\ndθ .\\nN −m\\n`\\nN\\nm\\n0\\n`=0\\n\\n\\x0c18\\n\\nIt is easy to see that the summation is dominated by the term ` = s − 1 since other terms are\\nan extra factor m/N smaller. The same way, all of the terms involving O(1/m) are smaller by a\\nfactor m. Therefore, using [1 + m/(N − m)]r−1 = 1 + O(m/N ), the largest order term is equal to\\nR1\\n−(m/N )r−s s 0 θs−1 (1 − θ)r−s dθ and everything else is dominated by a constant times (m/N )r−s+1 ;\\ni.e.,\\nm i \\x10 m \\x11r−s\\ns1 (γr,s ) ≥ − 1 + O( )\\ns\\nN\\nN\\nh\\n\\nZ\\n\\n1\\n\\nθs−1 (1 − θ)r−s dθ .\\n\\n0\\n\\nNow, considering all possible cycles γr,s we obtain\\nk\\n\\nr−1\\n\\n\\x10 m \\x11r−s\\nm iXX\\nS1 (G) ≥ − 1 + O( )\\ns\\n|Cr,s (G)|\\nN r=3 s=1\\nN\\nh\\n\\nZ\\n\\n1\\n\\nθs−1 (1 − θ)r−s dθ .\\n\\n0\\n\\nThe last step involves simplifying the terms that involve an extra O(m/N ) term. In particular,\\nusing Lemma 8(b) we have\\nk r−1\\nk X\\nr−1\\n\\x10 m \\x11r−s Z 1\\nX\\nm XX\\ns−1\\nr−s\\nO( )\\n|Cr,s (G)|\\ns\\nθ (1 − θ) dθ = O\\nn(r−s)+s(k+3)α+(r−s+1)α−(r−s+1)\\nN r=3 s=1\\nN\\n0\\nr=3 s=1\\n\\x01\\n= O nα(k+3)(k−1)−1 .\\n\\n!\\n\\nThis finishes proof of part (a).\\nProof of Lemma 3 (b).\\n|F (Gπt )| =\\n\\nk\\nX X\\nGπ\\nt ,ij\\nI(\\nNr,r−1\\n> 0)\\n(ij)\\n\\n≥\\n\\nFirst we need to approximate the number of forbidden pairs |F (Gπt )|.\\n\\nk\\nX\\n\\n(22)\\n\\nr=3\\n\\n\" k\\n#\\nk\\n\\x10\\n\\x11 X X\\n\\x10X\\n\\x11\\nπ\\nG\\n,ij\\nGπ\\nπ\\nt\\nt ,ij\\nI γ ∈ Cr,r−1 (Gt ) −\\nNr,r−1 I\\nNr,r−1\\n>1 ,\\n\\nX\\n\\nr=3 γ∈Cr,r−1 (G)\\n\\n(ij)\\n\\nr=3\\n\\nr=3\\n\\nwhere the inequality is based on a version of inclusion-exclusion formula. In particular, each edge\\nPk\\nGπ\\nt ,ij\\n(ij) with r=3 Nr,r−1\\n= 1 is counted exactly once in both sides of the inequality. But the edges\\nPk\\nPk\\nGπ\\n,ij\\nGπ\\nt\\nt ,ij\\n(ij) with r=3 Nr,r−1\\n> 1 could be counted at most r=3 Nr,r−1\\ntimes in the first summation of\\nthe right hand side. Next, we are going to show that the second term on the right hand side can\\nbe ignored. In particular, the second term is less than the number of times two vertices i and j are\\nconnected by two paths of length at most k − 1 in Gπt . This means i and j are two vertices of a\\ncycle of length between k + 1 to 2k − 2 in Gπt (note that by design Gπt has no cycle of length up to\\nk). Since the number of vertices in such cycles is still a constant, we have\\n\" k\\n#\\n!\\nk\\n2k−2\\n\\x10X\\n\\x11\\nX X\\nX\\nπ\\nGπ\\n,ij\\nG\\n,ij\\nt\\nt\\nNr,r−1\\nI\\nNr,r−1\\n>1 =O\\n|Cs,s (G)| = O(n(2k−1)(k+1)α ) ,\\n(ij)\\n\\nr=3\\n\\nr=3\\n\\nwhere the last equality uses G ∈ Hn,m,k .\\n\\ns=k+1\\n\\n(23)\\n\\n\\x0c19\\n\\nOn the other hand, for any cycle γ ∈ Cr,r−1 (G), using Lemma 9(c) for a = r − 1, b = c = 0, we\\nhave\\n\\nm−1\\nX\\n\\n\\x10\\n\\nEπ I γ ∈\\n\\n√\\n\\n\\x11\\n\\nCr,r−1 (Gπt )\\n\\n1\\n\\nZ\\n\\nθr−1 dθ .\\n\\n≥ −O( m) + m\\nθ=0\\n\\nt=0\\n\\nThus,\\nm−1\\n1 X\\nEπ F (Gπt )\\nS2 (G) =\\nN t=0\\n\\n\\x10\\n\\n≥ −O n\\n\\n\\x10\\n\\n≥ −O n\\n\\n\\x10\\n\\n≥ −O n\\n\\n√\\n\\n(2k−1)(k+1)α+α−1\\n\\n2k(k+2)α−1\\n\\n\\x11\\n\\nk(k+3)α−1/2\\n\\n\\x11\\n\\nk\\n\\nk\\n\\nmX\\nmX\\n−\\n|Cr,r−1 (G)| +\\n|Cr,r−1 (G)|\\nN r=3\\nN r=3\\nk\\n\\n− O(n\\n\\n\\x11\\n\\nα−1\\n2\\n2\\n\\n)O(n\\n\\n1+(k−1)(k+3)α\\n\\nk\\n\\nZ\\n\\nmX\\n|Cr,r−1 (G)|\\n+\\nN r=3\\n\\nZ\\n\\n1\\n\\nθr−1 dθ\\n\\nθ=0\\n\\nmX\\n)+\\n|Cr,r−1 (G)|\\nN r=3\\n\\nZ\\n\\n1\\n\\nθr−1 dθ (24)\\n\\nθ=0\\n\\n1\\n\\nθr−1 dθ .\\nθ=0\\n\\nHere Eq. (24) uses Lemma 8(b). This concludes proof of part (b).\\nRecall the set Q(Gt ) from §3. First note that by definition of Z(Gπt )\\n\\nProof of Lemma 3 (c).\\nand Z0 (Gπt ) we obtain\\nS3 (G) = −\\n\\nm−1\\nX\\n\\n\\x10 P P\\n\\x11\\uf8f6\\nGπ ,ij\\nk\\nr−2\\n− r=3 `=0 Nr,`t qtr−1−`\\n\\uf8f8.\\nP\\n(ij)∈Q(Gπ ) 1\\n\\n\\uf8ebP\\nEπ log \\uf8ed\\n\\nexp\\n(ij)∈Q(Gπ\\nt)\\n\\nt=0\\n\\nt\\n\\n2\\n\\nNow using e−x ≤ 1 − x + x2 for x > 0 we have\\n\\uf8eb\\n\\x10P P\\n\\x10P P\\n\\x11\\n\\x112 \\uf8f6\\nGπ\\nGπ\\nk\\nr−2\\nk\\nr−2\\n1\\nt ,ij r−1−`\\nt ,ij r−1−`\\nm−1\\nqt\\nqt\\nX\\nX\\nr=3\\n`=0 Nr,`\\nr=3\\n`=0 Nr,`\\n2\\n\\uf8ec\\n\\uf8f7\\nS3 (G) ≥ −\\nEπ log \\uf8ed1 −\\n−\\n\\uf8f8.\\nπ\\nπ\\n|Q(Gt )|\\n|Q(Gt )|\\nπ\\nt=0\\n(ij)∈Q(Gt )\\n\\nAlso note that, using Lemma 8(a), we have\\n\\x10P P\\n\\x112\\nGπ\\nk\\nr−2\\nt ,ij r−1−`\\nN\\nq\\nX\\nt\\nr,`\\nr=3\\n`=0\\n(ij)∈Q(Gπ\\nt)\\n\\n|Q(Gπt )|\\n\\n\\uf8eb\"\\n=O\\uf8ed\\n\\nk X\\nr−2\\nX\\n\\n#2 \\uf8f6\\nnr−`−2+`(k+3)α n(r−1−`)(α−1)\\n\\n\\uf8f8\\n\\nr=3 `=0\\n\\n=O n\\nand, using a similar argument, each term\\n\\nPk\\n\\n2(k+3)(k−1)α−2\\n\\nPr−2\\n\\nr=3\\n\\n`=0\\n\\n\\x01\\n\\nGπ ,ij r−1−`\\nqt\\n\\nNr,`t\\n\\n,\\nis of order n(k+3)(k−1)α−1 . There-\\n\\nfore, this term and its squared are asymptotically very small (in particular, added together, they\\nare less than 1). This means we can use − log(1 − x) ≥ x for x < 1 and |Q(Gπt )| ≤ N to obtain\\n\\uf8f9\\n\\uf8ee\\nm−1\\nk X\\nr−2\\nX\\nX\\nX\\n\\x01\\nπ\\n1\\nG ,ij\\nS3 (G) ≥ Eπ \\uf8f0\\nNr,`t qtr−1−` \\uf8fb − m O n2(k+3)(k−1)α−2\\nN t=0\\n(ij)∈Q(Gπ\\n) r=3 `=0\\n\\uf8eet\\n\\uf8f9\\nm−1 k r−2\\n\\x01\\nπ\\n1 X XX \\uf8f0 X\\nG ,ij\\n≥\\nEπ\\nNr,`t qtr−1−` \\uf8fb − O n2k(k+3)α−1 .\\n(25)\\nN t=0 r=3 `=0\\nπ\\n(ij)∈Q(Gt )\\n\\n\\x0c20\\n\\nAlso, in Eq. (25), the summation\\n\\nP\\n\\n(ij)∈Q(Gπ\\nt)\\n\\ncan be broken to two parts; when (ij) ∈ Q(Gπt ) \\\\ G\\n\\nand when (ij) ∈ Q(Gπt ) ∩ G. The latter group is small since, using the same bounds as above, those\\nterms satisfy\\n\\uf8f9\\n\\n\\uf8ee\\nm−1 k r−2\\n1 X XX \\uf8f0\\nEπ\\nN t=0 r=3 `=0\\n\\nGπ\\nt ,ij\\n\\nX\\n\\nNr,`\\n\\nqtr−1−` \\uf8fb\\n\\n\\x12\\n=O\\n\\n(ij)∈Q(Gπ\\nt )∩G\\n\\nm2 n(k+3)(k−1)α−1\\nN\\n\\n\\x13\\n\\n= O(n(k+3)kα−1 )\\n\\n\\x01\\nthat can be absorbed in the O n2(k+3)kα−1 term of Eq. (25).\\nNow, similar to the proof of (a) we will find contribution of a cycle γr,s ∈ Cr,s (G) that is denoted\\nby s3 (γr,s ). The only difference is that this time the edge (ij) should be part of the (r − s) edges\\nγr,s \\\\{e1 , . . . , es } that are not in G. Then we use part (c) of Lemma 9 for a = `, b = (s − `), c =\\nr − ` − 1, and qt ≥ (m/N )(1 − t/m) to obtain,\\ns \\x12 \\x13\\nm−1\\nX\\n1 X s\\nt,π\\nr−1−`\\n(r − s)\\nP(At,π\\ns3 (γr,s ) =\\nei1 ,...,ei ∩ B{e1 ,...,es }\\\\{ei ,...,ei } )qt\\n`\\n1\\n`\\nN `=0 `\\nt=1\\n\\x12 \\x13\\n\\x14 Z 1\\n\\x15\\ns \\x10\\n\\x11\\nX\\nr−1−`\\n√\\nm\\ns\\n1\\ns\\nr+s−2`−1\\n(r − s) m\\nθ (1 − θ)\\ndθ − O( m) .\\n≥\\nN `=0 N\\n`\\n0\\n\\n(26)\\n\\nSimilar to part (a), the contribution of ` = s term will dominate and the remaining terms can be\\n√\\nabsorbed to the O( m) term. In particular,\\n\\x12\\n\\x13\\nZ 1\\n\\x10 m\\n√ \\x11\\nm r−s\\ns\\nr−s−1\\ns3 (γr,s ) ≥ O ( ) (r − s)\\nθ (1 − θ)\\ndθ − O ( )r−s m .\\nN\\nN\\n0\\nTherefore,\\nS3 (G) ≥\\n\\nk X\\nr−2\\nX\\n\\n|Cr,s (G)|\\n\\n\\x10 m \\x11r−s\\n\\nr=3 s=0\\n\\nN\\n\\nZ\\n(r − s)\\n\\n1\\n\\nθs (1 − θ)r−s−1 dθ − O\\n\\n0\\n\\nk X\\nr−2\\nX\\nr=3\\n\\nm\\n1\\n|Cr,s (G)|( )r−s m− 2\\nN\\ns=0\\n\\n!\\n.\\n\\nNow, using Lemma 8(b), we have\\nO\\n\\nk X\\nr−2\\nX\\nr=3\\n\\nm\\n1\\n|Cr,s (G)|( )r−s m− 2\\nN\\ns=0\\n\\n!\\n= O(n−1/2+(k+3)kα )\\n\\nwhich finishes the proof \\x03\\n\\n6. Running Time of RandGraph and Proof of Theorem 2\\nIn this section we will prove that RandGraph can be implemented in a way that its expected\\nrunning time would be of order n2 m operations. The idea is to define surrogate quantities for\\nprobabilities p(ij |Gt ) that are efficiently computable using sparse matrix multiplications (take order\\nn2 operations per each step of the algorithm). The key point is that, by definition, p(ij |Gt ) is a\\nweighted sum over simple cycles. It is known that one can count all cycles (not necessarily simple\\n\\n\\x0c21\\n\\ncycles) of a graph via matrix multiplication of the its adjacency matrix. We will use this fact and\\nprove that the contribution of non-simple cycles will be negligible.\\n(c)\\n\\nDuring the execution of RandGraph, after adding t edges, let Mt and Mt\\nmatrices of the partially constructed graph Gt and its complement\\n\\n(c)\\nGt\\n\\nbe the adjacency\\n\\nrespectively. In addition,\\n\\nlet Qt be the adjacency matrix of the graph obtained by all edges (ij) such that Gt ∪ (ij) ∈ Gn,t+1,k .\\nWe modify RandGraph so that it selects the (t + 1)th edge from all pairs (ij) with probability\\np0 (ij |Gt ) that is equal to (i, j) entry of the symmetric matrix P0Gt , defined by\\n!r #\\n\" k−1\\nX\\n1\\nm − t (c)\\n0\\n0\\n.\\nPGt ≡ [p (ij |Gt )] ≡ 0\\nQt \\x0c ed\\nxp −\\nMt + n\\x01\\nMt\\nZ (Gt )\\n−\\nt\\n2\\nr=2\\n\\n(27)\\n\\nHere Z 0 (Gt ) is a normalization constant. Symbols \\x0c and ed\\nxp represent the coordinate-wise multiplication and exponentiation of square matrices. More precisely, for n × n matrices A, B, C the\\nexpression A = B \\x0c C means that for all i, j ∈ [n] we have aij = bij cij , and similarly A = ed\\nxp(B)\\nmeans for all i, j ∈ [n] we have aij = ebij . Let us call this modification RandGraph0 .\\nThe key result of this section is the following Lemma and is proved in Appendix A.\\nLemma 10. For any non-zero probability term p0 (ij |Gt ),\\np0 (ij |Gt ) ≥\\nwhere Z(Gt ) =\\n\\n−Ek (Gt ,rs)\\nrs∈Q(Gt ) e\\n\\nP\\n\\n\\x10\\n\\n−Ek (Gt ,ij)−O nk(k+3)α−2\\n\\n1\\ne\\nZ(Gt )\\n\\n\\x11\\n,\\n\\nis the normalization term in definition of p(ij |Gt ) from §3.\\n\\nUsing Lemmas 1 and 10 we can see that the output distribution of RandGraph0 still satisfies the\\n0\\n\\ninequality PRG0 (G) ≥ e−c1 n\\n\\n−1/2+k(k+3)α\\n\\nkα\\n\\nPU (G) for all but O(e−n )|Gn,m,k | graphs G in Gn,m,k . More\\n\\nformally, a variant of Lemma 1 holds for PRG0 using Lemma 1 for PRG and Lemma 10. Next, we\\nfocus on the implementation of RandGraph0 .\\nThe fact that RandGraph0 has polynomial running time is clear since the matrix of the probabilities at any step, PGt , can be calculated using matrix multiplication. In fact a myopic calculation\\nshows that PGt can be calculated with O(k n3 ) = O(n3 ) operations. This is because rth power of a\\nmatrix for any r takes O(rn3 ) operations to compute. So we obtain the simple bound of O(n3 m)\\nfor the running time. But we can improve this running time by at least a factor n with exploiting\\nthe structure of the matrices.\\nPk−1 r\\nd\\nNotice that the adjacency matrix Qt is equal to Jn − sign(\\nr=0 Mt ) where Jn is the n by n\\nd\\nmatrix of all ones and the sign(B)\\nfor any matrix B means the sign function is applied to each\\nentry of B. This is correct since any bad pair (ij), that cannot be added to Gt , corresponds to a\\npath in Gt of length r between i and j for 0 ≤ r ≤ k − 1. Such path forces the ij entry of the matrix\\nMrt to be positive.\\n\\n\\x0c22\\n\\nNow we can store the matrices Mt , . . . , Mk−1\\nat the end of each iteration and use them to\\nt\\nefficiently calculate Mt+1 , . . . , Mk−1\\nt+1 . This is because the differences Mt+1 − Mt are sparse matrices\\nand updating the matrix multiplications can be done with O(n2 ). More precisely, we can use\\nr\\n\\nMrt+1 = [Mt + (Mt+1 − Mt )] = Mrt + L ,\\nwhere L is a linear sum of matrix products where each term contains at least one of (Mt+1 −\\nMt ), · · · , (Mt+1 − Mt )r−1 . Since Mt+1 − Mt has O(1) non-zero entries then the total operations\\nh\\nrequired for calculating L is of O(n2 ). A similar argument can be used for calculating Mt+1 +\\nir\\n(c)\\n(c)\\n(c)\\nm−t+1\\nusing sparsity of both Mt+1 − Mt and Mt+1 − Mt .\\nM\\n(n2 )−t+1 t+1\\nSince Theorem 1 shows that RandGraph and hence RandGraph0 are successful with probability\\n1 − n−1/2+k(k+3)α , the expected running-time of RandGraph0 for generating an element of Gn,m,k is\\nalso O(n2 m), for n large enough, which finishes proof of Theorem 2 \\x03\\n\\n7. Comparing RandGraph and Ck -free Process\\nIn this section, we perform a theoretical (§7.1) and an empirical comparison (§7.2) between our\\nresults for RandGraph and existing theory for Ck -free process. The motivation for this comparison\\nis due to recent research by Pontiveros et al. (2013), Bohman and Keevash (2013). They show\\nthat certain graph parameters in the C3 -free process concentrate around their value in uniformly\\nrandom C3 -free graphs. But these papers do not provide any formal statement on closeness of\\nthe two distributions. Our goal is to understand how close the output distribution of C3 -free and\\nRandGraph are to the uniform distribution on Gn,m,k .\\n7.1. Concentration Inequality for Graph Parameters\\nRecall that Q(G) was defined to be the subset of edges in Kn that adding them to G does not\\ncreate a cycle of length at most k. We enrich this notation by adding a subscript k, i.e. using\\nQk (G). Also let TF be the short notation for the triangle-free (C3 -free) process. We will show\\nthat Theorem 1 provides a sharper concentration than Theorem 2.1 of Pontiveros et al. (2013) for\\nQ3 (G). Pontiveros et al. (2013) show that\\n\\x0c\\n\\x1a \\x0c\\n\\x1b\\n\\x0c\\n\\x0c\\n2\\n3\\n|\\nQ\\n(G)\\n|\\n3\\n2m\\n/n\\n−1/4\\n3\\n\\x0c < 2e\\nn\\n(log n) = 1 .\\nlim PTF \\x0c\\x0c1 −\\nn→∞\\nEU |Q3 (G)| \\x0c\\n\\n(28)\\n\\nOn the other hand, we note the following corollary of Theorem 1 for Qk (G) that is proved in\\nAppendix A.\\nCorollary 1. Let n, m, and k satisfy the conditions of Theorem 1. Then there exists a constant\\nc3 such that\\n\\x0c\\n\\x1a \\x0c\\n\\x1b\\n\\x0c\\n|Qk (G)| \\x0c\\x0c\\n−1+(2k−1)(k+1)α\\n\\x0c\\nPRG\\n= 1 − O(n−1/2+k(k+3)α ) .\\n\\x0c1 − EU |Qk (G)| \\x0c < c3 n\\n\\n(29)\\n\\n\\x0c23\\n\\nFor small enough α, the bound (29) is clearly more general than (28) since it applies to k ≥ 3\\nand the rate of convergence for the probability is provided. But, more importantly, the error term\\nn−1+(2k−1)(k+1)α is much smaller than 2e2m\\n\\n2\\n\\n/n3\\n\\nn−1/4 (log n)3 ≈ n−1/4 when (2k − 1)(k + 1)α < 3/4.\\n\\nFor example, when k = 3 and α < 0.025, the error term in (29) is O(n−1/2 ). We should note that the\\nresult of Pontiveros et al. (2013) is instead valid for a much larger range of graphs (up to m ≈ n1.5 )\\ncompared to our bound that is valid for m = O(n1+α ).\\nPontiveros et al. (2013) also prove similar asymptotic approximations as in (28) for several other\\ngraph parameters than |Q(G)|. We expect the same argument as above can be applied to obtain\\nsharper concentrations for those parameters as well (when α is a small).\\nIt is worth noting that the above comparison is between the bounds proved for two different\\nalgorithms, Ck -free process and RandGraph. But an interesting comparison, that we leave for future\\nresearch, could be done by applying the analysis of RandGraph from this paper to Ck -free process\\nand obtaining a similar variant of (29) for the Ck -free process.\\n7.2. Empirical Comparison\\nIn last section we showed that our bound on dT V (PRG , PU ) is sharper than existing theory on\\ncloseness of C3 -free process to PU . But we did not answer the question: Is dT V (PRG , PU ) is smaller\\nthan dT V (PTF , PU ). In order to shed light on this, below we perform an empirical comparison between\\nRandGraph and triangle-free process.\\nGiven that at step t of either algorithm we know the value of p(π(t + 1)|Gπt ), we can use that to\\n(empirically) compare the output distribution of each algorithm with uniform. In particular, for a\\nsuccessful run of RandGraph that outputs a graph G with ordering π of its edges we estimate its\\nmultiplicative bias by\\nQm−1\\nm!\\np(π(t + 1)|Gπt )\\nBiasπRG ≡ n \\x01 t=0 h\\n\\x01 m \\x013 io−1 .\\nN\\nn\\nexp − 3 N\\nm\\n\\n(30)\\n\\nFrom Lemma 2, for α < min[1/(2k − 1), 1/(3k)] ≈ 0.11, the denominator in BiasπRG is close to PU (G)\\nand the numerator is approximately equal to PRG (G) since there are m! orderings π for edges of G.\\nSimilarly, we can define BiasπTF by using the values p(π(t + 1)|Gπt ) from the triangle-free process.\\nTherefore, BiasπRG and BiasπTF are approximations to PRG /PU and PTF /PU respectively. In other words,\\nif the multiplicative bias of an algorithm is closer to 1 then its output distribution is also closer to\\nuniform.\\nNext, for n in {50, 100, 200, 400} and m = n1+α where α = 0.1, we execute RandGraph and trianglefree process 1, 000 times. First we note that no algorithm failed during the 1,000 repetitions. Figure\\n1 shows the histograms of BiasπRG and BiasπTF for each n. The following observations can be made\\nfrom the simulation:\\n\\n\\x0c24\\n\\nFigure 1\\n\\n(a) n = 50\\n\\n(b) n = 100\\n\\n(a) n = 200\\n\\n(b) n = 400\\n\\nHistogram of multiplicative bias for 1,000 runs of RandGraph and triangle-free process (i.e., BiasπRG and\\nBiasπTF ) for n ∈ {50, 100, 200, 400}. In all cases m = n1+α with α = 0.1.\\n\\n• Bias values for RandGraph are more concentrated around 1 than the ones by triangle-free\\n\\nprocess. This supports the fact that the distance between PRG and PU is less than the distance\\nbetween PTF and PU .\\n• The bias of RandGraph seems to converge to 1 as n grows which suggests that our results\\n\\n(possibly) hold for a larger range of α than what is required by Theorem 1, i.e., α ∈ (0, 0.11) versus\\nα ∈ (0, 0.027).\\n\\n\\x0c25\\n\\n8. Extension to Bipartite Graphs with Given Degrees\\nThe ideas described in §4 can be used to generate random bipartite graphs with given node degrees.\\nSuch graphs define the standard model for irregular LDPC codes. In this section we will show how\\nto modify RandGraph for this application. The analysis of this extension is somewhat cumbersome\\nand is beyond the scope of this paper but we expect it to be conceptually similar to the analysis of\\nRandGraph. Since this is a short section, the notation introduced here is not presented in Table 1.\\nConsider two ordered sequences of positive integers r̄ = (r1 , . . . , rn1 ) and c̄ = (c1 , . . . , cn2 ) for\\nPn1\\nPn2\\ndegrees of the vertices such that m = i=1\\nri = j=1\\ncj . We would like to generate a random\\nbipartite graph G(V1 , V2 ), V1 = [n1 ] and V2 = [n2 ], with girth greater than k and with degree\\nsequence (r̄, c̄). We also assume that k is an even number. Denote the set of all such graphs by\\nGr̄,c̄,k . The algorithm is a natural generalization of RandGraph where the probabilities p(ij |Gt ) are\\nadjusted properly.\\nAlgorithm 2 BipRandGraph.\\nInput: Degree sequence (r̄, c̄) and k\\nOutput: An element of Gr̄,c̄,k or FAIL\\nset G0 to be a graph over vertex sets V1 = [n1 ], V2 = [n2 ] and with no edges.\\nlet r̂ = (r̂1 , . . . , r̂n ) and ĉ = (ĉ1 , . . . , ĉm ) be ordered sets that are initialized by r̂ = r̄ and ĉ = c̄\\nfor each t in {0, . . . , m − 1} do\\nif adding any edge to Gt creates a cycle of length at most k then\\nstop and return FAIL\\nelse\\nsample an edge (ij) from V1 × V2 with probability p00 (ij |Gt ), defined by Eq. (31)\\nset Gt+1 = Gt ∪ (ij)\\nset r̂i = r̂i − 1 and ĉj = ĉj − 1\\nend if\\nend for\\nif the algorithm does not FAIL before t = m − 1 then\\nreturn Gm\\nend if\\n\\nHere each probability p00 (ij |Gt ) is an approximation to the probability that a uniformly random\\nextension of graph Gt ∪ (ij) has girth larger than k (the intuitive reason for this is described in §4).\\nThe estimation procedure for p00 (ij |Gt ) is slightly more involved than the one used for p(ij |Gt ).\\nIt relies on considering a configuration model representation for the graphs with degree sequence\\n(r̄, c̄), see (Bender and Canfield 1978, Bollobás 1980) for more details on configuration model. Then,\\nbuilding on the idea discussed in §4, we get the following Poisson-type approximation for p00 (ij |Gt ),\\n00\\n\\nr̂i ĉj e−Ek (Gt ,ij)\\np (ij |Gt ) ≡\\n,\\nZ 00 (Gt )\\n00\\n\\n(31)\\n\\n\\x0c26\\n\\nwhere Z 00 (Gt ) is a normalization term, and r̂i ĉj , denote the remaining degrees of i and j. FurtherPk/2 P\\nmore, Ek00 (Gt , ij) ≡ r=1 γ∈C2r ptij (γ), where C2r is the set of all simple cycles of length 2r in the\\n(ij)∈γ\\n\\ncomplete bipartite graph on vertices of V1 and V2 Also, ptij (γ) is approximately the probability that\\nγ is in a random extension of Gt to a random bipartite graph with degree sequence (r̄, c̄). More\\nprecisely,\\nptij (γ)\\nwhere\\n\\n=\\n\\n(m − t − 2r + |γ ∩ Gt |)!\\n\\nQ\\n`∈γ∩V1\\n\\nt\\nRij\\n(`, γ)\\n\\nQ\\n`∈γ∩V2\\n\\nCijt (`, γ)\\n\\n(m − t − 1)!\\n\\n,\\n\\n\\uf8f1\\n\\x10\\n\\x02\\n\\x03\\x11\\n\\uf8f4\\nr̂\\n(r̂\\n−\\n1)\\nIf\\ndeg\\nγ\\n∩\\nG\\n∪\\n(ij)\\n= 0,\\n\\uf8f4\\n`\\n`\\nt\\n`\\n\\uf8f4\\n\\uf8f2\\n\\x10\\n\\x02\\n\\x03\\x11\\nt\\nRij (`, γ) = r̂`\\nIf deg` γ ∩ Gt ∪ (ij) = 1 ,\\n\\uf8f4\\n\\x10\\n\\uf8f4\\n\\x02\\n\\x03\\x11\\n\\uf8f4\\n\\uf8f31\\nIf deg` γ ∩ Gt ∪ (ij) = 2 .\\n\\nSimilarly,\\n\\n\\uf8f1\\n\\x10\\n\\x02\\n\\x03\\x11\\n\\uf8f4\\nĉ\\n(ĉ\\n−\\n1)\\nIf\\ndeg\\nγ\\n∩\\nG\\n∪\\n(ij)\\n= 0,\\n\\uf8f4 ` `\\nt\\n`\\n\\uf8f4\\n\\uf8f2\\n\\x10\\n\\x11\\n\\x02\\n\\x03\\nIf deg` γ ∩ Gt ∪ (ij) = 1 ,\\nCijt (`, γ) = ĉ`\\n\\uf8f4\\n\\x10\\n\\uf8f4\\n\\x02\\n\\x03\\x11\\n\\uf8f4\\n\\uf8f31\\nIf deg` γ ∩ Gt ∪ (ij) = 2 .\\n\\nHere the notation degv (H) for a node v of graph G and subgraph H of G refers to the induced\\ndegree of v in H.\\nAppendix A: Proofs of Auxiliary Lemmas\\nProof of Lemma 4\\n\\nIt is easy to see that |Cr | = constant · nr . Now we try to find an upper bound\\n\\nfor the number of paths of length r that intersect at least one edge of G. The number of paths\\n\\x01\\nγ that intersect a fixed edge (ij) in G is of order O(nr−2 ) since there are n−2\\nways to pick the\\nr−2\\nremaining r − 2 vertices of γ and this is the dominating term. And Therefore,\\n!\\nP\\nr−2\\n|Cr \\\\Cr,0 (G)|\\n(ij)∈G n\\n=O\\n|Cr |\\nnr\\n\\x01\\n\\x01\\n= O mn−2 = O nα−1 \\x03\\nProof of Lemma 5\\n\\nWe note that for any 0 < p < 1, the random graph model G(n, p) is equivalent\\n\\nto the random graph model Gn,m conditioned on m(G) = m. Thus, for a random graph G we have\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\nPn,p (Ak ) = Pn,p Ak ∩ {m(G) ≥ m} + Pn,p Ak ∩ {m(G) < m}\\n\\x12\\n\\x13\\nN\\n\\x10 \\x0c\\n\\x11\\n\\x10\\n\\x11\\nX\\n≤\\nPn,p Ak \\x0cm(G) = m Pn,p m(G) = ` + Pn,p m(G) < m\\n`=m\\n\\n\\x12\\n\\x13\\nN\\n\\x10 \\x0c\\n\\x11X\\n\\x10\\n\\x11\\n\\x0c\\n≤ Pn,p Ak m(G) = m\\nPn,p m(G) = ` + Pn,p m(G) < m\\n`=m\\n\\n\\x12\\n≤ Pn,m (Ak ) + Pn,p\\n\\n\\x13\\n|m(G)| < m ,\\n\\n\\x0c27\\n\\nwhere the second inequality uses monotonicity of property Ak . Similarly,\\n\\x10\\n\\x11\\nPn,p (Ak ) ≥ Pn,p Ak ∩ {m(G) ≤ m}\\nm\\n\\x10 \\x0c\\n\\x11\\n\\x10\\n\\x11\\nX\\n=\\nPn,p Ak \\x0cm(G) = ` Pn,p m(G) = `\\n`=0\\nm\\n\\x10\\n\\x11\\n\\x10 \\x0c\\n\\x11X\\nPn,q m(G) = ` ,\\n≥ Pn,p Ak \\x0cm(G) = m\\n\\nusing monotonicity of Ak\\n\\n`=0\\n\\n\\x10\\n\\x11\\n= Pn,m (Ak )Pn,p m(G) ≤ m\\n\\x10\\n\\x11\\n= Pn,m (Ak ) − Pn,p m(G) > m .\\nProof of Lemma 6\\n\\nFirst we state the following modified version of Hoeffding inequality, adapted\\n\\nfrom Corollary 3.2 in (Steger and Wormald 1999).\\nProposition 2 (Hoeffding inequality). Let X1 , . . . , Xn be independent variables with 0 ≤ Xi ≤\\nPn\\n1 for all i ∈ [n], and let X = i=1 Xi . Then for δ ≤ 4/5,\\n\\x0c\\n\\x02\\x0c\\n\\x03\\n2\\nP \\x0cX − E(X)\\x0c > δ E(X) ≤ e−δ E(X)/4 .\\nWe can now take N iid Bernoulli(p) random variables corresponding to the potential edges of G\\nin Gn,p and use Proposition 2 to obtain, for any 0 < p < 1 and 0 < δ < 4/5,\\n\\x0c\\n\\x0c\\n\\x01\\n2\\nPn,p \\x0cm(G) − N p\\x0c > δN p ≤ e−δ N p/4 .\\nNow we can see that by taking δ =\\n\\nm(1+β)/2\\n,\\nm−m(1+β)/2\\nβ\\n\\nwhen β ∈ (0, 1) and m is large enough, we have\\n\\nδ < 4/5, (1 + δ)N p1 = m, and δ 2 N p1 ≥ m /2 which give\\n\\x10\\n\\x11\\n\\x10\\n\\x11\\n2\\nβ\\nPn,p1 m(G) > m ≤ Pn,p1 m(G) > (1 + δ)N p1 ≤ e−δ N p1 /4 ≤ e−m /8 .\\n\\x10\\n\\x11\\nβ\\nm(1+β)/2\\nFor the second inequality, Pn,p2 m(G) < m ≤ e−m /8 , we take δ = m+m\\n(1+β)/2 , which gives (1 −\\nδ)N p2 = m and δ 2 N p2 ≥ mβ /2 and the result similarly follows \\x03\\nProof of Lemma 7\\nn(k+3)α\\n\\nFirst, we will find an upper bound for probability of the event ∆(G) >\\nP2k−2\\nand a separate bound for the event s=k+1 |Cs,s (G)| > n2kα . Then we combine them via\\n\\nunion bound.\\nFor maximum degree, we use the following version of Chernoff inequality, Theorem A.1.18 in\\n(Alon and Spencer 1992). For i.i.d. Bernoulli random variables X1 , . . . , XN with mean p\\n!\\nN\\nX\\n2\\nP\\nXi > η + N p < e−2η .\\ni=1\\n\\nNow combining this with a union bound, for graphs G in Gn,m,k we have for any p ∈ (0, 1)\\nh\\ni\\n2\\nPn,p ∆(G) > (n − 1)p + η < ne−2η .\\n\\n\\x0c28\\n\\nNote that the event {∆(G) > (n − 1) p + η } is a monotone property (see beginning of §5.2.1 for\\ndefinition) but in the opposite direction as Ak that is adding edges to G maintains the property.\\n1+β\\n\\nTherefore, similar to the proof of Lemma 2 we can take p2 =\\n\\nm+m 2\\nN\\n\\nand obtain\\n\\nh\\ni\\nh\\ni\\nh\\ni\\nPn,m ∆(G) > (n − 1)p2 + η < Pn,p2 ∆(G) > (n − 1)p2 + η + Pn,p2 m(G) < m\\n2\\n\\n< ne−2η + e−\\nThus, for β = 1/2 and η = n\\n\\n(k+2)α\\n2\\n\\nmβ\\n8\\n\\n.\\n\\n, combining the above bounds with np2 = O(nα ) and mβ /8 >\\n\\n2n(k+2)α we have\\nh\\ni\\n(k+1)α\\nPn,m ∆(G) > n(k+3)α < e−n\\n.\\n\\n(32)\\n\\nP2k−2\\nNext, we will find a similar bound for Pn,p [ s=k+1 |Cs,s (G)| > n2kα ]. For this, we use the following\\nconcentration inequality for |Cs,s (G)| in Gn,p that is adapted from Corollary 6.2 of Vu (2002),\\n\\x03\\n\\x02\\n(k+1)α\\n).\\nPn,p |Cs,s (G)| > En,p |Cs,s (G)| + ns(k+1)α = O(e−n\\n\\n(33)\\n\\nIn fact, Corollary 6.2 of Vu (2002) provides a bound for more general subgraph counts (not necessarily cycle counts). But in Vu’s bound the tail is of order En,p |Cs,s (G)| = O(nsα ) and the probability\\nis of order exp(−nα ). However, we require a smaller probability of order exp(−n(k+1)α ) and can\\nafford to pick a larger tail. By choosing λ = 4anα(k+1) instead of λ = anα , and leaving everything\\nelse unchanged in Vu’s proof, all conditions satisfy and we obtain (33). Therefore,\\nPn,p\\n\\nh 2k−2\\nX\\n\\n2k−2\\ni\\nh 2k−2\\nX\\nX\\n\\x01i\\n|Cs,s (G)| > n(2k−1)(k+1)α ≤ Pn,p\\n|Cs,s (G)| >\\nEn,p |Cs,s (G)| + ns(k+1)α\\n\\ns=k+1\\n\\ns=k+1\\n\\n≤\\n\\n2k−2\\nX\\n\\ns=k+1\\n\\nh\\n\\nPn,p |Cs,s (G)| > En,p |Cs,s (G)| + ns(k+1)α\\n\\ni\\n\\ns=k+1\\n(k+1)α\\n\\n= O(e−n\\n\\n).\\n\\nNow, defining p2 , m, and β the same as above and repeating the same argument for the monotone\\nP2k−2\\nproperty s=k+1 |Cs,s (G)| > n(2k−1)(k+1)α we have\\nPn,m\\n\\nh 2k−2\\nX\\n\\ni\\nh 2k−2\\ni\\nh\\ni\\nX\\n|Cs,s (G)| > n(2k−1)(k+1)α < Pn,p2\\n|Cs,s (G)| > n(2k−1)(k+1)α + Pn,p2 m(G) < m\\n\\ns=k+1\\n\\ns=k+1\\n\\n= O(e\\n\\n−n(k+1)α\\n\\n).\\n\\nFinally, note that in §5.2 we explicitly calculated Pn,m (Ak ) which shows that Pn,m (Ak )−1 is of\\norder eO(n\\n\\nkα\\n\\n)\\n\\n. Hence,\\n\\n!\\nh\\ni h 2k−2\\ni \\x0c\\x0c\\nX\\n|Hn,m,k |\\n= Pn,m ∆(G) ≤ n(k+3)α ∩\\n|Cs,s (G)| ≤ n(2k−1)(k+1)α \\x0c\\x0c G ∈ Gn,m,k\\n|Gn,m,k |\\ns=k+1\\n\\n\\x0c29\\n\\n\\x10h\\n=\\n\\n≥\\n\\nPn,m\\n\\ni hP\\ni\\n\\x11\\n2k−2\\n(2k−1)(k+1)α\\n∆(G) ≤ n(k+3)α ∩\\n|C\\n(G)\\n|\\n≤\\nn\\n∩\\nA\\ns,s\\nk\\ns=k+1\\nPn,m (Ak )\\n\\nh\\ni\\nhP\\ni\\n2k−2\\n(2k−1)(k+1)α\\nPn,m (Ak ) − Pn,m ∆(G) > n(k+3)α − Pn,m\\n|C\\n(G)\\n|\\n>\\nn\\ns,s\\ns=k+1\\nPn,m (Ak )\\n\\n= 1 − O(e−n\\n\\n(k+1)α\\n\\n+O(nkα )\\n\\nkα\\n\\n) = 1 − O(e−n ) .\\n\\nThis finishes proof of Lemma 7 \\x03\\nProof of Lemma 8\\n\\nGt ,ij\\nClearly Nr,`\\nis bounded from above by the number of paths (not neces-\\n\\nsarily simple paths) of length r − 1 from i to j that have at least ` edges of the Gt . Number of all\\nsuch paths is equal to the number of sequences C = (i = i0 , i1 , . . . , ir−1 = j) with is ∈ [n] for all s,\\nand at least ` of pairs (is is+1 ) in Gt . Since ` < r − 1 there is a pair (is is+1 ) that does not belong to\\nGt . We take s to be the smallest such number. So any path C breaks into C = C1 ∪ {(is is+1 )} ∪ C2\\nwhere C1 is a path starting from i with length s and completely lies inside Gt . Number of such\\npaths is at most ∆(G)s . Similarly C2 is a path with one endpoint equal to j and length r − 2 − s\\nthat contains exactly ` − s edges of Gt . Number of such paths is at most ∆(G)`−s nr−2−` . Therefore\\nusing G ∈ Hn,m,k ,\\nGt ,ij\\nNr,`\\n≤\\n\\n`\\nX\\n\\n∆(G)` nr−2−` = O(nr−2−`+(k+3)`α ) ,\\n\\n(34)\\n\\ns=0\\n\\nwhich finishes proof of part (a).\\nProof of part (b) is similar. If s = 0 then clearly the bound O(nr ) is valid since it is the order of\\nall cycles of length r. Otherwise, each cycle in Cr,s contains an edge (ij) ∈ G. So the cycle contains\\na path of length r that contains (ij) and exactly s − 1 edges of G \\\\ {(ij)}. Therefore, the number\\nP\\nG\\\\{(ij)},(ij)\\nof such cycles is at most O( (ij)∈G Nr,s−1\\n). Note that each cycle is counted at most s times\\nin the bound which is a constant and can be ignored. Using part (a), this number is of order\\nO(m∆(G)s−1 nr−s−1 ) = O(∆(G)s−1 nr−s+α ) which finishes the proof (b).\\n\\x03\\nProof of Lemma 9\\n\\x10\\n\\nNote that Gπt is a random subgraph of G that has t edges. Therefore,\\n\\x11\\n\\nm−a−b\\nt−a\\n\\x01\\nm\\nt\\na+b\\n\\n\\x01\\n\\nt,π\\nPπ At,π\\ne1 ,...,ea ∩ Bea+1 ,...,ea+b =\\n\\x14\\n\\x15\\x14\\n\\x15\\x14\\n\\x15\\nm\\n(m − t) · · · (m − t − b + 1) t · · · (t − a + 1)\\nfa,b (t)\\n=\\nm · · · (m − a − b + 1)\\n(m − t)b\\nta\\n\\n\\x0c30\\n\\nwhere fa,b (t) = ( mt )a ( m−t\\n)b . This means,\\nm\\n\\x10\\nPπ\\n\\nAt,π\\ne1 ,...,ea\\n\\n∩ Bet,π\\na+1 ,...,ea+b\\n\\n\\x11\\n\\n\\x12\\n\\x13a+b\\na+b\\nt c\\nfa,b+c (t)\\n(1 − ) ≤ 1 +\\nm\\nm−a−b\\n\\x12\\n\\x13\\n1\\n≤ 1 + O( ) fa,b+c (t) .\\nm\\n\\n(35)\\n\\nNow using the fact that the function θa (1 − θ)b has at most one maximum in the interval (0, 1)\\nthen\\n\\nPm−1\\n\\nZ 1\\nfa,b+c (t)\\n1\\n≤\\nθa (1 − θ)b+c dθ + O( ) .\\nm\\nm\\nθ=0\\nCombining Eqs. (35) and (36) proves part (a) of Lemma 9.\\nt=0\\n\\n(36)\\n\\nPart (b) is now easy to prove. In particular, given that\\n\\x01\\nm−a−b−1\\n\\x10\\n\\x11\\nt c\\nt\\nt−a\\nt,π\\nt,π\\n\\x01\\n)c =\\n) ,\\nPπ At,π\\ne1 ,...,ea ∩ Bea+1 ,...,ea+b ∩ Cea+b+1 (1 −\\nm (1 −\\nm\\nm\\n(m − t) t\\nusing a similar bound as above, but with an extra m in the denominator, we have\\nPm−1\\nm−1\\n\\x11\\nX \\x10\\nfa,b+c (t)\\n1\\nt c\\nt,π\\nt,π\\nt,π\\n,\\nPπ Ae1 ,...,ea ∩ Bea+1 ,...,ea+b ∩ Cea+b+1 (1 − ) ≤ O( ) + t=0\\nm\\nm\\nm\\nt=0\\nwhich finishes proof of part (b) via Eq. (36).\\nNow, we prove part (c). First we use Bernoulli’s inequality (1 − x)y ≥ 1 − yx for 0 ≤ x < 1, y ≥ 1\\n√\\n√\\nto show that for m − m > t > m\\n\\x01\\nm−a−b\\n\\x10\\n\\x11\\nt c\\nt c t−a\\nt,π\\nt,π\\n\\x01\\nPπ Ae1 ,...,ea ∩ Bea+1 ,...,ea+b (1 − ) = (1 − )\\nm\\nm\\nm\\nt\\na\\nb b\\n≥ (1 − )a (1 −\\n) fa,b+c (t)\\nt\\nm\\n−\\nt\\n\\x14\\n\\x15\\n1\\n≥ 1 − O( √ ) fa,b+c (t) .\\n(37)\\nm\\nAlso, as before,\\n\\nPm−1\\nt=0\\n\\nfa,b+c (t)\\n≥\\nm\\n\\nZ\\n\\n1\\n\\nθa (1 − θ)b+c dθ − O(\\n\\nθ=0\\n\\n1\\n).\\nm\\n\\n(38)\\n\\nHence,\\nm−1\\nX\\nt=0\\n\\n\\x10\\n\\x11\\nt c\\nt,π\\n) ≥\\nPπ At,π\\n∩\\nB\\ne1 ,...,ea\\nea+1 ,...,ea+b (1 −\\nm\\n≥\\n\\n≥\\n≥\\n\\n=\\n\\n\\x10\\n\\x11\\nt c\\nt,π\\n)\\nPπ At,π\\n∩\\nB\\ne1 ,...,ea\\nea+1 ,...,ea+b (1 −\\nm\\n√\\n√\\nm<t<m− m\\n\\x12\\n\\x13\\nX\\n1\\n1 − O( √ )\\nfa,b+c (t)\\nm √\\n√\\nm<t<m− m\\n\\x12\\n\\x13 m−1\\nX\\n√\\n1\\n1 − O( √ )\\nfa,b+c (t) − O( m)\\nm\\nt=0\\nZ\\n√ \\x01 1 a\\n√\\nm − O( m)\\nθ (1 − θ)b+c dθ − O( m)\\nθ=0\\nZ 1\\n√\\nm\\nθa (1 − θ)b+c dθ − O( m) ,\\nX\\n\\nθ=0\\n\\n\\x0c31\\n\\nwhich finishes proof of Lemma 9 \\x03\\nProof of Lemma 10\\n\\n(c)\\n\\nm−t\\nM corresponds\\n(n2 )−t t\\nthat correspond to paths of length\\n\\nThe main idea is that each entry of the matrix Mt +\\n(c)\\n\\nm−t\\nM\\n(n2 )−t t\\nr in Kn . Moreover the sum is dominated by those products that correspond to simple paths rather\\n\\nto sum of all products of entries of the matrix Mt +\\n\\nthan self intersecting paths. Below, we will show this formally.\\nBy definition, for any non-zero (ij) entry of the matrix P0Gt we have:\\n(P0Gt )ij = exp −\\n\\nk X\\nr−2\\nX\\n\\nGt ,ij r−1−`\\nqt\\n−\\nNr,`\\n\\nG ,(ij)\\n\\n!\\nG ,(ij) r−1−`\\nqt\\n\\nMr,`t\\n\\nr=3 `=0\\n\\nr=3 `=0\\n\\nwhere Mr,`t\\n\\nk X\\nr−2\\nX\\n\\nis the number of self intersecting cycles of length r in Kn that include (ij) and\\n\\nexactly ` edges of Gt . Similarly to the argument used in Lemma 8 to prove an upper bound for\\nGt ,ij\\nNr,`\\n, we can show that\\nGt ,ij\\nMr,`\\n= O(nr−3−`+(k+3)`α ) .\\n\\n(39)\\n\\nThere is one factor n less in the right hand side of Eq. (39) compared to the bound we showed\\nGt ,ij\\nin Lemma 8 for Nr,`\\nand the reason is, due to self-intersection of the paths, there exist one less\\n\\ndegree of freedom. Therefore,\\n(P0Gt )ij = exp −\\n\\nk X\\nr−2\\nX\\n\\nG ,(ij) r−1−`\\nqt\\n\\nNr,`t\\n\\n−O n\\n\\n!\\n\\x01\\nk(k+3)α−2\\n\\n.\\n\\nr=3 `=0\\n\\n\\x10 P P\\n\\x11\\nk\\nr−2\\nG ,(ij)\\nGt\\nFor simplicity of the notation let Dij\\n= exp − r=3 `=0 Nr,`t qtr−1−` . Hence,\\n(P0Gt )ij\\nZ 0 (Gt )\\n(P0Gt )ij\\n= P\\n0\\nrs∈Q(Gt ) (PGt )rs\\n\\np0 (ij |Gt ) =\\n\\n(40)\\n= P\\n\\nGt\\nDij\\n\\nexp −O n\\n\\nGt\\nrs∈Q(Gt ) Drs\\n\\nk(k+3)α−2\\n\\n\\x01\\x01\\n\\nexp (−O (nk(k+3)α−2 ))\\n(41)\\n\\n≥ P\\n\\nGt\\nDij\\n\\nGt\\nrs∈Q(Gt ) Drs\\n\\n\\x01\\x01\\nexp −O nk(k+3)α−2\\n\\nwhich finishes the proof \\x03\\nProof of Corollary 1\\n\\nRecall from §5 that F (G) is the number of edges in Kn that when added\\n\\nto G a cycle of length at most k is created. Clearly, Q(G) = N − m − F (G). On the other hand, it\\nPk\\nis clear that F (G) ≤ r=3 |Cr,r−1 |. Therefore, using Lemma 8(b), for all G in Hn,m,k\\nF (G) = O(n(k−1)(k+3)α+1 ) .\\n\\n\\x0c32\\n\\nHence,\\n1−\\n\\nN − m − O(n(k−1)(k+3)α+1 ) O(n(k−1)(k+3)α+1 )\\n|Qk (G)|\\n≤1−\\n=\\n= O(n(k−1)(k+3)α−1 ) .\\nEU|Qk (G)|\\nN −m\\nN −m\\n\\nSimilarly,\\n1−\\n\\nN −m\\n|Qk (G)|\\nO(n(k−1)(k+3)α+1 )\\n≥1−\\n=\\n−\\n= −O(n(k−1)(k+3)α−1 ) .\\nEU|Qk (G)|\\nN − m − O(n(k−1)(k+3)α+1 )\\nO(n2 )\\n\\nTherefore, combining the above two equations and using Lemma 7, there is a constant c3 such that\\n\\x0c\\n\\x1a\\x0c\\n\\x1b\\n\\x0c\\nkα\\n|Qk (G)| \\x0c\\x0c\\n(k−1)(k+3)α−1\\n\\x0c\\nP U \\x0c1 −\\n< c3 n\\n= 1 − O(e−n ) .\\n(42)\\n\\x0c\\nEU|Qk (G)|\\nNow, define the event A by\\n\\x0c\\n\\x1b\\n\\x1a\\x0c\\n\\x0c\\n|Qk (G)| \\x0c\\x0c\\n(k−1)(k+3)α−1\\n\\x0c\\n> c3 n\\n.\\nA = \\x0c1 −\\nEU|Q (G)| \\x0c\\nk\\n\\nFrom the definition of dT V and Theorem 1 we have\\n|PRG (A) − PU (A)| ≤ dT V (PRG , PU ) = O(n−1/2+k(k+3)α ) .\\n\\nTherefore, combining this with Eq. (42),\\nkα\\n\\nPRG (A) ≤ PU (A) + O(n−1/2+k(k+3)α ) = O(e−n ) + O(n−1/2+k(k+3)α ) = O(n−1/2+k(k+3)α )\\nwhich finishes the proof \\x03\\nAppendix B: Mathematical Notations\\nNotation\\n\\nDescription\\n\\n[n]:\\nKn :\\nO:\\no:\\n(ij):\\nn:\\nm:\\nN:\\nm(G):\\nGn,m :\\nGn,p :\\nPn,m :\\nPn,p :\\nGn,m,k :\\nHn,m,k :\\nGn,m,k (τ ):\\nPRG :\\nPU :\\ndT V (P, Q):\\nGt :\\n\\nWhen n is a positive integer it denotes the set {1, 2, . . . , n}.\\nComplete graph with vertex set [n].\\nFor sequences {an }n≥1 , {bn }n≥1 big O notation an = O(bn ) means lim supn→∞ an /bn < ∞.\\nFor sequences {an }n≥1 , {bn }n≥1 little O notation an = o(bn ) means lim supn→∞ an /bn = 0.\\nAn edge that connects node i to node j (i, j ∈ [n]) (in a graph G with vertices [n]).\\nNumber of vertices of graphs considered in the paper.\\nNumber of edges\\x01of most graphs in the paper.\\nDefined to be n2 .\\nNumber edges of a graph G.\\nSet of all simple graphs with m edges and vertices [n].\\nRandom graph model of simple graphs on [n] where each edge is present (independently) with probability p.\\nUniform probability distribution over Gn,m .\\nProbability distribution obtained by random graph model Gn,p .\\nThe subset of graphs in Gn,m with girth greater than k.\\nThe set of graphs G in Gn,m,k with maximum degree of order O(n(k+3)α )\\nSubset of graphs G in Gn,m,k where PRG (G) < (1 − τ )PU (G).\\nOutput distribution of RandGraph which is a distribution on Gn,m,k .\\nUniform distribution on Gn,m,k .\\nTotal variation distance between measures on X and is equal to sup {|P(A) − Q(A)| : A ⊂ X}.\\nPartially constructed graph in RandGraph after t steps.\\n\\n\\x0c33\\nqt :\\nθ:\\nπ:\\nGπt :\\nEπ :\\nPπ :\\nγ:\\nQ(Gt ):\\np(ij|Gt ):\\nEk (Gt , ij):\\nT:\\nπt :\\nnk (Gt , πt ):\\nG,ij\\nNr,`\\n:\\nZ(G):\\nF (Gπt ):\\nZ0 (G):\\nS1 (G):\\nS2 (G):\\nS3 (G):\\nCr :\\nCr,` (G):\\nγr,s :\\nsi (Cr,s ):\\nAk :\\ndegv (H):\\n∆(G):\\nAt,π\\ne1 ,...,es :\\nBet,π\\n:\\n1 ,...,es\\nCet,π :\\nMt :\\n(c)\\nMt :\\nQt :\\nA = B \\x0c C:\\nA = ed\\nxp(B):\\nd\\nA = sign(B):\\nJn :\\n\\nEquals to (m − t)/(N − t).\\nEquals to t/m.\\nA permutation of the edges of G where G ∈ Gn,m .\\nThe graph having [n] as vertex set and {π(1), . . . , π(t)} as edge set.\\nExpectation with respect to a uniformly random permutation π.\\nProbability with respect to a uniformly random permutation π.\\nNotation used for cycles.\\nThe set of edges (ij) that do not belong to Gt and Gt ∪ (ij) ∈ Gn,t+1,k .\\nFor each (ij) ∈ Q(Gt ), it is the probability of selecting (ij) in step t of RandGraph.\\nP\\nP\\nGt ,ij r−1−`\\nqt\\n.\\nEquals to kr=3 r−2\\n`=0 Nr,`\\nExecution tree of a sequential graph generation algorithm like RandGraph (see §4 for details).\\nFor a partially constructed graph Gt , it is an ordering (permutation) of its edges.\\nNumber of cycles of length at most k in a random extension of of a pair (Gt , πt ) in T.\\nNumber of simple cycles in Kn that have length r, include (ij), and include exactly ` edges of G.\\nNormalization constant in definition of p(ij|Gt ) in Eq. (1).\\nThe set of edges (ij) where Gπt ∪ (ij) has a cycle of length at most k.\\nIs equal to N − t − F (Gπt ).\\nP\\nE E (Gπ , π(t + 1)).\\nEquals to − m−1\\nPt=0 π k πt\\n1\\nEquals to N m−1\\nt=0 Eπ F (Gt ). π\\nPm−1\\nZ(G )\\nEquals to − t=0 Eπ log Z0 (Gtπ ) .\\nt\\nSet of all simple cycles of length r in Kn .\\nCycles in Cr that include exactly ` edges of G.\\nAn element of Cr,` (G).\\nFor each i = 1, 2, 3 denotes contribution of cycle Cr,s in Si (G).\\nThe event that a random graph has girth greater than k.\\nInduced degree of a note v in a subgraph H of a larger graph containing v.\\nMaximum degree of graph G.\\nThe event {∀i ∈ [s] : ei ∈ Gπt } when e1 , . . . , es are edges of G.\\nThe event {∀i ∈ [s] : ei ∈\\n/ Gπt } when e1 , . . . , es are edges of G.\\nThe event {π(t + 1) = e} for edge e in G.\\nAdjacency matrix of Gt .\\nAdjacency matrix of complement of Gt .\\nAdjacency matrix of all edges in Q(Gt ).\\nFor n × n matrices A, B, C it means that for all i, j ∈ [n]: aij = bij cij .\\nFor n × n matrices A, B it means that for all i, j ∈ [n]: aij = ebij .\\nFor n × n matrices A, B it means that for all i, j ∈ [n]: aij = sign(bij ).\\nIt is the n by n matrix of all ones.\\nTable 1: Mathematical notations.\\n\\nAcknowledgments\\nThe authors gratefully acknowledge the National Science Foundation (awards CMMI: 1554140 and CCF:\\n1216698) and Office of Naval Research (N00014-16-1-2893) for financial support.\\nThis paper has also benefitted from valuable feedback from Balaji Prabhakar, Joel Spencer, Daniel Spielman, Stefanos Zenios, and anonymous referees.\\n\\nReferences\\nAlon, N., J. Spencer. 1992. The Probabilistic Method . Wiley, New York.\\nAmraoui, A., A. Montanari, R. Urbanke. 2007. How to find good finite-length codes: From art towards\\nscience. Eur. Trans. Telecomm. 18 491–508.\\n\\n\\x0c34\\nBayati, M., R. Keshavan, A. Montanari, S. Oh, A. Saberi. 2009a. Generating random tanner graphs with\\nlarge girth. IEEE Information Theory Workshop. Taormina, Italy. Code available here: http://web.\\nengr.illinois.edu/~swoh/software/girth/index.html.\\nBayati, Mohsen, Jeong Han Kim, Amin Saberi. 2010. A sequential algorithm for generating random graphs.\\nAlgorithmica 58(4) 860–910.\\nBayati, Mohsen, Andrea Montanari, Amin Saberi. 2009b. Generating random graphs with large girth.\\nProceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms. SODA ’09, 566–\\n575. URL http://dl.acm.org/citation.cfm?id=1496770.1496833.\\nBender, Edward A., E. Rodney Canfield. 1978. The asymptotic number of labeled graphs with given degree\\nsequences. J. Comb. Theory, Ser. A 24(3) 296–307.\\nBlanchet, J. 2009. Efficient importance sampling for binary contingency tables. Ann. Appl. Probab. 19\\n949–982.\\nBlitzstein, J., P. Diaconis. 2010. A sequential importance sampling algorithm for generating random graphs\\nwith prescribed degrees. Internet Math. 6 489–522.\\nBohman, T., P. Keevash. 2010. The early evolution of the h-free process. Inventiones mathematicae 181(2)\\n291–336.\\nBohman, T., P. Keevash. 2013. Dynamic concentration of the triangle-free process URL https://arxiv.\\norg/abs/1302.5963.\\nBollobás, B., O. Riordan. 2000. Constrained graph processes. Electronic Journal of Combinatorics 7.\\nBollobás, Béla. 1980. A probabilistic proof of an asymptotic formula for the number of labelled regular\\ngraphs. European Journal of Combinatorics 1(4) 311–316.\\nBu, T., D. Towsley. 2002. On distinguishing between internet power law topology generators. INFOCOM .\\nIEEE.\\nChandrasekhar, A. 2015. Econometrics of network formation. Oxford handbook on the economics of networks.\\n(edited by yann bramoulle, andrea galeotti and brian rogers).\\nChen, Y., P. Diaconis, S. Holmes, J. S. Liu. 2005. Sequential monte carlo methods for statistical analysis of\\ntables. Journal of the American Statistical Association 100 109–120.\\nChung, S. Y., G. D. Forney, T. J. Richardson, R. Urbanke. 2001. On the design of low-density parity-check\\ncodes within 0.0045 db of the shannon limit. IEEE Comm. Lett 5 58–60.\\nDi, C., D. Proietti, I. E. Teletar, T. J. Richardson, R. Urbanke. 2002. Finite-length analysis of low-density\\nparity-check codes on the binary erasure channel. IEEE Trans. Inform. Theory 46.\\nEfron, B. 1979. Bootstrap methods: another look at the jackknife. Ann. Statistics 7 1–26.\\nErdős, P., S. Suen, P. Winkler. 1995. On the size of a random maximal graph. Random Structure and\\nAlgorithms 6 309–318.\\n\\n\\x0c35\\nFaloutsos, M., P. Faloutsos, Ch. Faloutsos. 1999. On power-law relationships of the internet topology. ACM,\\nNew York, NY, USA, 251–262.\\nIoannides, Y. 2006. Random graphs and social networks: An economics perspective. Preprint.\\nJackson, M., D. Watts. 2002. The evolution of social and economic networks. Journal of Economic Theory\\n106 265–295.\\nJanson, Luczak, Rucinski. 2000. Random Graphs. Wiley-Interscience.\\nJanson, S. 1990. Poisson approximation for large deviations. Random Structures and Algorithms 1 221229.\\nKim, J. H., V. H. Vu. 2007. Generating random regular graphs. Combinatorica 26 683–708.\\nKleinberg, J. 2000. Navigation in a small world. Nature 406 845.\\nKoetter, R., P. Vontobel. 2003. Graph covers and iterative decoding of finite-lenght codes. Proc. Int. Conf.\\non Turbo codes and Rel. Topics. Brest, France.\\nLuby, M., M. Mitzenmacher, A. Shokrollahi, D. A. Spielman, V. Stemann. 1997. Practical loss-resilient\\ncodes. ACM Symposium on Theory of Computing (STOC).\\nMedina, A., I. Matta, J. Byers. 2000. On the origin of power laws in internet topologies. ACM Computer\\nCommunication Review 30 18–28.\\nMilo, R., S. ShenOrr, S. Itzkovitz, N. Kashtan, D. Chklovskii, U. Alon. 2002. Network motifs: Simple building\\nblocks of complex networks. Science 298 824–827.\\nNewman, M. 2003. The structure and function of complex networks. SIAM Review 45 167–256.\\nOsthus, D., A. Taraz. 2001. Random maximal h-free graphs. Random Struct. Algorithms 18(1) 61–82.\\nPapadimitriou, C. 2001. Algorithms, games, and the internet 749–753.\\nPontiveros, G. F., S. Griffiths, R. Morris. 2013. The triangle-free process and r(3, k). URL http://arxiv.\\norg/abs/1302.6279. Eprint.\\nRichardson, T. 2003. Error-floors of ldpc codes. Proceedings of the 41st Annual Conference on Communication, Control and Computing. 1426–1435.\\nRichardson, T., R. Urbanke. 2008. Modern Coding Theory. Cambridge University Press, Cambridge.\\nRucinski, A., N. Wormald. 1992. Random graph processes with degree restrictions. Combinatorics Prob.\\nComput. 1.\\nSinclair, A. 1993. Algorithms for random generation and counting: a Markov chain approach. Birkhauser.\\nSpencer, J. 1995. Maximal triangle-free graphs and ramsey r(3, t). Manuscript.\\nSteger, A., N. C. Wormald. 1999. Generating random regular graphs quickly. Combinatorics Prob. and\\nComput 8 377–396.\\n\\n\\x0c36\\nTangmunarunkit, H., R. Govindan, S. Jamin, S. Shenker, W. Willinger. 2002. Network topology generators:\\nDegree-based vs. structural. Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications. SIGCOMM ’02, ACM, New York, NY, USA,\\n147–159.\\nValente, T., K. Fujimoto, C. Chou, D. Spruijt-Metz. 2009. Adolescent affiliations and adiposity: A social\\nnetwork analysis of friendships and obesity. J Adolesc Health 45 202204. doi:10.1016/j.jadohealth.\\n2009.01.007.\\nVu, Van H. 2002. Concentration of non-lipschitz functions and applications. Random Struct. Algorithms\\n20(3) 262–316.\\nWarnke, L. 2014. The c` -free process. Random Struct. Algorithms 44(4) 490–526.\\nWolfovitz, G. 2011. Triangle-free subgraphs in the triangle-free process. Random Struct. Algorithms 39(4)\\n539–543.\\nWormald, N. C. 1999. Models of random regular graphs. London Mathematical Society Lecture Note Series\\n239–298.\\n\\n\\x0c', '1\\n\\nVariations on a theme by Schalkwijk and Kailath\\nRobert G. Gallager\\n\\nBarış Nakiboğlu\\n\\narXiv:0812.2709v4 [cs.IT] 20 Nov 2009\\n\\nAbstract\\nSchalkwijk and Kailath (1966) developed a class of block codes for Gaussian channels with ideal feedback\\nfor which the probability of decoding error decreases as a second-order exponent in block length for rates below\\ncapacity. This well-known but surprising result is explained and simply derived here in terms of a result by Elias\\n(1956) concerning the minimum mean-square distortion achievable in transmitting a single Gaussian random variable\\nover multiple uses of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath scheme is then\\nshown to have an error probability that decreases with an exponential order which is linearly increasing with block\\nlength. In the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy\\nat all rates below capacity. A lower bound on error probability for the finite bandwidth case is then derived in which\\nthe error probability decreases with an exponential order which is linearly increasing in block length at the same\\nrate as the upper bound.\\n\\nI. I NTRODUCTION\\nThis note describes coding and decoding strategies for discrete-time additive memoryless Gaussian-noise (DAMGN)\\nchannels with ideal feedback. It was shown by Shannon [14] in 1961 that feedback does not increase the capacity\\nof memoryless channels, and was shown by Pinsker [10] in 1968 that fixed-length block codes on Gaussiannoise channels with feedback can not exceed the sphere packing bound if the energy per codeword is bounded\\nindependently of the noise realization. It is clear, however, that reliable communication can be simplified by the\\nuse of feedback, as illustrated by standard automatic repeat strategies at the data link control layer. There is a\\nsubstantial literature (for example [11], [3], [9]) on using variable-length strategies to substantially improve the rate\\nof exponential decay of error probability with expected coding constraint length. These strategies essentially use\\nthe feedback to coordinate postponement of the final decision when the noise would otherwise cause errors. Thus\\nsmall error probabilities can be achieved through the use of occasional long delays, while keeping the expected\\ndelay small.\\nFor DAMGN channels an additional mechanism for using feedback exists whereby the transmitter can transmit\\nunusually large amplitude signals when it observes that the receiver is in danger of making a decoding error. The\\npower (i.e., the expected squared amplitude) can be kept small because these large amplitude signals are rarely\\nrequired. In 1966, Schalkwijk and Kailath [13] used this mechanism in a fixed-length block-coding scheme for\\ninfinite bandwidth Gaussian noise channels with ideal feedback. They demonstrated the surprising result that the\\nresulting probability of decoding error decreases as a second order exponential1 in the code constraint length at all\\ntransmission rates less than capacity. Schalkwijk [12] extended this result to the finite bandwidth case, i.e., DAMGN\\nchannels. Later, Kramer [8] (for the infinite bandwidth case) and Zigangirov [15] (for the finite bandwidth case)\\nshowed that the above doubly exponential bounds could be replaced by kth order exponential bounds for any k > 2\\nin the limit of arbitrarily large block lengths. Later encoding schemes inspired by the Schalkwijk and Kailath\\napproach have been developed for multi-user communication with DAMGN [16], [17], [18], [19], [20], secure\\ncommunication with DAMGN [21] and point to point communication for Gaussian noise channels with memory\\n[22].\\nThe purpose of this paper is three-fold. First, the existing results for DAMGN channels with ideal feedback are\\nmade more transparent by expressing them in terms of a 1956 paper by Elias on transmitting a single signal from\\na Gaussian source via multiple uses of a DAMGN channel with feedback. Second, using an approach similar to\\nthat of Zigangirov in [15], we strengthen the results of [8] and [15], showing that error probability can be made\\nto decrease with blocklength n at least with an exponential order an − b for given coefficients a > 0 and b > 0.\\n1\\n\\nFor integer k ≥ 1, the kth order exponent function gk (x) is defined as gk (x) = exp(exp(· · · (exp(x)) · · · )) with k repetitions of exp. A\\nfunction f (x) ≥ 0 is said to decrease as a kth order exponential if for some constant A > 0 and all sufficiently large x, f (x) ≤ 1/gk (Ax).\\n\\n\\x0c2\\n\\nZ1 , . . . , Zn\\n✲\\n\\nU1\\n\\nXi = f (U1 , Y i−1\\n1 )\\n\\nX1 , . . . , Xn\\n\\nY , . . . , Yn\\n✐ 1\\n✲ ❄\\n\\n✻\\nFig. 1.\\n\\n✲ Decoder\\n\\n✲ Û\\n1\\n\\n❄\\n\\nThe setup for n channel uses per source use with ideal feedback.\\n\\nThird, a lower bound is derived. This lower bound decreases with an exponential order in n equal to an + b′ (n)\\nwhere a is the same as in the upper bound and b′ (n) is a sublinear function 2 of the block length n.\\nNeither this paper nor the earlier results in [12], [13], [8], and [15] are intended to be practical. Indeed, these\\nsecond and higher order exponents require unbounded amplitudes (see [10], [2], [9]). Also Kim et al [7] have\\nrecently shown that if the feedback is ideal except for additive Gaussian noise, then the error probability decreases\\nonly as a single exponential in block length, although the exponent increases with increasing signal-to-noise ratio\\nin the feedback channel. Thus our purpose here is simply to provide increased understanding of the ideal conditions\\nassumed.\\nWe first review the Elias result [4] and use it to get an almost trivial derivation of the Schalkwijk and Kailath\\nresults. The derivation yields an exact expression for error probability, optimized over a class of algorithms including\\nthose in [12], [13]. The linear processing inherent in that class of algorithms is relaxed to obtain error probabilities\\nthat decrease with block length n at a rate much faster than an exponential order of 2. Finally a lower bound to\\nthe probability of decoding error is derived. This lower bound is first derived for the case of two codewords and\\nis then generalized to arbitrary rates less than capacity.\\nII. T HE\\n\\nFEEDBACK CHANNEL AND THE\\n\\nE LIAS\\n\\nRESULT\\n\\nXn1\\n\\nLet X1 , . . . , Xn =\\nrepresent n > 1 successive inputs to a discrete-time additive memoryless Gaussian noise\\n(DAMGN) channel with ideal feedback. That is, the channel outputs Y1 , . . . , Yn = Y1n satisfy Y1n = Xn1 + Zn1\\nwhere Zn1 is an n-tuple of statistically independent Gaussian random variables, each with zero mean and variance\\nσZ2 , denoted N (0, σZ2 ). The channel inputs are constrained to some given average power constraint S in the sense\\nthat the inputs must satisfy the second-moment constraint\\nn\\n\\n1X\\nSi ≤ S\\nn\\n\\nwhere Si = E[Xi2 ].\\n\\n(1)\\n\\ni=1\\n\\nWithout loss of generality, we take σZ2 = 1. Thus S is both a power constraint and a signal-to-noise ratio constraint.\\nA discrete-time channel is said to have ideal feedback if each output Yi , 1 ≤ i ≤ n, is made known to the\\ntransmitter in time to generate input Xi+1 (see Figure 1). Let U1 be the random source symbol to be communicated\\nvia this n-tuple of channel uses. Then each channel input Xi is some function f (U1 , Y1i−1 ) of the source and\\nprevious outputs. Assume (as usual) that U1 is statistically independent of Zn1 .\\nElias [4] was interested in the situation where U1 ∼ N (0, σ12 ) is a Gaussian random variable rather than a\\ndiscrete message. For n = 1, the rate-distortion bound (with a mean-square distortion measure) is achieved without\\ncoding or feedback. For n > 1, attempts to map U1 into an n dimensional channel input in the absence of feedback\\ninvolve non-linear or twisted modulation techniques that are ugly at best. Using the ideal feedback, however, Elias\\nconstructed a simple and elegant procedure for using the n channel symbols to send U1 in such a way as to meet\\nthe rate-distortion bound with equality.\\nLet Si = E[Xi2 ] be an arbitrary choice of energy, i.e., second moment, for each i, 1 ≤ i ≤ n. It will be shown\\nshortly that the optimal choice for S1 , . . . , Sn , subject to (1), is Si = S for 1 ≤ i ≤ n. Elias’s strategy starts\\nby choosing the first transmitted signal X1 to be a linear scaling of the source variable U1 , scaled to meet the\\nsecond-moment constraint, i.e.,\\n√\\nX1 = Sσ11U1 .\\n2\\n\\ni.e. lim\\n\\nn→∞\\n\\nb′ (n)\\nn\\n\\n=0\\n\\n\\x0c3\\nS1 Y 1\\nAt the receiver, the minimum mean-square error (MMSE) estimate of X1 is E[X1 |Y1 ] = 1+S\\n, and the error in\\n1\\nS1\\nestimate is N (0, 1+S1 ). It is more convenient to keep track of the MMSE estimate of U1 and the error U2 in\\n√\\nestimate. Since U1 and X1 are the same except for the scale factor σ1 / S1 , these are given by\\n√\\nσ1 S 1 Y 1\\nE[U1 |Y1 ] =\\n1 + S1\\nU2 = U1 − E[U1 |Y1 ]\\n\\nthat\\nthat\\n\\n(2)\\n(3)\\n\\n2\\n\\nσ1\\nwhere U2 ∼ N (0, σ22 ) and σ22 = 1+S\\n.\\n1\\nUsing the feedback, the transmitter can calculate the error term U2 at time 2. Elias’s strategy is to use U2 as\\nthe source signal (without a second-moment constraint) for the second transmission. This unconstrained signal U2\\nis then linearly scaled to meet the second moment constraint S2 for the second transmission. Thus the second\\ntransmitted signal X2 is given by\\n√\\nX2 = Sσ22U2 .\\n\\nWe use this notational device throughout, referring to the unconstrained source signal to be sent at time i by Ui\\nand to the linear scaling of Ui , scaled to meet the second moment\\nconstraint Si , as Xi .\\n√\\nσ2 S2 Y2\\nThe receiver calculates the MMSE estimate E[U2 |Y2 ] = 1+S2 and the transmitter then calculates the error in\\nthis estimate, U3 = U2 − E[U2 |Y2 ]. Note that\\nU1 = U2 + E[U1 |Y1 ]\\n\\n= U3 + E[U2 |Y2 ] + E[U1 |Y1 ].\\n\\nThus U3 can be viewed as the error arising from estimating U1 by E[U1 |Y1 ] + E[U2 |Y2 ]. The receiver continues\\nto update its estimate of U1 on subsequent channel uses, and the transmitter continues to transmit linearly scaled\\nversions of the current estimation error. Then the general expressions are as follows:\\n√\\nS i Ui\\nXi =\\n;\\n(4)\\nσi\\n√\\nσi S i Y i\\n;\\n(5)\\nE[Ui |Yi ] =\\n1 + Si\\nUi+1 = Ui − E[Ui |Yi ].\\n(6)\\n2\\n\\nσi\\n2 ) and σ 2\\nwhere Ui+1 ∼ N (0, σi+1\\ni+1 = 1+Si .\\nIterating on equation (6) from i = 1 to n yields\\n\\nUn+1 = U1 −\\n2\\nSimilarly, iterating on σi+1\\n= σi2 /(1 + Si ), we get\\n\\nn\\nX\\ni=1\\n\\nE[Ui |Yi ].\\n\\n(7)\\n\\nσ12\\n.\\n(8)\\ni=1 (1 + Si )\\nP\\n2\\nThis says that the error arising from estimating U1 by ni=1 E[UP\\ni |Yi ] is N (0, σn+1 ). This is valid for any (nonn\\nnegative) choice of S1 , . . . , Sn , and this is minimized, subject to i=1 Si = nS , by Si = S for 1 ≤ i ≤ n. With\\nthis optimal assignment, the mean square estimation error in U1 after n channel uses is\\n2\\nσn+1\\n= Qn\\n\\n2\\nσn+1\\n=\\n\\nσ12\\n.\\n(1 + S)n\\n\\n(9)\\n\\nWe now show that this is the minimum mean-square error over all ways of using the channel. The rate-distortion\\nfunction for this Gaussian source with a squared-difference distortion measure is well known to be\\nR(d) =\\n\\n1 σ12\\nln\\n2\\nd\\n\\n\\x0c4\\n\\nThis is the minimum mutual information, over all channels, required to achieve a mean-square error (distortion)\\nequal to d. For d = σ12 /(1 + S)n , R(d) is n2 ln(1 + S), which is the capacity of this channel over n uses (it\\nwas shown by Shannon [14] that feedback does not increase the capacity of memoryless channels). Thus the\\nElias scheme actually meets the rate-distortion bound with equality, and no other coding system, no matter how\\ncomplex, can achieve a smaller mean-square error. Note that (9) is also valid in the degenerate case n = 1. What\\nis surprising about this result is not so much that it meets the rate-distortion bound, but rather that the mean-square\\nestimation error goes down geometrically with n. It is this property that leads directly to the doubly exponential\\nerror probability of the Schalkwijk-Kailath scheme.\\nIII. T HE S CHALKWIJK -K AILATH\\n\\nSCHEME\\n\\nThe Schalkwijk and Kailath (SK) scheme will now be defined in terms of the Elias scheme,3 still assuming the\\ndiscrete-time channel model of Figure 1 and the power constraint of (1). The source is a set of M equiprobable\\nsymbols, denoted by {1, 2, . . . , M }. The channel uses will now be numbered from 0 to n − 1, since the use at\\ntime 0 will be quite distinct from the others. The source signal, U0 is a standard M -PAM modulation of the source\\nsymbol. That is, for each symbol m, 1 ≤ m ≤ M , from the source alphabet, m is mapped into the signal am where\\nam = m − (M +1)/2. Thus the M signals in U0 are symmetric around 0 with unit spacing. Assuming equiprobable\\nsymbols, the second moment σ02 of U0 is (M 2 − 1)/12. The initial channel input X0 is a linear scaling of U√\\n0 , scaled\\nto have an energy S0 to be determined later. Thus X0 is an M -PAM encoding, with signal separation d0 = S0 /σ0 .\\ns\\ns\\nS0\\nS0\\nX0 = U0\\n= U0\\n.\\n(10)\\n2\\n2 − 1)\\n12(M\\nσ0\\nThe received signal Y0 = X0 + Z0 is fed back to the transmitter, which, knowing X0 , determines Z0 . In the\\nfollowing n − 1 channel uses, the Elias scheme is used to send the Gaussian random variable Z0 to the receiver,\\nthus reducing the effect of the noise on the original transmission. After the n − 1 transmissions to convey Z0 , the\\nreceiver combines its estimate of Z0 with Y0 to get an estimate of X0 , from which the M -ary signal is detected.\\nSpecifically, the transmitted and received signals for times 1 ≤ i ≤ n − 1 are given by equations (4), (5) and\\n(6). √At time 1, the unconstrained signal U1 is Z0 and σ12 = E[U12 ] = 1. Thus the transmitted signal X1 is given\\nby S1 U1 , where the second moment S1 is to be selected later. We choose Si = S1 for 1 ≤ i ≤ n − 1 for\\noptimized use of the Elias scheme, and thus the power constraint in (1) becomes S0 + (n − 1)S1 = nS . At the end\\nof transmission n − 1, the receiver’s estimate of Z0 from Y1 , . . . , Yn−1 is given by (7) as\\nE[Z0 |\\n\\nY1n−1 ]\\n\\n=\\n\\nn−1\\nX\\ni=1\\n\\nE[Ui | Yi ].\\n\\nThe error in this estimate, Un = Z0 − E[Z0 | Y1n−1 ], is a zero-mean Gaussian random variable with variance σn2 ,\\nwhere σn2 is given by (9) to be\\n1\\nσn2 =\\n.\\n(11)\\n(1 + S1 )n−1\\nSince Y0 = X0 + Z0 and Z0 = E[Z0 | Y 1n−1 ] + Un we have\\n\\nY0 − E[Z0 | Y1n−1 ] = X0 + Un\\n\\n(12)\\n\\nwhere Un ∼ N (0, σn2 ).\\nNote that Un ∼ N (0, σn2 ) is a function of the noise vector Z0n−1 and is thus statistically independent4 of X0 .\\nThus, detecting X0 from Y0 − E[Z0 | Y1n−1] (which is known at the receiver.) is the simplest of classical detection\\nproblems, namely that of detecting an M -PAM signal X0 from the signal plus an independent Gaussian noise\\nvariable Un . Using maximum likelihood detection, an error occurs only if Un exceeds half the distance between\\n3\\n\\nThe analysis here is tutorial and was carried out in slightly simplified form in [5, p481]. A very readable further simplified analysis is\\nin [23].\\n4\\nFurthermore, for the given feedback strategy, Gaussian estimation theory can be used to show, first, that Un is independent of E[Z0 |\\nn−1\\nY1 ], and, second, that Ỹ = Y0 − E[Z0 | Y1n−1 ] is a sufficient statistic for X0 based on Y n−1\\n, (i.e. Pr[X0 | Y0n−1 ] = Pr[X0 | Ỹ ]). Thus\\n0\\nthis detection strategy is not as ad hoc as it might initially seem.\\n\\n\\x0c5\\n\\nsignal points, i.e., if |Un | ≥\\nis given by5\\n\\n√\\n1 S0\\n2 σ0\\n\\n=\\n\\n1\\n2\\n\\nq\\n\\n12S0\\nM 2 −1 .\\n\\nSince the variance of Un is (1 + S1 )−n+1 , the probability of error\\n\\nPe = 2\\n\\nwhere γn =\\n\\n1\\n2\\n\\nq\\n\\n12S0 (1+S1 )n−1\\nM 2 −1\\n\\n(M −1)\\nQ(γn )\\nM\\n\\n(13)\\n\\nand Q(x) is the complementary distribution function of N (0, 1), i.e.,\\nZ ∞\\n−z 2\\n1\\n) dz.\\nexp(\\nQ(x) = √\\n2\\n2π x\\n\\n(14)\\n\\nChoosing S0 and S1 , subject S0 +(n−1)S1 = nS , to maximize γn (and thus minimize Pe ), we get S1 = max{0, S −\\n1\\nn }. That is, if nS is less than 1, all the energy is used to send X0 and the feedback is unused. We assume nS > 1\\nin what follows, since for any given S > 0 this holds for large enough n. In this case, S0 is one unit larger than\\nS1 , leading to\\n1\\nS1 = S − ;\\nS0 = S1 + 1.\\n(15)\\nn\\nSubstituting (15) into (13),\\n(M −1)\\nPe = 2\\nQ(γn )\\n(16)\\nM\\nq\\n3(1+S− n1 )n\\n.\\nwhere γn =\\nM 2 −1\\nThis is an exact expression for error probability, optimized over energy distribution, and using M -PAM followed\\nby the Elias scheme and ML detection. It can be simplified as an upper bound by replacing the coefficient MM−1 by\\n1. Also, since Q(·) is a decreasing function of its argument, Pe can be further upper bounded by replacing M 2 − 1\\nby M 2 . Thus,\\nPe ≤ 2Q(γn )\\n(17)\\n\\x11\\n\\x10\\nn/2 (1+S)n/2\\n√\\n1\\n.\\nwhere γn ≥ 3 1 − (1+S)n\\nM\\nFor large M , which is the case of interest, the above bound is very tight and is essentially an equality, as first\\nderived by Schalkwijk6 in Eq. 12 of [12]. Recalling that nS ≥ 1 we can further lower bound γn (thus upper\\nbounding Pe ). Substituting C(S) = 21 ln(1 + S) and M = exp(nR) we get\\n\\x14\\n\\x15\\n√\\n1 n/2\\n3(1 −\\nγn ≥\\n)\\nexp(n(C(S) − R))\\n(18)\\n1+n\\nThe term in brackets is decreasing in n. Thus,\\n(1 −\\n\\n1 n/2\\n1+n )\\n\\n≥ lim (1 −\\nk→∞\\n−1/2\\n\\n1 k/2\\n1+k )\\n\\n≥e\\n\\nUsing this together with equations (17) and (18) we get,\\n\\x12q\\n\\x13\\n3\\nPe ≤ 2Q\\ne exp(n(C(S) − R)) ,\\n\\n(19)\\n∀n ≥ 1\\n\\n(20)\\n\\n(21)\\n\\nor more simply yet,\\n\\nPe ≤ 2Q(exp[n(C(S) − R)]).\\n\\n(22)\\n\\nNote that for R < C(S), Pe decreases as a second order exponential in n.\\nIn summary, then, we see that the use of standard M -PAM at time 0, followed by the Elias algorithm over the\\nnext n − 1 transmissions, followed by ML detection, gives rise to a probability of error Pe that decreases as a\\nsecond-order exponential for all R < C(S). Also Pe satisfies (21) and (22) for all n ≥ 1/S .\\n5\\nThe term(M −1)/M in (13) arises because the largest and smallest signals each have only one nearest neighbor, whereas all other signals\\nhave two nearest neighbors.\\n6\\nSchalkwijk’s work was independent of Elias’s. He interpreted the steps in the algorithm as successive improvements in estimating X0\\nrather than as estimating Z0 .\\n\\n\\x0c6\\n\\nAlthough Pe decreases as a second-order exponential with this algorithm, the algorithm does not minimize Pe\\nover all algorithms using ideal feedback. The use of standard M -PAM at time 0 could be replaced by PAM with\\nnon-equal spacing of the signal points for a modest reduction in Pe . Also, as shown in the next section, allowing\\ntransmissions 1 to n − 1 to make use of the discrete nature of X0 allows for a major reduction in Pe .7\\nThe algorithm above, however, does have the property that it is optimal among schemes in which, first, standard\\nPAM is used at time 0 and, second, for each i, 1 ≤ i ≤ n − 1, Xi is a linear function of Z0 and Y1i−1 . The reason\\nfor this is that Z0 and Y1n−1 are then jointly Gaussian and the Elias scheme minimizes the mean square error in\\nZ0 and thus also minimizes Pe .\\nA. Broadband Analysis:\\nTranslating these results to a continuous time formulation where the channel is used 2W times per second,8 the\\ncapacity (in nats per second) is CW = 2W C . Letting T = n/2W and letting RW = 2W R be the rate in nats per\\nsecond, this formula becomes\\nPe ≤ 2 Q (exp [(CW − RW )T ]) .\\n(23)\\nLet P = 2W S be the continuous-time power constraint, so that CW = W ln(1 + P/2W ). In the broadband limit\\nas W → ∞ for fixed P , CW → P/2. Since (23) applies for all W > 0, we can simply go to the broadband limit,\\nC∞ = P/2. Since the algorithm is basically a discrete time algorithm, however, it makes more sense to view the\\ninfinite bandwidth limit as a limit in which the number of available degrees of freedom n increases faster than\\nlinearly with the constraint time T . In this case, the signal-to-noise ratio per degree of freedom, S = PT /n goes\\nto 0 with increasing T . Rewriting γn in (17) for this case,\\n\\x15\\n\\x14\\n√\\nPT\\n1\\nn\\nγn ≥\\n(24)\\nln(1 +\\n− ) − T R∞\\n3 exp\\n2\\nn\\nn\\n\\x15\\n\\x14\\n√\\n1 P 2T 2\\nPT\\n3 exp\\n≥\\n− −\\n− T R∞ ,\\n(25)\\n2\\n2\\n4n\\nwhere the inequality ln(1 + x) ≥ x − x2 /2 was used. Note that if n increases quadratically with T , then the term\\nP 2T 2\\n4n is simply a constant which becomes negligible as the coefficient on the quadratic becomes large. For example,\\nif n ≥ 6P 2 T 2 , then this term is at most 1/24 and (25) simplifies to\\nγn ≥ exp [T (C∞ − R∞ )]\\n\\nfor\\n\\nn ≥ 6P 2 T 2 .\\n\\n(26)\\n\\nThis is essentially the same as the broadband SK result (see the final equation in [13]). The result in [13] used\\nn = e2T CW degrees of freedom, but chose the subsequent energy levels to be decreasing harmonically, thus slightly\\nweakening the coefficient of the result. The broadband result is quite insensitive to the energy levels used for each\\ndegree of freedom9 , so long as S0 is close to 1 and the other Si are close to 0. This partly explains why the\\nharmonic choice of energy levels in [13] comes reasonably close to the optimum result.\\n7\\nIndeed, Zigangirov [15] developed an algorithm quite similar to that developed in the next section. The initial phase of that algorithm is\\nvery similar to the algorithm [12] just described, with the following differences. Instead of starting with standard M -PAM, [15] starts with\\na random ensemble of non-equally-spaced M -PAM codes ingeniously arranged to form a Gaussian random variable. The Elias scheme is\\nthen used, starting with this Gaussian random variable. Thus the algorithm in [15] has different constraints than those above. It turns out to\\nhave an insignificantly larger Pe (over this phase) than the algorithm here for S greater than [(1/ ln π6 ) − 1] and an insignificantly smaller\\nPe otherwise.\\n8\\nThis is usually referred to as a channel bandlimited to W . This is a harmless and universally used abuse of the word bandwidth for\\nchannels without feedback, and refers to the ability to satisfy the Nyquist criterion with arbitrarily little power sent out of band. It is more\\nproblematic with feedback, since it assumes that the sum of the propagation delay, the duration of the transmit pulse, the duration of the\\nmatched filter at the receiver, and the corresponding quantities for the feedback, is at most 1/2W . Even allowing for a small fraction of\\nout-of-band energy, this requires considerably more than bandwidth W .\\nP\\n9\\nTo see this, replace (1 + S1 )(n−1)/2 in (13) by 12 exp[ i ln(1 + Si )], each term of which can be lower bounded by the inequality\\nln(1 + x) ≥ x − x2 /2.\\n\\n\\x0c7\\n\\n✻\\n\\n✻\\n\\n✛\\n\\nam−1 d0\\n\\nf (Y0 | U0 = am )\\n\\n✻\\n\\n✲\\n✛\\n\\nm̂ = m\\n\\nm̂ = m+1\\n\\n✲\\n\\nam+1 d0\\n\\na m d0\\n\\n√\\nFig. 2.\\nGiven that am is the sample value of the PAM source signal U0 , the sample value of X0 is am d0 where d0 = S0 /σ0 . The\\nfigure illustrates the probability density of Y0 given this conditioning and shows the M -PAM signal points for X0 that are neighbors to the\\nsample value X0 = am d0 . Note that this density is N (am d0 , 1), i.e., it is the density of Z0 , shifted to be centered at am d0 . Detection\\nusing maximum likelihood at this point simply quantizes Y0 to the nearest signal point.\\n\\nIV. A N ALTERNATIVE PAM S CHEME\\n\\nIN THE HIGH SIGNAL - TO - NOISE REGIME\\n\\nIn the previous section, Elias’s scheme was used to allow the receiver to estimate the noise Z0 originally added\\nto the PAM signal at time 0. This gave rise to an equivalent observation, Y0 − E[Z0 | Y1n−1 ] with attenuated noise\\nUn as given in (12). The geometric attenuation of E[Un2 ] with n is the reason why the error probability in the\\nSchalkwijk and Kailath (SK) [13] scheme decreases as a second order exponential in time.\\nIn this section, we explore an alternative strategy that is again based on the use of M -PAM at time 0, but is quite\\ndifferent from the SK strategy at times 1 to n−1. The analysis is restricted to situations in which the signal-to-noise\\nratio (SNR) at time 0 is so large that the distance between successive PAM signal points in X0 is large relative to\\nthe standard deviation of the noise. In this high SNR regime, a simpler and more effective strategy than the Elias\\nscheme suggests itself (see Figure 2). This new strategy is limited to the high SNR regime, but Section V develops\\na two-phase scheme that uses the SK strategy for the first part of the block, and switches to this new strategy when\\nthe SNR is sufficiently large.\\nIn this new strategy for the high SNR regime, the receiver makes a tentative ML decision\\nm̂0 at time 0. As seen\\n√\\nin the figure, that decision is correct unless the noise exceeds half the distance d0 = S0 /σ0 to either the signal\\nvalue on the right or the left of the sample value am of U0 . Each of these two events has probability Q(d0 /2).\\nThe transmitter uses the feedback to calculate m̂0 and chooses the next signal U1 (in the absence of a secondmoment constraint) to be a shifted version of the original M -PAM signal, shifted so that U1 = m̂0 − m where m\\nis the original message symbol being transmitted. In other words, U1 is the integer-valued error in the\\np receiver’s\\ntentative decision am̂0 of U0 . The corresponding transmitted signal X1 is essentially given by X1 = U1 S1 /E[U12 ],\\nwhere S1 is the energy allocated to X1 .\\nWe now give an approximate explanation of why this strategy makes sense and how the subsequent transmissions\\nare chosen. This is followed by a precise analysis. Temporarily ignoring the case where either m = 1 or m = M\\n(i.e., where am has only one neighbor), U1 is 0 with probability 1 − 2Q(d0 /2). The probability that |U1 | is two or\\nmore is essentially negligible, so U1 = ±1 with a probability approximately equal to 2Q(d0 /2). Thus\\n√\\nU1 S1\\n2\\nE[U1 ] ≈ 2Q(d0 /2);\\nX1 ≈ p\\n(27)\\n2Q(d0 /2)\\n√\\nThis means that X1 is not only a shifted version of X0 , but (since d0 = S0 /σ0 ) is also scaled up by a factor\\nthat is exponential in S0 when S0 is sufficiently large. Thus the separation between adjacent signal points in X1 is\\nexponentially increasing with S0 .\\nThis also means that when X1 is transmitted, the situation is roughly the same as that in Figure 2, except that\\nthe distance between signal points is increased by a factor exponential in S0 . Thus a tentative decision at time 1\\nwill have an error probability that decreases as a second order exponential in S0 .\\nRepeating the same procedure at time 2 will then give rise to a third order exponential in S0 , etc. We now turn\\nto a precise analysis and description of the algorithm at times 1 to n − 1.\\nThe following lemma provides an upper bound to the second moment of U1 , which was approximated in (27).\\nLemma 4.1: For any d ≥ 4, let U be a d-quantization of a normal random variable Z ∼ N (0, 1) in the sense\\nthat for each integer ℓ, if Z ∈ (dℓ − d2 , dℓ + d2 ], then U = ℓ. Then E[U 2 ] is upper bounded by\\nE[U 2 ] ≤\\n\\n1.6\\nd\\n\\n2\\n\\nexp[− d8 ]\\n\\n(28)\\n\\n\\x0c8\\n\\nNote from Figure 2 that, aside √\\nfrom a slight exception described below, U1 = m̂0 − m is the same as the\\nd0 -quantization of Z0 where d0 = S0 /σ0 . The slight exception is that m̂0 should always lie between 1 and M .\\nIf Z0 > (M − m + 1/2), then U1 = M − m, whereas the d0 -quantization takes on a larger integer value. There\\nis a similar limit for Z0 < 1 − m − 1/2. This reduces the magnitude of U1 in the above exceptional cases, and\\nthus reduces the second moment. Thus the bound in the lemma also applies to U1 . For simplicity in what follows,\\nwe avoid this complication by assuming that the receiver allows m̂0 to be larger than M or smaller than 1. This\\nincreases both the error probability and the energy over true ML tentative decisions, so the bounds also apply to\\nthe case with true ML tentative decisions.\\nProof: From the definition of U , we see that U = ℓ if Z ∈ (dℓ − d2 , dℓ + d2 ]. Thus, for ℓ ≥ 1,\\nd\\nd\\nPr[U = ℓ] = Q(dℓ − ) − Q(dℓ + )\\n2\\n2\\nFrom symmetry, Pr[U = −ℓ] = Pr[U = ℓ], so the second moment of U is given by\\n\\x14\\n\\x15\\n∞\\nX\\nd\\nd\\n2\\n2\\nE[U ] = 2\\nℓ Q(dℓ − ) − Q(dℓ + )\\n2\\n2\\nℓ=1\\n\\x15\\n\\x14\\n∞\\nX\\nd\\n2\\n2\\n[ℓ − (ℓ − 1) ] Q(dℓ − ) .\\n= 2Q(d/2) + 2\\n2\\nℓ=2\\n\\nUsing the standard upper bound Q(x) ≤\\nthis becomes\\n\\n√1\\n2π x\\n\\n4\\nE[U 2 ] ≤ √\\n2π d\\n\\nexp[−x2 /2] for x > 0, and recognizing that ℓ2 − (ℓ − 1)2 = 2ℓ − 1,\\n\\n(\\n\\nexp[−d2 /8] +\\n\\n∞\\nX\\n\\n)\\n\\nexp[−(2ℓ − 1)2 d2 /8]\\n\\nℓ=2\\n∞\\nX\\n\\n)\\n4\\n2\\n2\\nexp[−4ℓ(ℓ − 1)d /8]\\nexp[−d /8] 1 +\\n=√\\n2π d\\nℓ=2\\n\\x1a\\n\\x1b\\n4\\n1\\n2\\n≤√\\nexp[−d /8]\\n1 − exp(−d2 )\\n2π d\\n1.6\\n2\\nexp[− d8 ]\\n≤\\nfor d ≥ 4.\\nd\\n(\\n\\n(29)\\n\\nWe now define the rest of this new algorithm. We have defined the unconstrained signal U1 at time 1 to be\\nm̂0 − m but have not specified the energy constraint to be used in amplifying U1 to X1 . The analysis is simplified\\nby defining X1 in terms of a specified scaling factor between U1 and X1 . The energy in X1 is determined later by\\nthis scaling. In particular, let\\n\\x12 2\\x13\\n√\\nd0\\nX1 = d1 U1\\nwhere d1 = 8 exp\\n.\\n16\\nThe peculiar expression for d1 above looks less peculiar when expressed as d21 /8 = exp(d20 /8). When Y1 = X1 +Z1\\nis received, we can visualize the situation from Figure 2 again, where now d0 is replaced by d1 . The signal set\\nfor X1 is again a PAM set but it now has signal spacing d1 and is centered on the signal corresponding to the\\ntransmitted source symbol m. The signals are no longer equally likely, but the analysis is simplified if a maximum\\nlikelihood tentative decision m̂1 is again made. We see that m̂1 = m̂0 − Ŷ1 where Ŷ1 is the d1 -quantization of Y1\\n(and where the receiver again allows m̂1 to be an arbitrary integer) . We can now state the algorithm for each time\\ni, 1 ≤ i ≤ n − 1.\\n\\x10 2 \\x11\\n√\\ndi−1\\ndi =\\n8 exp 16\\n(30)\\nXi = di Ui\\n\\n(31)\\n\\nm̂i = m̂i−1 − Ŷi\\n\\n(32)\\n\\nUi+1 = m̂i − m.\\n\\n(33)\\n\\n\\x0c9\\n\\nwhere Ŷi is the di -quantization of Yi .\\nLemma 4.2: For d0 ≥ 4, the algorithm of (30)-(33) satisfies the following for all alphabet sizes M and all\\nmessage symbols m:\\nd2i\\n8\\n\\nE[Xi2 ] ≤\\n∞\\nX\\ni=1\\n\\nd20\\n) ≥ gi (2).\\n8\\n12.8\\n.\\ndi−1\\n\\n= gi (\\n\\nE[Xi2 ] ≤ 5.\\n\\n(34)\\n(35)\\n(36)\\n\\nPr(m̂i 6= m) ≤ 1/gi+1 (2),\\n\\n(37)\\n\\nwhere gi (x) = exp(· · · (exp(x)) · · · ) with i exponentials.\\nProof: From the definition of di in (30),\\nd2\\nd2\\nd2\\nd2i\\n= exp( i−1 ) = exp(exp( i−2 )) = · · · = gi ( 0 )\\n8\\n8\\n8\\n8\\nThis establishes the first part of (34) and the inequality follows since d0 ≥ 4 and gi (x) is increasing in x.\\nNext, since Xi = di Ui , we can use (34) and Lemma 4.1 to see that\\n\\nE[Xi2 ] = d2i E[Ui2 ]\\n\\x12\\n\\x13\\x12\\n\\x13\\nd2i−1\\nd2i−1\\n1.6\\n= 8 exp(\\n)\\nexp(−\\n)\\n8\\ndi−1\\n8\\n12.8\\n≤\\n,\\ndi−1\\nwhere we have canceled the exponential terms, establishing (35).\\nTo establish (36), note that each di is increasing as a function of d0 , and thus each E[Xi2 ] is upper bounded by\\ntaking d0 ≥ 4 to be 4. Then E[X12 ] = 3.2, E[X22 ] = 1.6648, and the other terms can be bounded in a geometric\\nseries with a sum less than 0.12.\\nFinally, to establish (37), note that\\nPr(m̂i 6= m) = Pr(|Ui |2 ≥ 1)\\n(a)\\n\\n≤\\n\\n2\\n≤ E[Ui+1\\n]\\n\\n(b)\\n1.6\\nexp(−d2i /8) ≤ exp(−d2i /8)\\ndi\\n\\n(c)\\n\\n(d)\\n\\n= 1/ exp(gi (d20 /8)) ≤ 1/gi+1 (2),\\n\\nwhere we have used Lemma 4.1 in (a), the fact that di ≥ 4 in (b), and equation (34) in (c) and (d).\\nWe have now shown that, in this high SNR regime, the error probability decreases with time i as an ith order\\nexponent. The constants involved, such as d0 ≥ 4 are somewhat ad hoc, and the details of the derivation are\\nsimilarly ad hoc. What is happening, as stated before, is that by using PAM centered on the receiver’s current\\ntentative decision, one can achieve rapidly expanding signal point separation with small energy. This is the critical\\nidea driving this algorithm, and in essence this idea was used earlier by10 Zigangirov [15]\\nV. A TWO - PHASE\\n\\nSTRATEGY\\n\\nWe now combine the Shalkwijk-Kailath (SK) scheme of Section III and the high SNR scheme of Section IV\\ninto a two phase strategy. The first phase, of block length n1 , uses the SK scheme. At time n1 − 1, the equivalent\\nreceived signal Y0 − E[Z0 | Y1n1 −1 ], (see (12)), is used in an ML decoder to detect the original PAM signal X0 in\\nthe presence of additive Gaussian noise of variance σn2 1 .\\n10\\n\\nHowever unlike the scheme presented above, in Zigangirov’s scheme the total amount of energy needed for transmission is increasing\\nlinearly with time.\\n\\n\\x0c10\\n\\nNote that if we scale the equivalent received signal, Y0 − E[Z0 | Y1n1 −1 ] by a factor of 1/σn1 so as to have an\\nequivalent unit variance additive noise, we see that the distance between adjacent signal points in the normalized\\nPAM is dn1 −1 = 2γn1 where γn1 is given in (13). If n1 is selected to be large enough to satisfy dn1 −1 ≥ 4, then\\nthis detection at time n1 − 1 satisfies the criterion assumed at time 0 of the high SNR algorithm of Section IV. In\\nother words, the SK algorithm not only achieves the error probability calculated in Section III, but also, if the block\\nlength of the SK phase n1 is chosen to be large enough, it creates the initial condition for the high SNR algorithm.\\nThat is, it provides the receiver and the transmitter at time n1 − 1 with the output of a high signal-to-noise ratio\\nPAM. Consequently not only is the tentative ML decision at time n1 − 1 correct with moderately high probability,\\nbut also the probability of the distant neighbors of the decoded messages vanishes rapidly.\\nThe intuition behind this two-phase scheme is that the SK algorithm seems to be quite efficient when the signal\\npoints are so close (relative to the noise) that the discrete nature of the signal is not of great benefit. When the SK\\nscheme is used enough times, however, the signal points becomes far apart relative to the noise, and the discrete\\nnature of the signal becomes important. The increased effective distance between the signal points of the original\\nPAM also makes the high SNR scheme, feasible. Thus the two-phase strategy switches to the high SNR scheme at\\nthis point and the high SNR scheme drives the error probability to 0 as an n2 order exponential.\\nWe now turn to the detailed analysis of this two-phase scheme. Note that 5 units of energy must be reserved for\\nphase 2 of the algorithm, so the power constraint S1 for the first phase of the algorithm is n1 S1 = nS − 5. For\\nany fixed rate R < C(S), we will find that the remaining n2 = n − n1 time units is a linearly increasing function\\nof n and yields an error probability upper bounded by 1/gn2 +1 (2).\\nA. The finite-bandwidth case\\nFor the finite-bandwidth case, we assume an overall block length n = n1 + n2 , an overall power constraint S ,\\nand an overall rate R = (ln M )/n. The overall energy available for phase 1 is at least nS − 5, so the average\\npower in phase 1 is at least (nS − 5)/n1 .\\nWe observed that the distance dn1 −1 between adjacent signal points, assuming that signal and noise are normalized\\nto unit noise variance, is twice the parameter γn1 given in (16). Rewriting (16) for the power constraint (nS −5)/n1 ,\\n\\x12\\n\\x13\\n√\\nnS − 5\\n1 n1 /2\\ndn1 ≥ 2 3 1 +\\n−\\nexp(−nR)\\nn1\\nn1\\n\\x12\\n\\x12\\n\\x13\\n\\x13n1 /2\\n√\\nnS n1 /2\\n6\\nexp(−nR) 1 −\\n=2 3 1+\\nn1\\nnS + n1\\n\\x12\\n\\x13\\n\\x13n1 /2\\n\\x12\\n(a) √\\n1\\nnS n1 /2\\nexp(−nR) 1 −\\n≥2 3 1+\\nn1\\n1 + n1 /6\\n\\x13n1 /2\\n\\x12\\n√\\nSn\\nexp(−nR),\\n(38)\\n≥ 2e33 1 +\\nn1\\nwhere to get (a) we assumed that nS ≥ 6. We can also show that the multiplicative term, (1 −\\ndecreasing function of n1 satisfying\\n\\x12\\n\\x12\\n\\x13n1 /2\\n\\x13n1 /2\\n1\\n1\\n1−\\n≥ lim 1 −\\n= e−3 .\\nn1 →∞\\n1 + n1 /6\\n1 + n1 /6\\n\\n1\\nn1 /2 ,\\n1+n1 /6 )\\n\\nis a\\n\\nThis establishes (38). In order to satisfy dn1 ≥ 4, it suffices for the right-hand side of (38) to be greater than or\\nequal to 4. Letting ν = n1 /n, this condition can be rewritten as\\n\\x13\\x15\\n\\x14 \\x12\\nS\\nν\\n3\\n√ .\\n≥ 2e\\nexp n −R + ln(1 +\\n(39)\\n3\\n2\\nν\\nDefine φ(ν) by\\n\\nν\\nln(1 + S/ν).\\n2\\nThis is a concave increasing function for 0 < ν ≤ 1 and can be interpreted as the capacity of the given channel\\nif the number of available degrees of freedom is reduced from n to νn without changing the available energy per\\nφ(ν) =\\n\\n\\x0c11\\n\\nblock, i.e., it can be interpreted as the capacity of a continuous time channel whose bandwidth has been reduced\\nby a factor of ν . We can then rewrite (39) as\\nφ(ν) ≥ R +\\n\\nβ\\n,\\nn\\n\\n(40)\\n\\n3\\n\\n√ ). This is interpreted in Figure 3.\\nwhere β = ln( 2e\\n3\\n\\nφ(ν)\\n\\nC\\nR + β/n\\nR\\n\\nν\\n0\\n\\nφ−1 (R)\\n\\nνn′\\n\\nνn\\n\\n1\\n\\nThis shows the function φ(ν) and also the value of ν, denoted φ−1 (R), at which φ(ν) = R. It also shows νn′ , which\\nsatisfies φ(νn′ ) = R + β/n, and gives the solution to (40) with equality. It turns out to be more convenient to satisfy (40) with\\n−1\\n(R))\\ninequality using νn , which by simple geometry satisfies νn = φ−1 (R) + β(1−φ\\nn(C−R) .\\nFig. 3.\\n\\nThe condition dn1 ≥ 4 is satisfied by choosing n1 = ⌈nνn ⌉ for νn defined in Figure 3, i.e.,\\n\\x18\\n\\x19\\nβ(1 − φ−1 (R))\\nn1 = nφ−1 (R) +\\nC−R\\n\\nThus the duration n2 of phase 2 can be chosen to be\\n\\x17\\n\\x16\\nβ(1 − φ−1 (R))\\n−1\\n.\\nn2 = n[1 − φ (R)] −\\nC −R\\n\\n(41)\\n\\nThis shows that n2 increases linearly with n at rate 1 − φ−1 (R) for n > β/(C − R). As a result of lemma 4.2\\nthe error probability is upper bounded as\\nPr(m̂ 6= m) ≤ 1/gn2 +1 (2),\\n\\n(42)\\n\\nThus the probability of error is bounded by an exponential order that increases at a rate 1 − φ−1 (R). We later\\nderive a lower bound to error probability which has this same rate of increase for the exponential order of error\\nprobability.\\nB. The broadband case - zero error probability\\nThe broadband case is somewhat simpler since an unlimited number of degrees of freedom are available. For\\nphase 1, we start with equation (24), modified by the fact that 5 units of energy must be reserved for phase 2.\\n\\x14\\n\\x15\\n√\\nn1\\nPT\\n6\\ndn1 ≥ 2 3 exp\\nln(1 +\\n− ) − T R∞\\n2\\nn1\\nn1\\n\\x14\\n\\x15\\n2\\n2\\n√\\nPT\\nP T\\n≥ 2 3 exp\\n−3−\\n− T R∞ ,\\n2\\n4n1\\nwhere, in order to get the inequality in the second step, we assumed that PT ≥ 6 and used the identity ln(1 + x) ≥\\nx − x2 /2. As in the broadband SK analysis, we assume that n1 is increasing quadratically with increasing T . Then\\nP 2T 2\\nP 2T 2\\nwe get,\\n4n1 becomes just a constant. Specifically if n1 ≥\\n4\\ndn1 ≥\\n\\nIt follows that dn1 ≥ 4 if\\n\\n√\\n2 3\\ne4\\n\\nT ≥\\n\\nexp [T (C∞ − R∞ )] ,\\n4+ln 2−0.5 ln 3\\n.\\nC∞ −R∞\\n\\n(43)\\n\\nIf (43) is satisfied, then phase 2 can be carried out for arbitrarily large n2 , with Pe satisfying (42). In principle, n2\\ncan be infinite, so Pe becomes 0 whenever T is large enough to satisfy(43).\\n\\n\\x0c12\\n\\nOne might object that the transmitter sequence is not well defined with n2 = ∞, but in fact it is, since at most\\na finite number of transmitted symbols can be nonzero. One might also object that it is impossible to obtain an\\ninfinite number of ideal feedback signals in finite time. This objection is certainly valid, but the entire idea of ideal\\nfeedback with infinite bandwidth is unrealistic. Perhaps a more comfortable way to express this result is that 0 is\\nthe greatest lower bound to error probability when (43) is satisfied, i.e., any desired error probability, no matter\\nhow small is achievable if the continuous-time block length T satisfies (43).\\nVI. A LOWER\\n\\nBOUND TO ERROR PROBABILITY\\n\\nThe previous sections have derived upper bounds to the probability of decoding error for data transmission using\\nparticular block coding schemes with ideal feedback. These schemes are non-optimal, with the non-optimalities\\nchosen both for analytical convenience and for algorithmic simplicity. It appears that the optimal strategy is quite\\ncomplicated and probably not very interesting. For example, even with a block length n = 1, and a message set size\\nM = 4, PAM with equi-spaced messages is neither optimal in the sense of minimizing average error probability\\nover the message set (see Exercise 6.3 of [6]) nor in the sense of minimizing the error probability of the worst\\nmessage. Aside from this rather unimportant non-optimality, the SK scheme is also non-optimal in ignoring the\\ndiscrete nature of the signal until the final decision. Finally, the improved algorithm of Section V is non-optimal\\nboth in using ML rather than maximum a posteriori probability (MAP) for the tentative decisions and in not\\noptimizing the choice of signal points as a function of the prior received signals.\\nThe most important open question, in light of the extraordinarily rapid decrease of error probability with block\\nlength for the finite bandwidth case, is whether any strictly positive lower bound to error probability exists for fixed\\nblock length n. To demonstrate that there is such a positive lower bound we first derive a lower bound to error\\nprobability for the special case of a message set of size M = 2. Then we generalize this to codes of arbitrary rate\\nand show that for R < C , the lower bound decreases as a kth order exponential where k increases with the block\\nlength n and has the form k = an − b′ where the coefficient a is the same as that in the upper bound in Section\\nV. It is more convenient in this section to number the successive signals from 1 to n rather than 0 to n − 1 as in\\nprevious sections.\\nA. A lower bound for M = 2\\nAlthough it is difficult to find and evaluate the entire optimal code, even for M = 2, it turns out to be easy to find\\nthe optimal encoding in the last step. Thus, for each Y 1n−1 , we want to find the optimal choice of Xn = f (U, Y 1n−1 )\\nas a function of, first, the encoding functions Xi = f (U, Y 1i−1 ), 1 ≤ i ≤ n − 1, and, second, the allocation of\\nenergy, S̃ = E[Xn2 |Y 1n−1 ] for that Y 1n−1 . We will evaluate the error probability for such an optimal encoding at time\\nn and then relate it to the error probability that would have resulted from decoding at time n − 1. We will use this\\nrelation to develop a recursive lower bound to error probability at each time i in terms of that at time i − 1.\\ni\\n11\\nFor a given code function Xi = f (U, Y i−1\\n1 ) for 1 ≤ i ≤ n − 1, the conditional probability density of Y 1 given\\nU = 1 or 2 is positive for all sample values for Y i1 ; thus the corresponding conditional probabilities of hypotheses\\nU = 1 and U = 2 are positive i.e.\\nPr(U =m|Y i1 ) > 0\\n\\nm ∈ {1, 2}, ∀Y i1 ∈ Ri .\\n\\nIn particular, for m ∈ {1, 2}, define Φm = Pr(U =m|Y 1n−1 ) for some given Y 1n−1 . Finding the error probability\\nΨ = Pr(Û (Y n1 ) 6= U | Y 1n−1 ) is an elementary binary detection problem for the given Y 1n−1 . MAP detection, using\\nthe a priori probabilities Φ1 and Φ2 , minimizes the resulting error probability.\\nFor a given sample value of Y 1n−1 , let b1 and b2 be the values of Xn for U = 1 and 2 respectively. Let a be half\\nthe distance between b1 and b2 , i.e., 2a = b2 − b1 . The error probability Ψ depends on b1 and b2 only through a.\\nFor a given S̃ , we choose b1 and b2 to satisfy E[Xn |Y 1n−1 ] = 0, thus maximizing a for the given S̃ . The variance\\nof Xn conditional on Y 1n−1 is given by\\n1X\\nVar(Xn |Y 1n−1 ) =\\nΦi Φj (bi − bj )2 = 4Φ1 Φ2 a2 ,\\n2\\ni,j\\n\\n11\\n\\nWe do not use the value of this density, but for completeness, it can be seen to be\\ndensity (2π)−1/2 exp(−x2 /2).\\n\\nQi\\n\\nj=1\\n\\nξ[Yj − f (U, Y j−1\\n)] where ξ(x) is the normal\\n1\\n\\n\\x0c13\\n\\nand since E[Xn |Y 1n−1 ] = 0, this means that a is related to S̃ by S̃ = 4Φ1 Φ2 a2 .\\nNow let Φ = min{Φ1 , Φ2 }. Note that Φ is the probability of error for a hypothetical MAP decoder detecting U\\nat time n − 1 from Y 1n−1 . The error probability Ψ for the MAP decoder at the end of time n is given by the classic\\nresult of binary MAP detection with a priori probabilities Φ and 1 − Φ,\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\nln η\\nln η\\nΨ = (1 − Φ)Q a +\\n+ ΦQ a −\\n,\\n(44)\\n2a\\n2a\\nR∞\\n−1/2 exp(−z 2 /2) dz . This equation relates the error probability Ψ at the end\\nwhere η = 1−Φ\\nΦ and Q(x) = x (2π)\\nof time n to the error probability Φ at the end of time n − 1, both conditional on Y 1n−1 . We are now going to view\\nΨ and Φ as functions of Y 1n−1 , and thus as random variables. Similarly S̃ ≥ 0 can be any non-negative function\\nof Y 1n−1 , subject to a constraint Sn on its mean; so we can view S̃ as an arbitrary non-negative random variable\\nwith mean Sn . For each Y 1n−1 , S̃ and Φ determine the value of a; thus a is also a non-negative random variable.\\nWe are now going to lower bound the expected value of Ψ in such a way that the result is a function only of\\nthe expected value of Φ and the expected value Sn of S̃ . Note that Ψ in (44) can be lower bounded by ignoring\\nthe first term and replacing the second term with ΦQ(a). Thus,\\nΨ ≥ ΦQ(a)\\n\\uf8ebs\\n\\n\\uf8f6\\nS̃\\n\\uf8f8\\n= ΦQ \\uf8ed\\n4Φ(1 − Φ)\\n\\uf8ebs \\uf8f6\\nS̃ \\uf8f8\\n.\\n≥ ΦQ \\uf8ed\\n2Φ\\n\\n(45)\\n\\nwhere the last step uses the facts that Q(x) is a decreasing function of x and that 1 − Φ > 1/2.\\n\\uf8eb\\n\\n\\uf8ee s\\n\\n\\uf8f9\\uf8f6\\n\\nS̃ \\uf8fb\\uf8f8\\n2Φ\\n\\x12\\nhp i\\x13\\n1\\nΦS̃\\n= E[Φ]Q √\\nE\\n2E[Φ]\\n\\x12\\n\\x13\\nq\\n1\\nE[Φ]E[S̃]\\n≥ E[Φ]Q √\\n2E[Φ]\\ns\\n!\\nSn\\n.\\n= E[Φ]Q\\n2E[Φ]\\n\\nE[Ψ] ≥ E[Φ]Q \\uf8ed\\n\\n1\\nE \\uf8f0Φ\\nE[Φ]\\n\\n(46)\\n\\n(47)\\n(48)\\n\\nIn (46), we used Jensen’s inequality, based on the facts that Q(x) is a convex function for x ≥ 0 and that Φ/E[Φ]\\nis a probability distribution on Y 1n−1 . In (47), we used the Schwarz inequality along with the fact that Q(x) is\\ndecreasing for x ≥ 0.\\nWe now recognize that E[Ψ] is simply the overall error probability at the end of time n and E[Φ] is the overall\\nerror probability (if a MAP decision were made) at the end of time n − 1. Thus we denote these quantities as pn\\nand pn−1 respectively,\\ns\\n!\\nSn\\npn ≥ pn−1 Q\\n.\\n(49)\\n2pn−1\\nNote that this lower bound is monotone increasing in pn−1. Thus we can further lower bound pn by lower\\nbounding\\nppn−1 . We can lower bound pn−1 (for a given pn−2 and Sn−1 ) in exactly the same way, so that pn−1 ≥\\npn−2 Q( Sn−1 /2pn−2 ). These two bounds can be combined to implicitly bound pn in terms of pn−2 , Sn and Sn−1 .\\nIn fact, the same technique can be used for each i, 1 ≤ i ≤ n, getting\\ns\\n!\\nSi\\npi ≥ pi−1 Q\\n.\\n(50)\\n2pi−1\\n\\n\\x0c14\\n\\nThis gives us a recursive lower bound on pn for any given choice of S1 , . . . , Sn subject to the power constraint\\nP\\ni Si ≤ nS .\\nWe have been unable to find a clean way to optimize this over the choice of S1 , . . . , Sn , so as a very crude\\nlower bound on pn , we upper bound each Si by nS . For convenience, multiply each side of (50) by 2/nS ,\\n2pi\\n2pi−1 \\x10q nS \\x11\\nfor 1 ≤ i ≤ n.\\n(51)\\n≥\\nQ\\n2pi−1 ;\\nnS\\nnS\\n\\x10q \\x11\\nnS\\nnS\\nAt this point, we can see what is happening in this lower bound. As pi approaches 0, 2p\\n→\\n∞\\n.\\nAlso\\nQ\\n2pi\\ni\\n− nS\\n\\napproaches 0 as e 4pi . Now we will lower bound the expression on the right hand side of (51). We can check\\nnumerically12 that for x ≥ 9,\\n1 √\\n(52)\\nQ( x) ≥ exp(−x).\\nx\\n√\\nFurthermore x1 Q( x) is decreasing in x for all x > 0, and thus\\n1 √\\nQ( x) ≥ exp(− max{x, 9})\\nx\\n\\n∀x > 0.\\n\\nSubstituting this into (51) we get,\\n2pi\\n1\\n≥\\n;\\nnS\\n, 9})\\nexp(max{ 2pnS\\ni−1\\n\\nfor 1 ≤ i ≤ n.\\n\\nApplying this recursively for i = n down to i = k + 1 for any k ≥ 0 we get,\\n\\n1\\n2pn\\n≥\\nnS\\n, 9}), 9})\\nexp(max{exp(max{ 2pnS\\nn−2\\n1\\n(a)\\n=\\nexp(exp(max{ 2pnS\\n, 9}))\\nn−2\\n1\\noi .\\nh\\nn\\n≥\\nnS\\n,\\n9\\ngn−k max 2p\\nk\\n\\n(53)\\n\\nwhere (a) simply follows from the fact that exp(9) > 9. This bound holds for k = 0, giving an overall lower bound\\non error probability in terms of p0 . In the usual case where the symbols are initially equiprobable, p0 = 1/2 and\\npn ≥\\n\\nnS\\n.\\n2gn [max(nS, 9)]\\n\\n(54)\\n\\nNote that this lower bound is an nth order exponential. Although it is numerically much smaller than the upper\\nbound in Section V, it has the same general form. The intuitive interpretation is also similar. In going from block\\nlength n − 1 to n, with very small error probability\\nq at n − 1, the symbol of large a priori probability is very close\\nto 0 and the other symbol is approximately at S̃/pn−1 . Thus the error probability is decreased in one time unit\\nby an exponential in pn−1 , leading to an nth order exponential over n time units.\\nB. Lower bound for arbitrary M\\nNext consider feedback codes of arbitrary rate R < C with sufficiently large blocklength n and M = enR\\ncodewords. We derive a lower bound on error probability by splitting n into an initial segment of length n1 and\\na final segment of length n2 = n − n1 . This segmentation is for bounding purposes only and does not restrict the\\nfeedback code. The error probability of a hypothetical MAP decoder at the end of the first segment, Pe (n1 ), can\\nbe lower bounded by a conventional use of the Fano inequality. We will show how to use this error probability as\\nthe input of the lower bound for M = 2 case derived in the previous subsection, i.e., equation (53). There is still\\nthe question of allocating power between the two segments, and since we are deriving a lower bound, we simply\\n12\\n\\nThat is, we can check numerically that (52) is satisfied for x = 9 and verify that the right-hand side is decreasing faster than the left\\nfor x > 9.\\n\\n\\x0c15\\n\\nassume that the entire available energy is available in the first segment, and can be reused in the second segment.\\nWe will find that the resulting lower bound has the same form as the upper bound in Section V.\\nUsing energy Sn over the first segment corresponds to power Sn/n1 , and since feedback does not increase the\\nchannel capacity, the average directed mutual information over the first segment is at most n1 C(Sn/n1 ). Reusing\\nthe definitions ν = n1 /n and φ(ν) = ν2 ln(1 + Sν ) from Section V,\\nn1 C(Sn/n1 ) = nφ(ν).\\n\\nThe entropy of the source is ln M = nR, and thus the conditional entropy of the source given Y n1 1 satisfies\\nn [R − φ(ν)] ≤ H(U |Y n1 1 )\\n\\n≤ h(Pe (n1 )) + Pe (n1 )nR\\n\\n≤ ln 2 + Pe (n1 )nR,\\n\\n(55)\\n\\nwhere we have used the Fano inequality and then bounded the binary entropy h(p) = −p ln p − (1 − p) ln(1 − p)\\nby ln 2.\\nTo use (55) as a lower bound on Pe (n1 ), it is necessary for n1 = nν to be small enough that φ(ν) is substantially\\nless than R, and to be specific we choose ν to satisfy\\nR − φ(ν) ≥\\n\\n1\\n.\\nn\\n\\n(56)\\n\\nWith this restriction, it can be seen from (55) that\\n1 − ln 2\\n.\\n(57)\\nnR\\nFigure 4 illustrates that the following choice of n1 in (58) satisfies both equation (56) and equation (57). This uses\\nthe fact that φ(ν) is a monotonically increasing concave function of ν .\\n\\x16\\n\\x17\\n1 − φ−1 (R)\\n−1\\nn1 = nφ (R) −\\n.\\n(58)\\nC−R\\nPe (n1 ) ≥\\n\\nC\\n\\nφ(ν)\\n\\nR\\nR − 1/n\\nν\\n0\\n\\nνn νn′\\n\\nφ\\n\\n−1\\n\\n(R)\\n\\n1\\n\\nFig. 4. This shows the value of ν, denoted φ−1 (R), at which φ(ν) = R. It also shows νn′ , where φ(νn′ ) = R − 1/n. This gives the\\n−1\\n(R)\\ncan be seen to be less than νn′ and thus also satisfies (56).\\nsolution to (56) with equality, but νn = φ−1 (R) − 1−φ\\nn(C−R)\\n\\nThe corresponding choice for n2 is\\n\\x18\\n\\n−1\\n\\nn2 = n[1 − φ\\n\\n\\x19\\n1 − φ−1 (R)\\n.\\n(R)] +\\nC −R\\n\\n(59)\\n\\nThus with this choice of n1 , n2 , the error probability at the end of time n1 satisfies (57).\\nThe straightforward approach at this point would be to generalize the recursive relationship in (50) to arbitrary\\nM . This recursive relationship could then be used, starting at time i = n and using each successively smaller i\\nuntil terminating the recursion at i = n1 where (57) can be used. It is simpler, however, since we have already\\nderived (50) for M = 2, to define a binary coding scheme from any given M -ary scheme in such a way that the\\nbinary results can be used to lower bound the M -ary results. This technique is similar to one used earlier in [1].\\nLet Xi = f (U, Y i−1\\n1 ) for 1 ≤ i ≤ n be any given coding function for U ∈ M = {1, . . . , M }. That code is used\\nto define a related binary code. In particular, for each received sequence Y n1 1 over the first segment, we partition\\n\\n\\x0c16\\n\\nthe message set M into two subsets, M1 (Y n1 1 ) and M2 (Y n1 1 ). The particular partition for each Y n1 1 is defined later.\\nThis partitioning defines a binary random variable V as follows,\\n\\x1b\\n\\x1a\\n1\\nU ∈ M1 (Y n1 1 )\\nV =\\n2\\nU ∈ M2 (Y n1 1 )\\nAt the end of the transmission, the receiver will use its decoder to decide Û . We define the decoder for V at time\\nn, using the decoder of U as follows,\\n\\x1a\\n\\x1b\\n1\\nÛ ∈ M1 (Y n1 1 )\\nV̂ =\\n2\\nÛ ∈ M2 (Y n1 1 )\\nNote that with the above mentioned definitions, whenever the M -ary scheme decodes correctly, the related binary\\nscheme does also, and thus the error probability Pe (n) for the M -ary scheme must be greater than or equal to the\\nerror probability pn of the related binary scheme.\\nThe binary scheme, however, is one way (perhaps somewhat bizarre) of transmitting a binary symbol, and thus\\nit satisfies the results13 of section VI-A. In particular, for the binary scheme, the error probability pn at time n is\\nlower bounded by the error probability pn1 at time n1 by (53),\\nPe (n) ≥ pn ≥\\n\\nnS\\n2\\n\\n»\\n\\uf6be1\\nﬀ– .\\nnS\\ngn2 max 2p ,9\\nn1\\n\\n(60)\\n\\nOur final task is to relate the error probability pn1 at time n1 for the binary scheme to the error probability Pe (n1 )\\nin (57) for the M -ary scheme. In order to do this, let Φm (Y n1 1 ) be the probability of message m conditional on\\nthe received first segment Y n1 1 . The MAP error probability for an M -ary decision at time n1 , conditional on Y n1 1 ,\\nis 1 − Φmax (Y n1 1 ) where Φmax (Y n1 1 ) = max{Φ1 (Y n1 1 ), . . . ΦM (Y n1 1 )}. Thus Pe (n1 ), given in (57), is the mean of\\n1 − Φmax (Y n1 1 ) over Y n1 1 .\\nNow pn1 is the mean, over Y n1 1 , of the error probability of a hypothetical MAP decoder for V at time n1 conditional\\non Y n1 1 , pn1 (Y n1 1 ). This is the smaller of the a posteriori probabilities of the subsets M1 , M2 conditional on Y n1 1 ,\\ni.e.,\\n\\uf8f1\\n\\uf8fc\\n\\uf8f2 X\\n\\uf8fd\\nX\\npn1 (Y n1 1 ) = min\\nΦm (Y n1 1 )\\nΦm (Y n1 1 ),\\n(61)\\n\\uf8f3\\n\\uf8fe\\nn1\\nn1\\nm∈M1 (Y 1 )\\n\\nm∈M2 (Y 1 )\\n\\nThe following lemma shows that by an appropriate choice of partition for each Y n1 1 , this binary error probability\\nis lower bounded by 1/2 the corresponding M -ary error probability.\\nLemma 6.1: For any probability distribution Φ1 , . . . , ΦM on a message set M with M > 2, let Φmax =\\nmax{Φ1 , . . . , ΦM }. Then there is a partition of M into two subsets, M1 and M2 such that\\nX\\nX\\n1 − Φmax\\n1 − Φmax\\nΦm ≥\\nΦm ≥\\nand\\n.\\n(62)\\n2\\n2\\nm∈M2\\nm∈M1\\nProof:\\nOrder the messages in order of decreasing Φm . Assign the messages one by one in this order to the sets M1\\nand M2 . When assigning the kth most likely message, we calculate the total probability of the messages that\\nhave already been assigned to each set, and assign the kth message to the set which has the smaller probability\\nmass. If the probability mass of the sets are the same we choose one of the sets arbitrarily. With such a procedure,\\nthe difference in the probabilities of the sets, as they evolve, never exceeds Φmax . After all messages have been\\nassigned, let\\nX\\nX\\nΦm .\\nΦm ;\\nΦ′2 =\\nΦ′1 =\\nm∈M2\\n\\nm∈M1\\n\\nWe have seen that\\n\\n|Φ′1\\n\\n−\\n\\nΦ′2 |\\n\\n≤ Φmax . Since\\n\\nΦ′1\\n\\n+\\n\\nΦ′2\\n\\n= 1, (62) follows.\\n\\nThis is not quite as obvious as it sounds. The binary scheme here is not characterized by a coding function f (V, Y i−1\\n1 ) as in Section\\n1\\nVI-A , but rather is a randomized binary scheme. That is, for a given Y n\\n1 and a given choice of V , the subsequent transmitted symbols Xi\\nare functions not only of V and Y i−1\\n1 , but also of a random choice of U conditional on V . The basic conclusion of (50) is then justified by\\naveraging over both Y 1i−1 and the choice of U conditional on V .\\n13\\n\\n\\x0c17\\n\\nSince the error probability for the binary scheme is now at least one half of that for the M -ary scheme for each\\nY n1 1 , we can take the mean over Y n1 1 , getting pn1 ≥ Pe (n1 )/2. Combining this with (60) and (57)\\nPe (n) ≥\\n\\nnS\\n1\\n,\\nn2 SR\\n, 9)]\\n2 gn2 [max( 1−ln\\n2\\n\\n(63)\\n\\nwhere n2 is given in (59). The exact terms in this expression are not particularly interesting because of the very\\nweak bounds on energy at each channel use. What is interesting is that the order of exponent in both the upper\\nbound of (42) and (41) and the lower bound here are increasing linearly14 at the same rate 1 − φ−1 (R).\\nVII. C ONCLUSIONS\\nThe SK data transmission scheme can be viewed as ordinary PAM combined with the Elias scheme for noise\\nreduction. The SK scheme can also be improved by incorporating the PAM structure into the transmission of the\\nerror in the receiver’s estimate of the message, particularly during the latter stages. For the bandlimited version,\\nthis leads to an error probability that decreases with an exponential order an + b where a = 1 − φ−1 (R) and b is\\na constant. In the broadband version, the error probability is zero for sufficiently large finite constraint durations\\nT . A lower bound to error probability, valid for all R < C was derived. This lower bound also decreases with an\\nexponential order an + b′ (n) where again a = 1 − φ−1 (R) and b′ (n) is essentially a constant.15 It is interesting\\nto observe that the strategy yielding the upper bound uses almost all the available energy in the first phase, using\\nat most 5 units of energy in the second phase. The lower bound relaxed the energy constraint, allowing all the\\nallowable energy to be used in the first phase and then to be used repeatedly in each time unit of the second\\nphase. The fact that both bounds decrease with the same exponential order suggests that the energy available for\\nthe second phase is not of primary importance. An open theoretical question is the minimum overall energy under\\nwhich the error probability for two code words can be zero in the infinite bandwidth case.\\nR EFERENCES\\n[1] P. Berlin, B. Nakiboğlu, B. Rimoldi, and E. Telatar. A simple converse of Burnashev’s reliability function. Information Theory, IEEE\\nTransactions on, 55(7):3074–3080, July 2009.\\n[2] M. V. Burnashev. Sequential discrimination of hypotheses with control of observations. Mathematics of the USSR-Izvestiya, 15(3):419–\\n440, 1980.\\n[3] M. V. Burnashev. Data transmission over a discrete channel with feedback and random transmission time. Problemy Peridachi\\nInformatsii, 12(4):10–30, 1976.\\n[4] P. Elias. ‘Channel capacity without coding. Quarterly progress report, MIT Research Laboratory of Electronics, Oct 15 1956. also in\\nLectures on Communication System Theory, E. Baghdady, Ed., New York:McGraw Hill, 1961.\\n[5] R. G. Gallager, Information Theory and Reliable Communication, New York: Wiley, 1968.\\n[6] R. G. Gallager. Principles of Digital Communication. Cambridge Press, New York, 2008.\\n[7] Y-H. Kim, A. Lapidoth, and T. Weissman. The Gaussian channel with noisy feedback. In Information Theory, 2007. ISIT 2007. IEEE\\nInternational Symposium on, pages 1416–1420, June 2007.\\n[8] A. Kramer. Improving communication reliability by use of an intermittent feedback channel. Information Theory, IEEE Transactions\\non, 15(1):52–60, Jan 1969.\\n[9] B. Nakiboğlu and R.G. Gallager. Error exponents for variable-length block codes with feedback and cost constraints. Information\\nTheory, IEEE Transactions on, 54(3):945–963, March 2008.\\n[10] M. S. Pinsker. The probability of error in block transmission in a memoryless Gaussian channel with feedback. Problemy Peridachi\\nInformatsii, 4(4):1–14, 1968.\\n[11] A. Sahai. Why do block length and delay behave differently if feedback is present? Information Theory, IEEE Transactions on,\\n54(5):1860–1886, May 2008.\\n[12] J. Schalkwijk. A coding scheme for additive noise channels with feedback–ii: Band-limited signals. Information Theory, IEEE\\nTransactions on, 12(2):183–189, Apr 1966.\\n[13] J. Schalkwijk and T. Kailath. A coding scheme for additive noise channels with feedback–i: No bandwidth constraint. Information\\nTheory, IEEE Transactions on, 12(2):172–182, Apr 1966.\\n[14] C. E. Shannon. Two-way communication channels. In Proc. Fourth Berkeley Symp. on Math. Statist. and Prob., volume 1, pages\\n611–644, Berkeley CA, 1961. University of California Press.\\n[15] K. Sh. Zigangirov. Upper bounds for the error probability for channels with feedback. Problemy Peredaci Informatsii, 6(2):87–92,\\n1970.\\nNote that the argument of gn2 is proportional to n2 , so that this bound does not quite decrease with the exponential order n2 . It\\ndoes, however, decrease with an exponential order n2 + α(n), where α(n) increases with n much more slowly than, say, ln(ln(n)). Thus\\n(n2 + α(n))/n is asymptotically proportional to 1 − φ−1 (R).\\n′\\n15 ′\\nb (n) is a sublinear function of n, i.e. lim b n(n) = 0.\\n14\\n\\nn→∞\\n\\n\\x0c18\\n\\n[16] L. Ozarow. The capacity of the white Gaussian multiple access channel with feedback. Information Theory, IEEE Transactions on,\\n30(4):623–629, Jul 1984.\\n[17] L. Ozarow and S. Leung-Yan-Cheong. An achievable region and outer bound for the Gaussian broadcast channel with feedback\\n(corresp.). Information Theory, IEEE Transactions on, 30(4):667–671, Jul 1984.\\n[18] G. Kramer. Feedback strategies for white Gaussian interference networks. Information Theory, IEEE Transactions on, 48(6):1423–1438,\\nJun 2002.\\n[19] S.I. Bross and M.A. Wigger. On the relay channel with receiver transmitter feedback. Information Theory, IEEE Transactions on,\\n55(1):275–291, Jan. 2009.\\n[20] A. Sahai, S.C. Draper, and M. Gastpar. Boosting reliability over awgn networks with average power constraints and noiseless feedback.\\nIn Information Theory, 2005. ISIT 2005. Proceedings. International Symposium on, pages 402–406, Sept. 2005.\\n[21] D. Gündüz, D.R. Brown, and H.V. Poor. Secret communication with feedback. In Information Theory and Its Applications, 2008.\\nISITA 2008. International Symposium on, pages 1–6, Dec. 2008.\\n[22] Y-H. Kim. Feedback capacity of the first-order moving average Gaussian channel. Information Theory, IEEE Transactions on,\\n52(7):3063–3079, July 2006.\\n[23] Y-H. Kim. Gaussian Feedback Capacity. PhD thesis, Stanford University, 2006.\\n\\n\\x0c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apk0aLSQnkjN",
        "outputId": "e005db5e-2d45-4804-ef68-c1860c140042"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(list(zip(archivo,archivos)), columns = ['archivo','contenido'])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         archivo                                          contenido\n",
            "0  0704.3504.txt  Smooth Rényi Entropy of Ergodic Quantum\\nInfo...\n",
            "1  0706.1402.txt  Analyzing Design Process and Experiments on th...\n",
            "2  0710.0736.txt  1\\n\\nColour Image Segmentation by the\\nVector-...\n",
            "3  0803.2570.txt  1\\n\\nUnequal Error Protection:\\nAn Information...\n",
            "4  0808.0084.txt  On the hitting times of quantum versus random ...\n",
            "5  0811.1254.txt  arXiv:0811.1254v1 [math.CO] 8 Nov 2008\\n\\nChap...\n",
            "6  0811.2853.txt  Generating Random Networks Without Short Cycle...\n",
            "7  0812.2709.txt  1\\n\\nVariations on a theme by Schalkwijk and K...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEfjv01apcS7",
        "outputId": "9e944397-ac3e-46d4-dc68-56edc3cc9018"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = df['contenido'].tolist()\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "print(X.toarray())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00', '000', '0010117', '003', '0044', '0045', '007', '0084v1', '01', '0156', '015848', '02', '025', '027', '03', '0301043', '0309430', '04', '0403263', '042312', '05', '0512258', '052307', '06', '0608426', '0610143', '0610143v2', '0610146', '0610146v2', '07', '0704', '0710', '0736v1', '08', '0803', '0808', '0811', '0812', '09', '0i', '0ih0', '0kwα', '10', '100', '101', '1010001', '1016', '102', '103', '104', '105', '106', '10623', '107', '108', '1085', '109', '1095', '1099', '11', '110', '1101000', '1108', '111', '1117', '112', '1123', '113', '114', '115', '116', '117', '117543', '118', '119', '12', '120', '1201', '121', '1212', '1216698', '122', '1229', '123', '123000', '1234', '1238', '124', '1245', '1248', '125', '1254v1', '126', '1261', '1267', '1268', '1269', '127', '1271766600', '1273', '128', '1288', '129', '12s0', '13', '130', '1302', '131', '132', '133', '134', '1343', '135', '1359', '136', '1365', '137', '138', '1382', '139', '1391', '1393', '1397', '14', '140', '141', '1416', '142', '1420', '1423', '1426', '143', '1435', '1438', '144', '1441', '145', '146', '147', '148', '149', '1496770', '1496833', '14th', '15', '150', '151', '152', '1523', '1527', '1554140', '1567', '157', '1584', '159', '16', '160', '161', '16242', '1632', '1641', '1650', '166', '167', '168', '1682', '169', '1694', '17', '172', '1737', '1744', '175', '1764', '1770', '179', '18', '180', '181', '182', '183', '1839', '184', '185', '1853', '1860', '1861', '1865', '187', '1873', '1886', '189', '19', '190', '190680', '1933', '1937', '1938', '194', '1940', '1948', '1949', '195', '1950', '1952', '1953', '1954', '1956', '1959', '196', '1960', '1961', '1963', '1964', '1965', '196560', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '199', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '1i', '1ih1', '1n', '1st', '1t', '1x', '20', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2009a', '2009b', '2010', '2011', '2013', '2014', '2015', '2017', '202204', '205', '206', '207', '209685', '21', '210', '211', '212', '214', '215', '216', '219', '22', '220', '221', '221229', '2231', '2235', '224', '225', '228', '229', '23', '232', '233', '237', '239', '239329029060', '24', '240', '241', '24140500956', '243', '2438973', '2477', '25', '2504', '251', '253', '256', '2561', '257', '2570v4', '2595', '26', '261', '262', '264', '265', '266', '267', '269', '27', '2702', '2709v4', '2714', '272', '275', '276', '277', '28', '283', '2853v2', '287', '288', '289', '2893', '29', '291', '295', '2957388', '296', '298', '2a', '2a0', '2a20', '2c1', '2d', '2e', '2e2m', '2e33', '2gn', '2iθj', '2k', '2m', '2n', '2nd', '2p', '2pe', '2pi', '2pn', '2pns', '2q', '2r', '2t', '2w', '2ǫ', '2β', '2γn1', '2δ', '2δj2', '2ε', '2η', '2ηn', '2π', '2πk', '2πψ', '2φ', '2ℓ', '30', '301', '305', '3060', '3063', '307', '3074', '3079', '3080', '309', '31', '310', '311', '313', '314', '315', '316', '318', '319', '32', '323', '325', '328', '33', '3329', '333', '3333', '334', '335', '336', '337', '3378', '339', '33rd', '34', '342', '34337160', '344', '348', '349', '35', '3504v1', '354', '36', '36176', '362', '363', '364', '366', '367', '37', '377', '3788', '379', '38', '381', '384', '387', '388', '39', '391', '396', '397', '39th', '3g1', '3k', '3kα', '3pprovided', '3qd', '3rd', '40', '400', '402', '406', '407', '408', '4096', '41', '417', '419', '41st', '42', '421', '422', '423', '425', '43', '435', '438', '44', '440', '449', '449820', '45', '450', '453', '454', '456', '45th', '46', '465', '468', '47', '476', '48', '487', '489', '49', '490', '491', '4978', '4a', '4a2', '4a20', '4anα', '4b', '4c2', '4htε', '4n', '4n1', '4pi', '4th', '4ǫ', '4λ2', '4νj2', '4φ', '4φ1', '4ℓ', '50', '50456', '506', '507', '508', '51', '518', '52', '522', '526', '53', '539', '5390', '54', '543', '55', '5577', '56', '566', '57', '575', '576', '577', '58', '581', '584', '59', '5963', '5a', '5b', '5th', '60', '600', '607', '61', '610814', '611', '613', '62', '623', '6279', '629', '63', '633', '639', '64', '640', '644', '647', '65', '656', '657', '66', '6648', '667', '67', '671', '677', '68', '682', '683', '685', '687', '689', '69', '690', '692', '6p', '70', '700', '701', '702', '707', '708', '71', '71307600', '714', '72', '729', '73', '733', '74', '747', '749', '749999640', '75', '753', '759', '76', '77', '771', '773', '78', '789', '78α1', '79', '80', '803', '807', '81', '819', '82', '824', '827', '828', '83', '831', '84', '845', '85', '8580', '86', '860', '864', '866876', '87', '873', '873450', '88', '880', '883', '889', '89', '8a20', '90', '91', '910', '91405', '916', '92', '928', '93', '94', '94305', '945', '949', '95', '9511026', '96', '963', '97', '98', '982', '99', '9rf', 'a0', 'a1', 'a11', 'a12', 'a15', 'a16', 'a2', 'a20', 'a21', 'a23', 'a2j', 'a3', 'a4', 'a5', 'a7', 'a8', 'aa05', 'aakv01', 'aaronson', 'aat', 'abelian', 'aber', 'abh', 'abilities', 'ability', 'able', 'abn', 'about', 'above', 'abovementioned', 'abs', 'absence', 'absent', 'absorbed', 'absorption', 'abstract', 'abundance', 'abuse', 'ac', 'acad', 'academic', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepts', 'access', 'accessed', 'accessible', 'accommodate', 'accomplished', 'according', 'account', 'accounts', 'accumulated', 'accuracy', 'achievability', 'achievable', 'achieve', 'achieved', 'achievements', 'achieves', 'achieving', 'acknowledge', 'acknowledged', 'acknowledges', 'acknowledgment', 'acknowledgments', 'acm', 'acquired', 'across', 'act', 'acta', 'acting', 'action', 'active', 'actively', 'actual', 'actually', 'ad', 'adapt', 'adaptation', 'adapted', 'adapting', 'adaptive', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adequate', 'adiposity', 'adjacency', 'adjacent', 'adjoin', 'adjusted', 'admit', 'admits', 'adolesc', 'adolescent', 'adopt', 'adriano', 'ads', 'adv', 'advance', 'advanced', 'advances', 'advancing', 'advantage', 'advantages', 'ae1', 'aegean', 'aep', 'affected', 'affiliations', 'affine', 'affirmative', 'afford', 'aforementioned', 'afosr', 'afresh', 'after', 'ag', 'again', 'against', 'agents', 'ages', 'agl', 'ago', 'agreed', 'aharonov', 'ahlswede', 'ai', 'aict', 'aigner', 'aij', 'ailath', 'aim', 'aimed', 'aims', 'ain', 'aj', 'ak', 'akj', 'akr05', 'al', 'alarm', 'alarms', 'albanese', 'albeit', 'albenese', 'albert', 'ale', 'alert', 'alessandro', 'algebr', 'algebra', 'algebraic', 'algebraically', 'algebras', 'algoqp', 'algorithm', 'algorithmic', 'algorithmica', 'algorithms', 'all', 'allen', 'allencahn', 'allerton', 'allgemeine', 'allocate', 'allocated', 'allocating', 'allocation', 'allow', 'allowable', 'allowed', 'allowing', 'allows', 'almost', 'alon', 'alone', 'along', 'alphabet', 'alphabets', 'already', 'also', 'altered', 'alternate', 'alternating', 'alternative', 'alternatively', 'although', 'always', 'am', 'amb03', 'amb04', 'ambainis', 'amer', 'american', 'amin', 'among', 'amongst', 'amount', 'amplification', 'amplified', 'amplifying', 'amplitude', 'amplitudes', 'amraoui', 'ams', 'amsterdam', 'an', 'anal', 'analog', 'analogies', 'analogon', 'analogons', 'analogous', 'analogs', 'analogue', 'analogues', 'analyses', 'analysis', 'analytical', 'analyzable', 'analyze', 'analyzed', 'analyzes', 'analyzing', 'anayak', 'and', 'andes', 'andrea', 'angew', 'angle', 'anisotropic', 'anita', 'anita2', 'ann', 'annotation', 'annotations', 'announc', 'annual', 'anomalies', 'anonymous', 'another', 'anr', 'answer', 'answered', 'answering', 'answers', 'anti', 'anticodes', 'antiphase', 'antonio', 'any', 'anytime', 'anyway', 'anα', 'apache', 'apacity', 'apart', 'apparent', 'appear', 'appeared', 'appearing', 'appears', 'appendix', 'appl', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approx', 'approximate', 'approximated', 'approximately', 'approximating', 'approximation', 'approximations', 'apr', 'april', 'arbitrarily', 'arbitrary', 'arch', 'architecture', 'architectures', 'architektur', 'are', 'area', 'areas', 'argue', 'argued', 'argument', 'arguments', 'arises', 'arising', 'arithmetic', 'aro', 'arose', 'around', 'arq', 'arrange', 'arranged', 'array', 'arrive', 'art', 'article', 'articles', 'artificially', 'arxiv', 'ary', 'as', 'aschbacher', 'ashwin', 'asiacrypt', 'aside', 'ask', 'asked', 'asks', 'aspect', 'aspects', 'assertion', 'assertions', 'assign', 'assigned', 'assigning', 'assignment', 'assigns', 'assmus', 'assmusmattson', 'assoc', 'associate', 'associated', 'associates', 'association', 'associations', 'associative', 'associe', 'assume', 'assumed', 'assumes', 'assuming', 'assumption', 'assumptions', 'assured', 'astronomy', 'asymptotic', 'asymptotically', 'at', 'att', 'attack', 'attained', 'attains', 'attempted', 'attempts', 'attend', 'attended', 'attention', 'attenuated', 'attenuation', 'attracted', 'attribute', 'aufgabe', 'aug', 'augmenting', 'august', 'aujol', 'aut', 'authentication', 'author', 'authoring', 'authors', 'automatic', 'automorphism', 'automorphisms', 'autonomously', 'aux', 'auxiliary', 'available', 'average', 'averaged', 'averages', 'averaging', 'avgustinovich', 'avoid', 'avoided', 'avoiding', 'avoids', 'awards', 'away', 'awgn', 'ax', 'axillary', 'aydinian', 'aθ', 'aθ0', 'aθj', 'b1', 'b2', 'b3', 'b4', 'bach', 'bachoc', 'back', 'background', 'backward', 'bad', 'baghdady', 'bajaj', 'balaji', 'balance', 'balanced', 'ball', 'balls', 'band', 'bandlimited', 'bandwidth', 'bannai', 'bar', 'bare', 'barely', 'barron', 'barıs', 'based', 'basel', 'basic', 'basically', 'basis', 'bassalygo', 'bayati', 'bayes', 'bb', 'be', 'bea', 'beautiful', 'became', 'because', 'becausepof', 'become', 'becomes', 'been', 'before', 'beginning', 'begun', 'behave', 'behaves', 'behavior', 'behind', 'being', 'believe', 'bell', 'belong', 'belonging', 'belongs', 'below', 'bence', 'bender', 'benefit', 'benefitted', 'benes', 'benjamin', 'ber', 'berkeley', 'berlekamp', 'berlin', 'bernoulli', 'berry', 'besides', 'best', 'bet', 'beth', 'betten', 'better', 'between', 'beutelspacher', 'beyond', 'bezdek', 'bi', 'bias', 'bias3', 'biased', 'biasπrg', 'biasπtf', 'big', 'bij', 'bijective', 'bility', 'billiard', 'bin', 'binary', 'biological', 'bipartite', 'biplane', 'biprandgraph', 'birkha', 'birkhauser', 'birkhoff', 'birth', 'bit', 'bits', 'bius', 'bizarre', 'bj', 'bjelakovic', 'bk', 'black', 'blake', 'blanc', 'blanchet', 'blind', 'blitzstein', 'block', 'blocklength', 'blocks', 'blomer', 'blowey', 'blowing', 'blows', 'blue', 'blur', 'bn', 'bn1', 'bob', 'boca', 'bohman', 'bold', 'bolloba', 'bonnecaze', 'book', 'boolean', 'boost', 'boosting', 'bootstrap', 'borade', 'bordered', 'boris', 'borrowing', 'bose', 'boston', 'both', 'bottle', 'boulders', 'bound', 'boundaries', 'boundary', 'bounded', 'bounding', 'bounds', 'box', 'boyarinov', 'brackets', 'bramoulle', 'branches', 'branching', 'braun', 'brazil', 'brazilian', 'brazilians', 'break', 'breaks', 'breiman', 'brest', 'brevity', 'brian', 'brief', 'briefly', 'brightness', 'brighton', 'britz', 'broad', 'broadband', 'broadcast', 'broader', 'broken', 'bross', 'brothers', 'brouwer', 'brown', 'browser', 'bruck', 'brust', 'bs', 'bsc', 'bu', 'buekenhout', 'buffalo', 'buhrman', 'building', 'builds', 'built', 'burlington', 'burnashev', 'business', 'but', 'bv', 'bx', 'by', 'by10', 'by13', 'by5', 'byers', 'bǫ', 'c0', 'c1', 'c2', 'c2r', 'c3', 'ca', 'cahn', 'calculate', 'calculated', 'calculates', 'calculating', 'calculation', 'calculations', 'calderbank', 'california', 'call', 'called', 'calls', 'calm', 'cam', 'cambridge', 'cameron', 'camion', 'can', 'canad', 'canada', 'cancel', 'canceled', 'candidate', 'candidates', 'canfield', 'cannot', 'canonical', 'capability', 'capable', 'capacities', 'capacity', 'capital', 'captured', 'captures', 'capturing', 'carathe', 'cardinality', 'care', 'carefully', 'carla', 'carlo', 'carmichael', 'carried', 'carus', 'case', 'caselles', 'cases', 'casual', 'catalogue', 'catalyst', 'category', 'causal', 'cause', 'caused', 'cavities', 'cavity', 'cb', 'cbt', 'ccf', 'ce', 'cea', 'cei', 'celebrated', 'cell', 'cemm98', 'census', 'center', 'centered', 'centers', 'central', 'centre', 'century', 'certain', 'certainly', 'certainty', 'cet', 'cf', 'cfm', 'cg04', 'ch', 'chain', 'chains', 'chalkwijk', 'challenge', 'challenges', 'challenging', 'chalupecky', 'chan', 'chance', 'chandrasekhar', 'change', 'changes', 'changing', 'channel', 'channels', 'chap', 'chapt', 'chapter', 'character', 'characteristic', 'characterizations', 'characterize', 'characterized', 'characterizing', 'chart', 'chebyshev', 'check', 'checking', 'checks', 'cheme', 'chen', 'cheong', 'chequered', 'chernoff', 'childs', 'chklovskii', 'choice', 'choose', 'chooses', 'choosing', 'chose', 'chosen', 'chou', 'chromaticity', 'chuang', 'chung', 'ci', 'cifar', 'cij', 'cijt', 'cinq', 'circles', 'circuit', 'circuits', 'circulant', 'citation', 'cj', 'ck', 'cker', 'claim', 'claims', 'clarendon', 'clarification', 'clarity', 'class', 'classes', 'classes_path', 'classic', 'classical', 'classification', 'classified', 'classify', 'classifying', 'classpara', 'classroom', 'clean', 'clear', 'clearly', 'cleve', 'clever', 'client', 'close', 'closely', 'closeness', 'closer', 'clouds', 'cluster', 'clusters', 'cmaia', 'cmmi', 'cn', 'cn2', 'cnrs', 'co', 'coached', 'coarse', 'coarsen', 'coarsening', 'coarsest', 'cobenge', 'code', 'codebook', 'coded', 'codes', 'codeword', 'codewords', 'coding', 'coe', 'coefficient', 'coefficients', 'coexist', 'cohen', 'cohn', 'coincides', 'coins', 'colbourn', 'coleman', 'collaborative', 'collected', 'collection', 'collectively', 'collinear', 'color', 'colour', 'colours', 'column', 'columns', 'com', 'com2', 'comb', 'combin', 'combination', 'combinatorial', 'combinatorially', 'combinatorica', 'combinatorics', 'combinatorische', 'combine', 'combined', 'combines', 'combining', 'combo', 'come', 'comes', 'comfortable', 'comlab', 'comm', 'commented', 'comments', 'commission', 'common', 'commun', 'communicate', 'communicated', 'communicating', 'communication', 'communications', 'community', 'commutativity', 'commute', 'comp', 'compact', 'comparable', 'compare', 'compared', 'comparing', 'comparison', 'complement', 'complementary', 'complete', 'completely', 'completeness', 'completes', 'complex', 'complexities', 'complexity', 'complicate', 'complicated', 'complication', 'component', 'components', 'composed', 'composite', 'composition', 'compressing', 'compression', 'compromised', 'comput', 'computable', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computerbased', 'computers', 'computes', 'computing', 'concatenation', 'concave', 'concavity', 'concede', 'concentrate', 'concentrated', 'concentration', 'concentrations', 'concentric', 'concept', 'concepts', 'conceptual', 'conceptually', 'concerned', 'concerning', 'concise', 'conclude', 'concluded', 'concludes', 'conclusion', 'conclusions', 'concrete', 'condition', 'conditional', 'conditioned', 'conditioning', 'conditions', 'conducted', 'conf', 'conference', 'confidence', 'configuration', 'configurations', 'confirmation', 'confirmed', 'confusion', 'congress', 'conj', 'conjecture', 'conjugate', 'conjugates', 'conjunction', 'connect', 'connected', 'connecting', 'connection', 'connections', 'connects', 'consequence', 'consequences', 'consequently', 'consider', 'considerable', 'considerably', 'consideration', 'considerations', 'considered', 'considering', 'considers', 'consisting', 'consists', 'consolidated', 'constant', 'constantly', 'constants', 'constellation', 'constituents', 'constrain', 'constrained', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'constructions', 'constructors', 'contain', 'contained', 'containing', 'contains', 'contemporary', 'content', 'context', 'contexts', 'contingency', 'continually', 'continue', 'continues', 'continuous', 'contour', 'contours', 'contradicting', 'contradiction', 'contradicts', 'contrary', 'contrast', 'contrasts', 'contributed', 'contribution', 'contributions', 'control', 'controlled', 'controlling', 'controversy', 'convenience', 'convenient', 'convention', 'conventional', 'conventionally', 'converge', 'convergence', 'converges', 'converse', 'conversely', 'converses', 'conversions', 'convex', 'convexity', 'convey', 'conveyed', 'convolve', 'convolving', 'conway', 'cooperative', 'coordinate', 'coordinates', 'coordination', 'copies', 'coprime', 'copy', 'copyright', 'core', 'corners', 'corollary', 'correct', 'corrected', 'correcting', 'correction', 'corrections', 'correctly', 'correlated', 'correlation', 'corresp', 'correspond', 'correspondence', 'corresponding', 'corresponds', 'corrollary', 'cos', 'cos2', 'cost', 'costly', 'costs', 'cot', 'cot2', 'could', 'count', 'countably', 'counted', 'counterexample', 'counting', 'counts', 'course', 'cover', 'covered', 'covering', 'covers', 'cq', 'cr', 'craig', 'crc', 'create', 'created', 'creates', 'creating', 'creation', 'criterion', 'critical', 'crucial', 'crude', 'cryptographic', 'cryptography', 'cs', 'csisza', 'cspecial', 'ct', 'cube', 'cuff', 'culture', 'cumbersome', 'curiously', 'current', 'curtis', 'curvature', 'curve', 'curves', 'customary', 'cv', 'cvetkovic', 'cw', 'cx', 'cx0', 'cycle', 'cycles', 'cycles2', 'cyclic', 'd0', 'd1', 'd2', 'd20', 'd21', 'd2i', 'd4', 'd8', 'damgn', 'danger', 'daniel', 'darpa', 'das', 'dass', 'data', 'database', 'david', 'days', 'db', 'de', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dec', 'decades', 'decay', 'decaying', 'decays', 'december', 'decide', 'decided', 'decides', 'decision', 'decisions', 'declare', 'declared', 'declares', 'declercq', 'decode', 'decoded', 'decoder', 'decodes', 'decoding', 'decomposes', 'decomposition', 'decrease', 'decreased', 'decreases', 'decreasing', 'dedicata', 'deduce', 'deep', 'deepened', 'deeper', 'defer', 'deferring', 'define', 'defined', 'defined6', 'defines', 'defining', 'definition', 'definitions', 'deg', 'degenerate', 'degree', 'degrees', 'degv', 'delay', 'delays', 'delete', 'deleting', 'delivering', 'delsarte', 'delving', 'demand', 'demanded', 'demands', 'dembowski', 'demonstrate', 'demonstrated', 'demonstrates', 'denoised', 'denoising', 'denominator', 'denote', 'denoted', 'denotes', 'densest', 'densities', 'density', 'department', 'departments', 'departure', 'depend', 'dependant', 'dependencies', 'dependent', 'depending', 'depends', 'depicting', 'dept', 'depth', 'der', 'derivation', 'derivative', 'derive', 'derived', 'deriving', 'des', 'descendant', 'descendants', 'descent', 'describe', 'described', 'describes', 'description', 'design', 'designed', 'designing', 'designs', 'desirable', 'desire', 'desired', 'despite', 'destroy', 'det', 'detail', 'detailed', 'details', 'detect', 'detected', 'detecting', 'detection', 'detections', 'determinant', 'determination', 'determine', 'determined', 'determines', 'determining', 'deterministic', 'deutsche', 'devedzic', 'develop', 'developed', 'developing', 'development', 'develops', 'deviation', 'deviations', 'device', 'devices', 'devised', 'devoted', 'df', 'dfg', 'di', 'diaconis', 'diag', 'diagonal', 'diagram', 'diam', 'diameter', 'did', 'die', 'differ', 'difference', 'differences', 'different', 'differential', 'differentiate', 'differently', 'differing', 'differs', 'difficult', 'difficulties', 'diffuse', 'diffusion', 'diggavi', 'digital', 'dij', 'dimension', 'dimensional', 'dimensions', 'dinitz', 'diploma', 'dirac', 'direct', 'directed', 'direction', 'directional', 'directions', 'directives', 'directly', 'directory', 'disadvantages', 'disappointing', 'discarded', 'discipline', 'disciplines', 'disconnect', 'discontinued', 'discontinuities', 'discontinuous', 'discover', 'discovered', 'discovery', 'discrete', 'discretisation', 'discretise', 'discriminant', 'discrimination', 'discuss', 'discussed', 'discusses', 'discussing', 'discussion', 'discussions', 'disjoint', 'diskretn', 'displays', 'diss', 'disseminate', 'dissemination', 'dissertation', 'distance', 'distances', 'distant', 'distinct', 'distinction', 'distinctness', 'distinguish', 'distinguishing', 'distortion', 'distributed', 'distribution', 'distributions', 'distributions12', 'divergence', 'diversity', 'divide', 'divided', 'divisible', 'dixon', 'dl', 'dmax', 'dmc', 'dmcs', 'dmv', 'dn', 'dn1', 'do', 'doc', 'dodecad', 'dodecads', 'dodecahedron', 'does', 'doesn', 'doi', 'doing', 'doklady', 'dom', 'domain', 'domains', 'dominate', 'dominated', 'dominating', 'done', 'doob', 'dordrecht', 'dot', 'double', 'doublecounting', 'doubles', 'doubly', 'dover', 'down', 'draft', 'draper', 'drawback', 'drawn', 'dreien', 'dreizehn', 'driven', 'driver', 'drives', 'driving', 'drop', 'drs', 'ds', 'dt', 'dtd', 'dual', 'due', 'dueck', 'duff', 'duration', 'durations', 'during', 'dwispc8', 'dx', 'dynamic', 'dynamically', 'dynamics', 'dynkin', 'dz', 'dθ', 'dσ', 'dℓ', 'e1', 'e2', 'e2iθj', 'e2t', 'e4', 'e8', 'ea', 'each', 'earlier', 'early', 'ease', 'easier', 'easiest', 'easily', 'easy', 'eb', 'ebenen', 'ebf', 'ebij', 'ebits', 'eccc', 'econometrics', 'economic', 'economics', 'ed', 'edge', 'edges', 'edited', 'edition', 'editor', 'editors', 'edmonds', 'eds', 'edu', 'education', 'edward', 'ee', 'eecs', 'eedback', 'efa', 'efaf', 'efal', 'efau', 'eferences', 'effect', 'effective', 'effectively', 'effects', 'efficiency', 'efficient', 'efficiently', 'effort', 'efron', 'eh', 'ei', 'ei1', 'eigenbasis', 'eigenphase', 'eigenphases', 'eigenschaft', 'eigenspace', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eigenvectors', 'eight', 'eighth', 'eindhoven', 'eine', 'einer', 'either', 'eiα', 'eiαj', 'eiθj', 'ek', 'ek00', 'ekert', 'el', 'elaborate', 'elaborates', 'electrical', 'electron', 'electronic', 'electronically', 'electronics', 'elegant', 'element', 'elementary', 'elemente', 'elementen', 'elements', 'elias', 'eliminate', 'eliminating', 'elkies', 'elliott', 'elliptic', 'else', 'embedding', 'emd', 'emergence', 'emergency', 'emerging', 'emphasis', 'emphasize', 'emphasizes', 'emphasizing', 'empirical', 'empirically', 'employed', 'employs', 'emprically', 'empty', 'emre', 'en', 'enabled', 'enables', 'enc', 'encapsulate', 'encoded', 'encoder', 'encoding', 'encourage', 'encouragement', 'encouraging', 'encyclopedia', 'encyclopedias', 'end', 'endlicher', 'endogenous', 'endpoint', 'ends', 'energy', 'enf', 'enforce', 'engineer', 'engineering', 'english', 'engr', 'ength', 'enh', 'enhance', 'enhancement', 'enhances', 'enough', 'enr', 'enrich', 'enrq', 'enseignement', 'ensemble', 'ensure', 'ensures', 'ensuring', 'entire', 'entries', 'entropy', 'entry', 'enumerator', 'enumerators', 'environment', 'environments', 'eo', 'eprint', 'eq', 'eqs', 'equal', 'equalities', 'equality', 'equally', 'equals', 'equation', 'equations', 'equi', 'equilibrium', 'equiprobable', 'equivalence', 'equivalent', 'equivalently', 'er', 'era', 'erasure', 'erasures', 'erdo', 'ergodic', 'ern', 'erroneous', 'erroneously', 'error', 'errorcorrecting', 'errors', 'erweiterungen', 'es', 'esedog', 'esp', 'especially', 'essence', 'essentially', 'establish', 'established', 'establishes', 'establishing', 'establishment', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estimations', 'esult', 'eswaran', 'et', 'etc', 'eth', 'ethz', 'etzion', 'eu', 'eua', 'euclidean', 'eugenics', 'euler', 'eur', 'eurasip', 'europ', 'european', 'ev', 'eva', 'evaluate', 'evaluated', 'evaluation', 'evaluations', 'even', 'event', 'events', 'eventually', 'ever', 'every', 'everything', 'evgeny', 'evidently', 'evolution', 'evolve', 'evolvement', 'evolvements', 'ex', 'exact', 'exactly', 'exam', 'examination', 'examine', 'examined', 'examining', 'example', 'examples', 'exams', 'exceed', 'exceeds', 'except', 'exception', 'exceptional', 'exclude', 'excluding', 'exclusion', 'exclusively', 'execute', 'execution', 'exemplary', 'exercise', 'exercises', 'exhibit', 'exist', 'existence', 'existent', 'existing', 'exists', 'exits', 'exp', 'expanding', 'expect', 'expectation', 'expected', 'expects', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'explicitly', 'exploiting', 'explore', 'exponent', 'exponential', 'exponential1', 'exponentially', 'exponentials', 'exponentiation', 'exponents', 'exposition', 'express', 'expressed', 'expressing', 'expression', 'expressions', 'extend', 'extended', 'extending', 'extends', 'extensibility', 'extension', 'extensions', 'extensive', 'extensively', 'extent', 'extra', 'extract', 'extracting', 'extraction', 'extraordinarily', 'extremal', 'extreme', 'extremely', 'extremum', 'ez', 'eθ', 'eπ', 'f1', 'f2', 'f24', 'f2n', 'f3', 'f4', 'fa', 'fa9550', 'face', 'faced', 'fach', 'facing', 'fact', 'factor', 'factors', 'facts', 'faculty', 'fahnenhomogene', 'fail', 'failed', 'failure', 'falls', 'falmer', 'faloutsos', 'false', 'falsified', 'familiar', 'family', 'famous', 'fano', 'far', 'fast', 'faster', 'favor', 'fe', 'feasible', 'feature', 'features', 'fec', 'fed', 'feedback', 'fekri', 'felt', 'few', 'fidelity', 'field', 'fields', 'fifteen', 'fifteenth', 'fifth', 'fifty', 'fig', 'figure', 'figures', 'fijalkow', 'file', 'fileclassloader', 'filepap', 'fill', 'filling', 'filter', 'final', 'finalizing', 'finallly', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finer', 'finest', 'finishes', 'finite', 'first', 'firstly', 'firststage', 'fisher', 'fit', 'fitting', 'five', 'fix', 'fixed', 'fixing', 'fl', 'flag', 'flaw', 'flexibility', 'flexible', 'flip', 'floors', 'flowers', 'fn', 'fnq', 'focus', 'focused', 'focuses', 'focusing', 'fois', 'fold', 'folds', 'follow', 'followed', 'following', 'follows', 'follows4', 'fonction', 'fonctions', 'for', 'forbidden', 'forced', 'forces', 'fore', 'forget', 'form', 'formal', 'formalizations', 'formally', 'format', 'formation', 'formed', 'former', 'forms', 'formula', 'formulas', 'formulation', 'formulations', 'forney', 'forschungsgemeinschaft', 'forty', 'forward', 'found', 'foundation', 'foundations', 'four', 'fourteen', 'fourth', 'fq', 'fr', 'fraction', 'fractional', 'fragments', 'frame', 'framework', 'france', 'francis', 'frasson', 'fre', 'free', 'freedom', 'freedom9', 'french', 'frequency', 'friends', 'friendships', 'fripertinger', 'from', 'fruitful', 'fu', 'fuer', 'fujimoto', 'full', 'fully', 'function', 'functional', 'functionalities', 'functionality', 'functions', 'fundamental', 'fundamentals', 'funded', 'further', 'furthermore', 'furtherpk', 'furtherǫ', 'fusco', 'future', 'g0', 'g2', 'gage', 'gain', 'galeotti', 'gallager', 'galois', 'game', 'games', 'gap', 'garcke', 'gas', 'gastpar', 'gates', 'gathered', 'gauss', 'gaussian', 'gaussiannoise', 'gauthier', 'gave', 'gegenbauer', 'general', 'generality', 'generalization', 'generalizations', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generator', 'generators', 'generic', 'geom', 'geometer', 'geometric', 'geometrical', 'geometrically', 'geometries', 'geometry', 'german', 'germany', 'get', 'getclass', 'getmethod', 'getnodename', 'gets', 'getting', 'gf', 'gi', 'gibbs', 'gils', 'ginn', 'girth', 'give', 'given', 'gives', 'giving', 'gk', 'gleason', 'global', 'globally', 'globalvalues', 'gm', 'gn', 'gn2', 'go', 'goal', 'goals', 'goes', 'goethals', 'going', 'golay', 'goldstone', 'good', 'gore', 'government', 'govindan', 'gr', 'grade', 'gradient', 'gradients', 'gradually', 'graduate', 'graham', 'grain', 'grands', 'grant', 'graph', 'graphical', 'graphs', 'grassman', 'gratefully', 'gray', 'great', 'greater', 'greatest', 'greedy', 'green', 'gregory', 'grid', 'grids', 'griffiths', 'gro', 'gro96', 'group', 'groupes', 'groups', 'grouptheoretical', 'grover', 'growing', 'grows', 'gruppen', 'gt', 'gtπ', 'gu', 'guarantee', 'guaranteed', 'guarantees', 'gui', 'gulliver', 'gθ', 'gπ', 'gπt', 'h0', 'h02ǫ', 'h0ǫ', 'h1', 'h2ǫ', 'had', 'hadamard', 'haemers', 'haifa', 'half', 'hall', 'halves', 'hamburg', 'hamming', 'han', 'hand', 'handbook', 'handheld', 'handle', 'hanging', 'hannel', 'happening', 'happens', 'harada', 'hard', 'harder', 'harlow', 'harmless', 'harmonic', 'harmonically', 'harper', 'has', 'hat', 'have', 'having', 'he', 'headers', 'health', 'healthcare', 'heavy', 'heden', 'heidelberg', 'helleseth', 'help', 'helpful', 'hence', 'henceforth', 'hensel', 'her', 'here', 'hering', 'hermitian', 'herrera', 'hessler', 'heterogeneity', 'heterogeneous', 'heuristic', 'hexacode', 'hf', 'hfalse', 'hierarchy', 'high', 'higher', 'highest', 'highlight', 'highly', 'hilbert', 'hill', 'him', 'hint', 'hints', 'hirschfeld', 'his', 'histogram', 'histograms', 'historical', 'history', 'hitting', 'hn', 'hoc', 'hoeffding', 'hold', 'holds', 'holland', 'holmes', 'home', 'homogeneous', 'homogeneously', 'hong', 'honkala', 'hope', 'hoped', 'host', 'hours', 'how', 'however', 'hp', 'hs', 'hsv', 'ht', 'html', 'http', 'https', 'htε', 'huber', 'hue', 'huffman', 'hughes', 'huht', 'human', 'hundred', 'hurting', 'hut', 'hutter', 'hw', 'hwα', 'hx', 'hxy', 'hy', 'hybrid', 'hypercube', 'hyperoval', 'hyperovals', 'hyperplane', 'hyperplanes', 'hypotheses', 'hypothesis', 'hypothetical', 'hyx', 'hz', 'hµ', 'hµz', 'hǫ', 'hǫ0', 'hǫα', 'hα', 'hαǫ', 'hβ', 'hε', 'hηi', 'hξ', 'hφ0', 'i0', 'i1', 'i12', 'i2', 'i3', 'i4', 'i6', 'icece', 'icm', 'icosahedron', 'id', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identification', 'identified', 'identify', 'identifying', 'identities', 'identity', 'ieee', 'if', 'ignored', 'ignoring', 'ihvi', 'ihwα', 'ihx', 'ihµz', 'ihθ', 'ihφx', 'ihφz', 'ii', 'iid', 'iii', 'ij', 'ijcv', 'ik', 'ik2', 'illinois', 'illness', 'illustrate', 'illustrated', 'illustrates', 'illustrating', 'illustration', 'illustrative', 'im', 'image', 'images', 'imagine', 'immaterial', 'immediate', 'immediately', 'implement', 'implementation', 'implemented', 'implementing', 'implications', 'implicitly', 'implied', 'implies', 'imply', 'importance', 'important', 'importantly', 'impose', 'imposed', 'impossible', 'impression', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'imre', 'in', 'inc', 'inceptions', 'incidence', 'include', 'includes', 'including', 'inclusion', 'incomplete', 'inconsequential', 'incorporating', 'incorrect', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'indebted', 'indeed', 'independent', 'independent4', 'independently', 'indepth', 'index', 'indexed', 'indicate', 'indicates', 'indicating', 'indication', 'indicator', 'indicators', 'indices', 'indirectly', 'individually', 'individuals', 'induced', 'inen', 'inequalities', 'inequality', 'inf', 'infeasibility', 'infeasible', 'infimum', 'infinite', 'infinity', 'influence', 'influencing', 'infocom', 'inform', 'information', 'informatsii', 'infrared', 'ingeniously', 'ingredients', 'inherent', 'initial', 'initialize', 'initialized', 'initially', 'initiated', 'inner', 'input', 'inputs', 'inquiring', 'inscribed', 'insensitive', 'inserting', 'inside', 'insights', 'insignificant', 'insignificantly', 'inspecting', 'inspired', 'inst', 'install', 'installation', 'installed', 'instance', 'instead', 'institut', 'institute', 'instruction', 'instructive', 'int', 'integer', 'integers', 'integral', 'integrated', 'integrates', 'integration', 'intelligence', 'intelligent', 'intelligente', 'intended', 'intense', 'intensity', 'intentionally', 'interact', 'interaction', 'interactions', 'interactive', 'interest', 'interested', 'interesting', 'interestingly', 'interface', 'interface1', 'interfaces', 'interference', 'intermittent', 'internal', 'internally', 'international', 'internet', 'interplay', 'interpolant', 'interpolants', 'interpolating', 'interpret', 'interpretation', 'interpreted', 'interrelations', 'interscience', 'intersect', 'intersecting', 'intersection', 'intersects', 'interval', 'interwoven', 'intial', 'into', 'introduce', 'introduced', 'introduces', 'introducing', 'introduction', 'introspection', 'intuition', 'intuitive', 'intuitively', 'invariant', 'invariants', 'inventiones', 'inverse', 'inversely', 'investigate', 'investigated', 'investigating', 'investigation', 'investigations', 'invoke', 'invokes', 'involes', 'involve', 'involved', 'involves', 'involving', 'io', 'ioannides', 'ionin', 'iq', 'iqc', 'ir', 'ire', 'irreducible', 'irregular', 'irrespective', 'is', 'is3', 'isaac', 'isit', 'isita', 'isometries', 'isomorphic', 'isomorphism', 'isotropic', 'iss', 'issled', 'issue', 'issues', 'ist', 'it', 'italy', 'iterate', 'iterated', 'iterates', 'iterating', 'iteration', 'iterations', 'iterative', 'ith', 'itmanet', 'ito', 'itoh', 'its', 'itself', 'itzkovitz', 'iu', 'iv', 'iwuc', 'ix', 'ixx', 'iyw', 'izvestiya', 'iα', 'iαj', 'iθj', 'j12', 'j4', 'j6', 'ja', 'jackknife', 'jackson', 'jacobi', 'jacqueline', 'jadohealth', 'jakob', 'jamin', 'jan', 'jansen', 'janson', 'java', 'jdbc', 'jdsk', 'je', 'jensen', 'jeong', 'ji', 'jilles', 'jk', 'jn', 'job', 'joel', 'john', 'johnson', 'join', 'joint', 'jointly', 'journal', 'journey', 'joy', 'jr', 'jsp', 'jul', 'july', 'jun', 'junctions', 'june', 'jung', 'jungnickel', 'juni', 'just', 'justified', 'justifies', 'k2', 'k22', 'kailath', 'kang', 'karush', 'kashtan', 'kaski', 'katsman', 'kay', 'keep', 'keeping', 'keevash', 'kempe', 'kennedy', 'kept', 'kerber', 'kernel', 'kernels', 'keshavan', 'keug1', 'key', 'keyes', 'keyword', 'keywords', 'khachatrian', 'kilgus', 'kim', 'kind', 'kinds', 'kirkman', 'kissing', 'kit95', 'kitaev', 'kkt', 'kl', 'kleinberg', 'kn', 'know', 'knowing', 'knowledge', 'known', 'knows', 'ko', 'koetter', 'kohnert', 'koric', 'kornhuber', 'kpy', 'kpz', 'kq', 'kr', 'kramer', 'krause', 'krawtchouk', 'krishnan', 'ksk', 'ksk2', 'kth', 'kudryashov', 'kugeln', 'kuhn', 'kullback', 'kumar', 'kw', 'kw1α', 'kwy', 'kwα', 'kwα1', 'kyoto', 'kyushu', 'kµ', 'kα', 'kφe0', 'kφx', 'kψk', 'kψk2', 'la', 'lab', 'label', 'labeled', 'labeling', 'labelled', 'labels', 'laboratories', 'laboratory', 'labs', 'lam', 'lamb', 'land', 'language', 'lapidoth', 'large', 'larger', 'largest', 'larms', 'las', 'lassen', 'last', 'lastly', 'later', 'latin', 'latter', 'lattice', 'lattices', 'launch', 'law', 'laws', 'layer', 'layered', 'layers', 'ldpc', 'lead', 'leading', 'leads', 'leaf', 'learn', 'learning', 'least', 'leave', 'leaves', 'leaving', 'lect', 'lectnotes', 'lecture', 'lectures', 'led', 'leech', 'left', 'legendresymbol', 'leibler', 'leiva', 'lemma', 'lemmas', 'lemmata', 'lena', 'lends', 'lenght', 'length', 'lengths', 'lenz', 'leont', 'les', 'less', 'let', 'lets', 'lett', 'letter', 'letters', 'letting', 'leung', 'level', 'levels', 'levenshtein', 'levensthein', 'lewis', 'lexicographic', 'lexicographically', 'lias', 'lie', 'lies', 'lifted', 'light', 'like', 'likelihood', 'likely', 'lim', 'limit', 'limited', 'limits', 'limk', 'limx', 'limǫ', 'limβ', 'lin', 'line', 'linear', 'linearly', 'linearly14', 'liner', 'lines', 'link', 'links', 'lint', 'lipschitz', 'list', 'listed', 'literature', 'litsyn', 'little', 'liu', 'lizhong', 'lloyd', 'ln', 'lnc2', 'lncs', 'lne', 'loadclass', 'loader', 'lobstein', 'local', 'locate', 'locations', 'lock', 'locked', 'log', 'log2', 'logarithmic', 'logic', 'login', 'lond', 'london', 'long', 'longer', 'longman', 'look', 'looking', 'looks', 'looped', 'looping', 'loops', 'loseness', 'losing', 'loss', 'lost', 'lot', 'lova', 'low', 'lower', 'lowest', 'lri', 'lu', 'luby', 'luczak', 'luminosity', 'lumped', 'luxembourg', 'luxury', 'm0', 'm1', 'm11', 'm12', 'm2', 'm22', 'm23', 'm24', 'm3', 'm4', 'mac', 'macchiavello', 'machine', 'macro', 'macroscopic', 'macwilliams', 'made', 'mag', 'magniez', 'magnitude', 'main', 'maindetect', 'mainly', 'maintained', 'maintaining', 'maintains', 'major', 'majority', 'make', 'makers', 'makes', 'making', 'malik', 'management', 'manipulations', 'manner', 'manuscript', 'many', 'map', 'mapped', 'mapping', 'mappings', 'maps', 'mar', 'marcellin', 'march', 'margin', 'marked', 'market', 'markov', 'marshall', 'martingale', 'masiero', 'masnick', 'mass', 'massachusetts', 'master', 'mat', 'match', 'matched', 'material', 'materials', 'math', 'mathematicae', 'mathematical', 'mathematics', 'mathematik', 'mathieu', 'matrices', 'matrix', 'matta', 'matter', 'matthias', 'mattson', 'mauritius', 'max', 'max1', 'maxi', 'maximal', 'maximization', 'maximize', 'maximizers', 'maximizing', 'maximum', 'maxz', 'maxzn', 'may', 'mbo', 'mcgraw', 'mcmillan', 'me', 'mean', 'meaning', 'means', 'meantime', 'meanwhile', 'measurable', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'mechanical', 'mechanism', 'mechanisms', 'medina', 'medt', 'meet', 'meeting', 'meets', 'member', 'members', 'memorial', 'memory', 'memoryless', 'mention', 'mentioned', 'mentioning', 'merely', 'merriman', 'mes', 'mesh', 'mesner', 'message', 'messages', 'messagewise', 'messina', 'metal', 'meter', 'method', 'methodology', 'methods', 'metz', 'meyer', 'mg', 'mhuber', 'mi', 'michael', 'mid', 'might', 'mij', 'miklos', 'mikula', 'milan', 'milestones', 'milo', 'min', 'mini', 'minima', 'minimal', 'minimisation', 'minimise', 'minimised', 'minimises', 'minimising', 'minimize', 'minimized', 'minimizes', 'minimizing', 'minimum', 'ministry', 'ministère', 'minj', 'minqy', 'minute', 'miracle', 'misconceptions', 'misleading', 'missed', 'misseddetection', 'mistaken', 'mit', 'mitter', 'mitzenmacher', 'mixing', 'mj', 'mk', 'ml', 'mm', 'mmse', 'mn', 'mn07', 'mnrs07', 'mo', 'mobile', 'mod', 'mode', 'model', 'modeling', 'models', 'moderately', 'modern', 'modest', 'modica', 'modification', 'modifications', 'modified', 'modify', 'modifying', 'modulation', 'mog', 'mohsen', 'moire', 'moment', 'monograph', 'monotone', 'monotonically', 'monotonicity', 'montanar', 'montanari', 'monte', 'monthly', 'montreal', 'more', 'morelos', 'moreover', 'morris', 'mortimer', 'mortola', 'mosca', 'most', 'mostly', 'motifs', 'motion', 'motivate', 'motivated', 'motivates', 'motivation', 'mountain', 'move', 'moves', 'moving', 'mr', 'mrt', 'ms', 'mss07', 'mt', 'much', 'muller', 'multi', 'multiclassloader', 'multigrid', 'multilevel', 'multiphase', 'multiple', 'multiplechoice', 'multiples', 'multiplexing', 'multiplication', 'multiplications', 'multiplicative', 'multiplied', 'multiply', 'multiplying', 'multrigrid', 'mumford', 'musin', 'must', 'mutual', 'mutually', 'mx', 'myopic', 'mβ', 'n0', 'n00014', 'n0q', 'n1', 'n2', 'n2k', 'n2kα', 'n2l', 'n2rα', 'n3', 'nakib', 'nakibog', 'name', 'named', 'namely', 'national', 'nato', 'nats', 'natural', 'naturally', 'nature', 'naval', 'navigation', 'nayak', 'nd', 'ndu', 'ne', 'nearest', 'nearly', 'neb', 'nebe', 'neburg', 'neces', 'necessarily', 'necessary', 'necessitate', 'necessity', 'necked', 'need', 'needed', 'needless', 'needs', 'negative', 'negativity', 'negligible', 'neighbor', 'neighborhood', 'neighboring', 'neighbors', 'neighbouring', 'neighbours', 'neither', 'nemd', 'neo', 'neq', 'nested', 'nestler', 'netherlands', 'netw', 'network', 'networking', 'networks', 'neumaier', 'neumann', 'neuss', 'never', 'nevertheless', 'new', 'newinstance', 'newly', 'newman', 'newton', 'next', 'nh', 'ni', 'nielsen', 'nig', 'nighttime', 'nik', 'nine', 'nineteenth', 'njas', 'nk', 'nkα', 'nl', 'nn', 'no', 'node', 'nodes', 'noise', 'noiseless', 'noisy', 'non', 'nonadjacent', 'nonbinary', 'nonblock', 'nonempty', 'nonetheless', 'nonexistence', 'nonlinear', 'nonn', 'nonnegative', 'nonnormalized', 'nonuniform', 'nonzero', 'nor', 'norm', 'normal', 'normalization', 'normalized', 'normalizing', 'north', 'northholland', 'nos', 'not', 'notably', 'notation', 'notational', 'notations', 'note', 'noted', 'notes', 'notes5', 'nothing', 'notice', 'notices', 'noting', 'notion', 'notionpof', 'notions', 'nov', 'novel', 'now', 'nowadays', 'np', 'np2', 'nr', 'nr1', 'ns', 'nsa', 'nserc', 'nsα', 'nth', 'ntroduction', 'null', 'number', 'numbered', 'numbers', 'numerator', 'numerical', 'numerically', 'numerically12', 'numerics', 'numerische', 'numerous', 'nur', 'nv00', 'nx', 'ny', 'nyi', 'nyquist', 'nǫ', 'nα', 'nν', 'nνn', 'nφ', 'obesity', 'object', 'objection', 'objective', 'objects', 'observation', 'observations', 'observe', 'observed', 'observes', 'observing', 'obstacle', 'obstacles', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'occasional', 'occupy', 'occur', 'occurring', 'occurs', 'oct', 'octad', 'octads', 'octahedron', 'october', 'odbc', 'odd', 'odel', 'odes', 'odlyzko', 'odory', 'of', 'off', 'offer', 'offers', 'office', 'official', 'offs', 'often', 'oh', 'oi', 'old', 'older', 'olof', 'omitted', 'omitting', 'on', 'once', 'onclusions', 'one', 'ones', 'onion', 'onlinelearning', 'only', 'ontario', 'onto', 'open', 'oper', 'operating', 'operation', 'operations', 'operator', 'operators', 'opinion', 'opinions', 'opportunity', 'opposed', 'opposite', 'optimal', 'optimalities', 'optimality', 'optimisation', 'optimization', 'optimizations', 'optimize', 'optimized', 'optimizing', 'optimum', 'option', 'optional', 'options', 'or', 'oral', 'order', 'ordered', 'ordering', 'orderings', 'orders', 'ordinary', 'ordnen', 'org', 'organization', 'organized', 'origin', 'original', 'originally', 'origins', 'orlando', 'orsay', 'orthogonal', 'orthonormal', 'osaka', 'osc', 'oscillating', 'oscillation', 'osher', 'osthus', 'otation', 'other', 'others', 'otherwise', 'otions', 'our', 'ours', 'ourself', 'ourselves', 'out', 'outcome', 'outcomes', 'outer', 'outline', 'outlined', 'outperforms', 'output', 'outputs', 'outside', 'oval', 'ovals', 'over', 'overall', 'overlap', 'overlapping', 'overrelaxation', 'overview', 'overweight', 'own', 'ox', 'ox1', 'oxford', 'ozarow', 'ozbudak', 'p0', 'p00', 'p0gt', 'p1', 'p19', 'p2', 'p2k', 'p2r', 'p3', 'p481', 'p4c', 'p5', 'p6', 'pack', 'packed', 'packet', 'packing', 'packings', 'page', 'pages', 'paid', 'paige', 'pair', 'pairs', 'palek', 'paley', 'palmtop', 'pam', 'pand', 'papadimitriou', 'paper', 'papers', 'paradigm', 'paradigms', 'parallel', 'parallelisms', 'parameter', 'parameters', 'parametric', 'parametrisation', 'parent', 'paris', 'parity', 'park', 'parks', 'parser', 'part', 'partial', 'partially', 'participants', 'particles', 'particular', 'particularly', 'partition', 'partitioned', 'partitioning', 'partitions', 'partly', 'parts', 'pasch', 'pass', 'passed', 'passes', 'path', 'paths', 'pattern', 'patterns', 'paulo', 'payload', 'pde', 'pdes', 'pdf', 'pe', 'pecial', 'peculiar', 'pedagogical', 'peeling', 'pei', 'pem', 'penalizing', 'per', 'perasure', 'perdachi', 'peredaci', 'perfect', 'perform', 'performance', 'performances', 'performed', 'performing', 'perhaps', 'peridachi', 'perimeter', 'periods', 'permutation', 'permutations', 'permutationsgruppen', 'perona', 'personalized', 'persons', 'perspective', 'pet', 'peter', 'peterson', 'petitot', 'pfender', 'pg', 'pgt', 'ph', 'phase', 'phases', 'phd', 'phelps', 'phenomena', 'phenomenon', 'philadelphia', 'philips', 'philos', 'phone', 'photograph', 'photographed', 'phys', 'physica', 'physical', 'physician', 'physicians', 'physics', 'physiology', 'pi', 'pick', 'pickert', 'pictures', 'piece', 'pieces', 'piecewise', 'pij', 'pim', 'pinsker', 'piper', 'piracicaba', 'pishro', 'pixel', 'pixels', 'pk', 'pkij', 'pkji', 'place', 'placement', 'places', 'plain', 'plane', 'planes', 'plants', 'platforms', 'plausible', 'play', 'plays', 'plength', 'pless', 'plet', 'plu', 'plus', 'plusieurs', 'pm', 'pn', 'pn1', 'pn2', 'pny', 'pohang', 'point', 'pointed', 'pointing', 'points', 'pointwise', 'poisson', 'policy', 'poltyrev', 'polylog', 'polynomial', 'polynomials', 'pontiveros', 'poor', 'poorly', 'popular', 'populous', 'porto', 'portugal', 'pose', 'posed', 'positions', 'positive', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'posteriori', 'postmodern', 'postponed', 'postponement', 'potential', 'potentially', 'potenza', 'pottonen', 'poulliat', 'power', 'powers', 'pp', 'ppendix', 'ppn', 'pr', 'pr1', 'prabhakar', 'practical', 'practice', 'practitioners', 'prange', 'precise', 'precisely', 'precision', 'predecessor', 'predetermined', 'preece', 'prefer', 'prefix', 'preliminaries', 'preliminary', 'prepare', 'prepared', 'preprint', 'prescribed', 'presence', 'present', 'presentation', 'presented', 'presently', 'presents', 'preserve', 'preserved', 'preserves', 'press', 'prevailing', 'previous', 'previously', 'prg', 'prg0', 'primary', 'prime', 'primitive', 'principal', 'principle', 'principles', 'print', 'prior', 'priori', 'priority', 'privacy', 'prob', 'proba', 'probab', 'probabilistic', 'probabilities', 'probability', 'probability1', 'probable', 'probably', 'probl', 'problem', 'problematic', 'problems', 'problemy', 'proc', 'procedure', 'procedures', 'proceed', 'proceedings', 'proceeds', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'product', 'products', 'professor', 'professors', 'program', 'programming', 'programs', 'progress', 'proietti', 'project', 'projected', 'projecting', 'projection', 'projective', 'projector', 'projektive', 'promising', 'promotes', 'proof', 'proofs', 'propagation', 'properly', 'properties', 'property', 'proportional', 'proposals', 'propose', 'proposed', 'proposing', 'proposition', 'protect', 'protected', 'protecting', 'protection', 'protects', 'protocol', 'protocols', 'prove', 'proved', 'proven', 'proves', 'provide', 'provided', 'providence', 'provides', 'providing', 'province', 'proving', 'provision', 'ps', 'pseudo', 'pseudocode', 'pt', 'ptf', 'ptij', 'ptrue', 'pu', 'pub', 'public', 'publication', 'publications', 'published', 'publishers', 'publishing', 'pulkit', 'pulse', 'punctured', 'pure', 'pures', 'purpose', 'purposes', 'px', 'px1', 'px2', 'pxi', 'pxk', 'pxn', 'pxy', 'py', 'py1', 'pyx', 'pz', 'pθj', 'pλδ', 'pλδe', 'pπ', 'pτ', 'q0', 'q1', 'q3', 'qap', 'qd', 'qh', 'qht', 'qhtε', 'qi', 'qij', 'qk', 'qm', 'qn', 'qnpr', 'qq', 'qr', 'qt', 'qtr', 'quadrangle', 'quadratic', 'quadratically', 'quadric', 'quadrupelsysteme', 'quadruple', 'quality', 'quant', 'quantite', 'quantitie', 'quantities', 'quantity', 'quantization', 'quantizes', 'quantum', 'quantumly', 'quantumworks', 'quarterly', 'quartic', 'quaternary', 'qubit', 'question', 'questionaires', 'questionnaire', 'questionnaires', 'questions', 'quickly', 'quite', 'qy', 'r0', 'r1', 'r2', 'r24', 'r3', 'r4', 'r5', 'radius', 'radovic', 'rahnavard', 'raises', 'ramsey', 'randgraph', 'randgraph0', 'random', 'randomization', 'randomized', 'randomly', 'randomness', 'range', 'rank', 'rapid', 'rapidly', 'rare', 'rarely', 'rate', 'rates', 'rather', 'ratio', 'rational', 'raton', 'ray', 'rc', 'rc2', 'rd', 're', 'reach', 'reachable', 'reached', 'reaches', 'reaching', 'reachpan', 'readable', 'readapt', 'reader', 'readily', 'reading', 'ready', 'real', 'realization', 'realizations', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reals', 'realworld', 'reason', 'reasonable', 'reasonably', 'reasons', 'reboot', 'recall', 'recalling', 'recapitulate', 'receive', 'received', 'receiver', 'receivers', 'recent', 'recently', 'recherche', 'recognize', 'recognized', 'recognizing', 'recommend', 'reconciliation', 'reconstruction', 'recovered', 'recursion', 'recursive', 'recursively', 'red', 'redefine', 'redesign', 'redesigned', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'reed', 'ref', 'refer', 'referees', 'reference', 'references', 'referral', 'referrals', 'referred', 'referring', 'refers', 'refinability', 'refinable', 'refinement', 'refinements', 'refining', 'reflection', 'reflections', 'reflects', 'refrain', 'regarded', 'regarding', 'regardless', 'regime', 'region', 'regions', 'register', 'regular', 'regularity', 'reidel', 'reine', 'reinterpreted', 'reject', 'rejected', 'rejects', 'rel', 'relabeled', 'relate', 'related', 'relates', 'relating', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relaxed', 'relay', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'relies', 'reliminaries', 'rely', 'relying', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remark', 'remarkable', 'remarks', 'remedied', 'remembering', 'reminds', 'remmers', 'remote', 'removal', 'removed', 'removing', 'renamed', 'rend', 'renders', 'rendic', 'renner', 'reordering', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetition', 'repetitions', 'rephrased', 'replace', 'replaced', 'replaces', 'replacing', 'replication', 'report', 'reported', 'reports', 'represent', 'representation', 'representatives', 'represented', 'representing', 'represents', 'reprint', 'reproducible', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'res', 'research', 'researchers', 'reserved', 'residue', 'residues', 'resilient', 'resolution', 'resolved', 'resource', 'resources', 'resp', 'respect', 'respective', 'respectively', 'rest', 'restoration', 'restrict', 'restricted', 'restriction', 'restrictions', 'result', 'resulted', 'resulting', 'results', 'results13', 'retain', 'retransmission', 'retransmissions', 'retransmitted', 'return', 'returns', 'reuse', 'reused', 'reusing', 'rev', 'revealed', 'reveals', 'reversal', 'reverses', 'reversibility', 'reversible', 'review', 'reviews', 'revised', 'revisit', 'revisited', 'rewarding', 'rewrite', 'rewriting', 'rewritten', 'rg', 'rgb', 'ri', 'ric', 'rich', 'richardson', 'richter', 'richterp', 'rifa', 'right', 'rigorous', 'rij', 'rimoldi', 'ring', 'riordan', 'rise', 'rivosh', 'rj', 'rk', 'rl', 'rm', 'rn', 'rn1', 'rn3', 'rner', 'road', 'robert', 'robot', 'robust', 'robustness', 'rocky', 'rodney', 'rogers', 'roland', 'role', 'roles', 'romanov', 'roofs', 'room', 'root', 'rooted', 'roots', 'rotate', 'rotation', 'roth', 'rothkugel', 'roughly', 'round', 'rounded', 'row', 'rows', 'roy', 'royal', 'rq', 'rs', 'rth', 'rucinski', 'rudolf', 'rule', 'rummel', 'run', 'running', 'runs', 'runtime', 'rw', 'ryser', 'rθ', 'rω', 's0', 's1', 's12', 's2', 's2k', 's3', 'saberi', 'sahai', 'said', 'sake', 'same', 'sample', 'samples', 'sampling', 'san', 'san08', 'sand', 'sanghavi', 'sankaranarayanan', 'santha', 'sao', 'sapiro', 'sarily', 'satellite', 'satisfactory', 'satisfied', 'satisfies', 'satisfy', 'satisfying', 'saturation', 'say', 'saying', 'says', 'scalar', 'scale', 'scaled', 'scales', 'scaling', 'scan', 'scarcely', 'scenario', 'scenarios', 'schalkwijk', 'schalkwijk6', 'scheduling', 'scheme', 'schemes', 'schoenmakers', 'school', 'schu', 'schwartz', 'schwarz', 'sci', 'science', 'sciences', 'scope', 'scratch', 'se', 'search', 'searches', 'sec', 'second', 'secondly', 'secondmoment', 'seconds', 'secret', 'section', 'sections', 'secure', 'security', 'see', 'seek', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'segment', 'segmentation', 'segmentations', 'segmented', 'segments', 'seidel', 'selected', 'selecting', 'selection', 'selects', 'self', 'selfdual', 'sem', 'semakov', 'semidefinite', 'seminar', 'send', 'sending', 'sends', 'sense', 'sensitive', 'sensitivity', 'sent', 'sep', 'separate', 'separated', 'separately', 'separates', 'separation', 'separator', 'sept', 'september', 'sequel', 'sequence', 'sequences', 'sequential', 'sequentially', 'ser', 'series', 'serious', 'serve', 'server', 'services', 'servlet', 'seshadri', 'session', 'set', 'sets', 'settest', 'setting', 'settings', 'setup', 'seven', 'seventeenth', 'several', 'sh', 'shaded', 'shah', 'shalkwijk', 'shall', 'shannon', 'sharing', 'sharp', 'sharper', 'shashi', 'shed', 'shell', 'shells', 'shen', 'shenker', 'shenorr', 'shenvi', 'shift', 'shifted', 'shimamoto', 'shin', 'shiriaev', 'shiromoto', 'shokrollahi', 'short', 'shortened', 'shorter', 'shortest', 'shorthand', 'shortly', 'should', 'show', 'showed', 'showing', 'shown', 'shows', 'shrikhande', 'shutdowns', 'si', 'siam', 'sich', 'side', 'sides', 'sigcomm', 'sign', 'signal', 'signaling', 'signals', 'significance', 'significant', 'significantly', 'similar', 'similarly', 'simonis', 'simple', 'simpler', 'simplest', 'simplex', 'simplicity', 'simplifications', 'simplified', 'simplifies', 'simplify', 'simplifying', 'simply', 'simulating', 'simulation', 'simultaneous', 'simultaneously', 'sin', 'sin2', 'since', 'sinclair', 'singapore', 'singe', 'single', 'singleerror', 'singular', 'situation', 'situations', 'six', 'sixteenth', 'sixty', 'size', 'sizes', 'sk', 'skills', 'skip', 'skw03', 'sl', 'slides', 'slight', 'slightly', 'slim', 'sloane', 'slowly', 'small', 'smaller', 'smallest', 'smooth', 'smoothers', 'smoothing', 'smoothly', 'smoothness', 'sn', 'snapshots', 'snover', 'snr', 'so', 'soc', 'social', 'society', 'socrates', 'soda', 'software', 'sole', 'solids', 'solov', 'solution', 'solutions', 'solve', 'solved', 'solver', 'solvers', 'solving', 'some', 'something', 'sometimes', 'somewhat', 'sons', 'soon', 'sought', 'sounds', 'source', 'sources', 'sov', 'space', 'spacecraft', 'spaced', 'spaces', 'spacing', 'span', 'spanned', 'sparse', 'sparsity', 'spatial', 'spb', 'speak', 'special', 'specialty', 'specific', 'specifically', 'specification', 'specified', 'specifies', 'specify', 'specifying', 'spectral', 'spectrum', 'speed', 'speedup', 'spencer', 'spend', 'sphere', 'spherepacking', 'spheres', 'spherical', 'spielman', 'spiral', 'spite', 'split', 'splitting', 'spoil', 'sporadic', 'springer', 'spruijt', 'spurious', 'sqs', 'square', 'squared', 'squares', 'sr', 'ssc', 'st', 'stabilizer', 'stable', 'stage', 'stages', 'staircasing', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanford', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'static', 'stating', 'stationary', 'statist', 'statistic', 'statistical', 'statistically', 'statistics', 'steal', 'steer', 'stefanos', 'steffen', 'steger', 'stein', 'steiner', 'steinersche', 'stemann', 'step', 'stepping', 'steps', 'sterga', 'sth', 'stichtenoth', 'stick', 'stiffness', 'still', 'stinson', 'stoc', 'stochastic', 'stop', 'stopping', 'stops', 'storage', 'store', 'stored', 'storme', 'stoth', 'straightforward', 'strange', 'strategies', 'strategy', 'straße', 'strengthen', 'strengthening', 'strict', 'strictly', 'string', 'strings', 'strong', 'stronger', 'struct', 'structural', 'structure', 'structures', 'student', 'students', 'studied', 'studies', 'study', 'studying', 'stylized', 'sub', 'subclass', 'subcode', 'subdomains', 'subgradient', 'subgraph', 'subgraphs', 'subgroup', 'subject', 'sublinear', 'submessage', 'submessages', 'submitted', 'subpart', 'subscript', 'subscripts', 'subsection', 'subsections', 'subsections1', 'subsequent', 'subsequently', 'subset', 'subsets', 'subspace', 'subspaces', 'substantial', 'substantially', 'substitute', 'substituting', 'substitution', 'substructures', 'subtler', 'subtracting', 'subword', 'subwords', 'succesive', 'success', 'successful', 'successfully', 'successive', 'successively', 'such', 'sud', 'sudan', 'suen', 'suffices', 'sufficiency', 'sufficient', 'sufficiently', 'suggest', 'suggested', 'suggesting', 'suggests', 'suitable', 'sum', 'summarise', 'summarize', 'summarized', 'summarizes', 'summarizing', 'summary', 'summation', 'summing', 'sup', 'super', 'superimposed', 'superior', 'supn', 'supp', 'suppl', 'support', 'supported', 'supports', 'suppose', 'supposition', 'supq', 'supremum', 'supérieur', 'supσ', 'sur', 'sure', 'surprising', 'surprisingly', 'surrogate', 'survey', 'surveys', 'sussex', 'swap', 'swapped', 'swiercz', 'swiftness', 'swing', 'switch', 'switches', 'switching', 'swoh', 'sym2', 'sym36', 'symbol', 'symbols', 'symmetric', 'symmetrical', 'symmetrized', 'symmetry', 'symp', 'symposium', 'synchronization', 'syst', 'syste', 'system', 'systematic', 'systeme', 'systemindependent', 'systems', 'sz', 'sze04', 'szegedy', 'szkola', 'são', 'sǫα', 'sα', 'sαǫ', 'sβ', 'sν', 'sσ11u1', 'sσ22u2', 'sτ', 't1', 'ta', 'table', 'tables', 'tackle', 'tackles', 'tactical', 'tai', 'tail', 'take', 'taken', 'takes', 'taking', 'talking', 'tan', 'tan2', 'tanabe', 'tang', 'tangent', 'tangmunarunkit', 'tanner', 'taormina', 'taraz', 'target', 'task', 'tcc', 'teach', 'teaching', 'tech', 'technical', 'technicality', 'technically', 'technique', 'techniques', 'technische', 'technologies', 'technology', 'teirlinck', 'tel', 'telatar', 'telecomm', 'telecommunications', 'telephone', 'teletar', 'tell', 'tells', 'temporarily', 'tended', 'tentative', 'term', 'term4', 'terminates', 'terminating', 'terminologies', 'terminology', 'terms', 'ternary', 'test', 'testclass', 'testing', 'testinhtml', 'testmethod', 'testobject', 'tests', 'tetrahedron', 'texas', 'text', 'texts', 'texture', 'tf', 'th', 'than', 'thank', 'thanks', 'thas', 'that', 'the', 'theblank', 'their', 'them', 'theme', 'themselves', 'then', 'theorem', 'theorems', 'theoretic', 'theoretical', 'theories', 'theorists', 'theory', 'there', 'thereby', 'therefore', 'therein', 'these', 'thesis', 'they', 'thick', 'thickness', 'thiel', 'thing', 'think', 'third', 'thirteenth', 'this', 'thomas', 'thompson', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'thoughts', 'three', 'threshold', 'thresholding', 'through', 'throughout', 'throw', 'thus', 'tieta', 'tight', 'tighter', 'tilborg', 'till', 'time', 'times', 'tits', 'tjoelker', 'tl', 'to', 'todd', 'together', 'togonidze', 'tolerance', 'tomasi', 'tomecat', 'tonchev', 'too', 'took', 'tool', 'tools', 'top', 'topic', 'topics', 'topological', 'topologies', 'topology', 'torus', 'total', 'touch', 'towards', 'towsley', 'tpx', 'tpy', 'tr', 'trace', 'track', 'tracked', 'trade', 'tradeoffs', 'tradition', 'traditional', 'training', 'trans', 'transactions', 'transferred', 'transformations', 'transition', 'transitions', 'transitive', 'transitively', 'transitiven', 'transitivity', 'translated', 'translates', 'translating', 'translation', 'transmission', 'transmissions', 'transmit', 'transmits', 'transmitted', 'transmitter', 'transmitting', 'transparent', 'treat', 'treated', 'treating', 'treatment', 'treats', 'tree', 'trial', 'triangle', 'trianglefree', 'triangles', 'triangulation', 'trick', 'trier', 'tries', 'triple', 'trivial', 'trivially', 'trois', 'trott', 'troubles', 'true', 'truncated', 'try', 'trying', 'tsai', 'tschel', 'tse', 'tte', 'tu', 'tucker', 'tude', 'tue', 'tul08', 'tulsi', 'tuned', 'tuple', 'tuples', 'turbo', 'turn', 'turns', 'tutor', 'tutorial', 'tutorielle', 'tutoring', 'tuy', 'tuyls', 'tv', 'tvi', 'tw', 'twelve', 'twentieth', 'twenty', 'twentyeighth', 'twice', 'twin', 'twisted', 'two', 'tx', 'type', 'types', 'typical', 'typically', 'tǫ', 'tδ', 'tσn', 'u0', 'u1', 'u12', 'u1θ', 'u2', 'u2θ', 'u3', 'ubiquitous', 'uc', 'ucla', 'uep', 'ugly', 'uh', 'uhi', 'ui', 'ui2', 'uk', 'ultimately', 'un', 'un2', 'unable', 'unacceptable', 'unaffected', 'unam', 'unambiguous', 'unavoidable', 'unbiased', 'unbounded', 'uncanceled', 'unchanged', 'unclear', 'unconditional', 'unconstrained', 'uncoordinated', 'under', 'underline', 'underlying', 'understand', 'understanding', 'understood', 'undesirable', 'undetected', 'undirected', 'undo', 'unequal', 'unexpected', 'unfortunately', 'uniform', 'uniformity', 'uniformly', 'unify', 'unimodular', 'unimportant', 'union', 'unique', 'uniquely', 'uniqueness', 'unit', 'unitaries', 'unitary', 'units', 'univ', 'universal', 'universally', 'universita', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unlimited', 'unnecessary', 'unnormalized', 'unnoticed', 'unpublished', 'unrealistic', 'unreliable', 'unresolved', 'unrestricted', 'unsatisfactory', 'unsolved', 'unstable', 'unsymmetric', 'until', 'unused', 'unusually', 'up', 'update', 'updated', 'updating', 'upon', 'upper', 'urbanke', 'url', 'us', 'usa', 'use', 'used', 'useful', 'usefulness', 'user', 'uses', 'using', 'ussr', 'usual', 'usually', 'ut', 'utility', 'uture', 'uwaterloo', 'uy', 'v0', 'v1', 'v2', 'va', 'valente', 'valid', 'validate', 'vallentin', 'valuable', 'value', 'valued', 'values', 'van', 'vanish', 'vanishes', 'vanishing', 'vanishingly', 'vanlehn', 'vanstone', 'var', 'variable', 'variables', 'variance', 'variant', 'variation', 'variational', 'variations', 'varies', 'varieties', 'variety', 'various', 'vary', 'varying', 'vasic', 'vazirani', 'vector', 'vectorial', 'vectors', 'vegas', 'verbindung', 'verbitskiy', 'verification', 'verified', 'verify', 'verlag', 'version', 'versions', 'versus', 'vertex', 'vertices', 'very', 'vese', 'vh', 'vi', 'via', 'viable', 'view', 'viewed', 'vii', 'viii', 'villanueva', 'violate', 'virtual', 'vishwanath', 'visible', 'vision', 'visiting', 'visual', 'visualization', 'visualize', 'visually', 'vivid', 'vj', 'vn', 'vol', 'volume', 'von', 'vontobel', 'vorkommen', 'voyager', 'vr', 'vs', 'vt', 'vu', 'vub', 'vy', 'vτ', 'w0', 'w1', 'w3c', 'waerden', 'walk', 'walker', 'walks', 'want', 'wanted', 'ward', 'warnke', 'was', 'wassermann', 'wasteful', 'waterloo', 'watrous', 'watson', 'watts', 'wavelenghts', 'wavelength', 'wavelengths', 'way', 'ways', 'we', 'weak', 'weakening', 'weaker', 'weakly', 'web', 'webapplication', 'week', 'weeks', 'weight', 'weighted', 'weights', 'weissman', 'welche', 'weldon', 'well', 'were', 'weyl', 'whaley', 'what', 'whatever', 'when', 'whenever', 'where', 'whereas', 'whereby', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whose', 'why', 'wi', 'wide', 'widely', 'widen', 'wider', 'widlund', 'width', 'wielandt', 'wigger', 'wiley', 'will', 'willinger', 'wilson', 'win', 'winkler', 'wirel', 'wireless', 'wise', 'wish', 'with', 'within', 'without', 'witt', 'wj', 'wk', 'wn', 'wolf', 'wolfovitz', 'wolfson', 'wonder', 'woolhouse', 'word', 'words', 'work', 'working', 'works', 'workshop', 'world', 'wormald', 'worst', 'worth', 'worthwhile', 'would', 'wp', 'write', 'writing', 'written', 'wrong', 'www', 'wy', 'wyy', 'wα', 'wαz', 'x0', 'x1', 'x12', 'x2', 'x22', 'x6', 'xa', 'xd', 'xfl', 'xfu', 'xh', 'xi', 'xi2', 'xihx', 'xj', 'xk', 'xml', 'xmlbasierte', 'xn', 'xn1', 'xn2', 'xp', 'xpy', 'xr', 'xt', 'xu', 'xv', 'xx', 'xxx', 'xyi', 'y0', 'y0n', 'y1', 'y1i', 'y1n', 'y1n1', 'y2', 'y6', 'yamamoto', 'yamamotoitoh', 'yan', 'yann', 'years', 'yes', 'yet', 'yi', 'yield', 'yielding', 'yields', 'yihy', 'yis', 'yj', 'yk', 'yn', 'york', 'ypr', 'ypy', 'yt', 'yu', 'yx', 'z0', 'z0n', 'z1', 'z4', 'zahl', 'zaitsev', 'zaragoza', 'zenios', 'zero', 'zhao', 'zheng', 'zi', 'ziegler', 'zigangirov', 'zihz', 'zinov', 'zinoviev', 'zn', 'zn1', 'zu', 'zwei', 'zyablov', 'µ1', 'µ2', 'µi', 'µihµ', 'µm', 'µz', 'ǫa', 'ǫauk', 'ǫcloseness', 'ǫh', 'ǫn', 'ǫsmooth', 'ǫtypical', 'ǫua', 'ǫδta', 'α1', 'α2', 'α21', 'αd', 'αi', 'αil', 'αj', 'αk', 'αtv', 'αv', 'αx', 'αz', 'αβxx', 'αη', 'αηt', 'αθ', 'αθ1', 'βx', 'γdmax', 'γk', 'γn', 'γn1', 'γp', 'γq', 'γr', 'γℓn', 'δ0', 'δ02', 'δ2', 'δj', 'δj2', 'δk', 'δkk', 'δn', 'δt', 'δtt', 'δλ', 'δℓ', 'ζj', 'ζn', 'ηi', 'ηil', 'ηn', 'ηt', 'θ0', 'θ1', 'θ12', 'θ2', 'θ2k2', 'θa', 'θi', 'θihθ', 'θj', 'θj2', 'θjj', 'θm', 'θn', 'θp', 'θr', 'θs', 'κ1', 'κk', 'κn', 'λ1', 'λ2', 'λf', 'λi', 'λj', 'λm', 'λmax', 'λn', 'λs', 'λu', 'λw', 'λµ', 'λǫ', 'λγk', 'λδ', 'λδe', 'ν1', 'ν2', 'νez', 'νj', 'νj2', 'νn', 'νδk', 'ξj', 'π6', 'πi', 'πk', 'πp', 'πt', 'πx', 'πy', 'πψ', 'πψi', 'ρ0', 'ρα', 'σ0', 'σ02', 'σ1', 'σ12', 'σ2', 'σ22', 'σi', 'σi2', 'σn', 'σn1', 'σn2', 'σz2', 'τ1', 'τ2', 'τ24', 'τ3', 'τ4', 'τ8', 'τi', 'τj', 'τn', 'τδ', 'φ0', 'φ1', 'φ2', 'φe0', 'φi', 'φj', 'φm', 'φmax', 'φq', 'φs', 'φx', 'φy', 'φz', 'ψi', 'ψihψ', 'ψik2', 'ω2', 'ωi', 'ωm2', 'ℓ2', 'ℓn']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [2 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 1 0 ... 1 0 0]\n",
            " [4 3 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMSc6xtquDUB",
        "outputId": "45844032-409d-49ec-a385-1d1582716c59"
      },
      "source": [
        "data_df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
        "data_df.index = archivo\n",
        "print (data_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               00  000  0010117  003  0044  0045  ...  ψik2  ω2  ωi  ωm2  ℓ2  ℓn\n",
            "0704.3504.txt   0    0        0    0     0     0  ...     0   0   0    0   0   0\n",
            "0706.1402.txt   0    0        0    0     0     0  ...     0   0   0    0   0   0\n",
            "0710.0736.txt   2    0        0    0     2     0  ...     0   0   3    0   0   0\n",
            "0803.2570.txt   0    0        0    0     0     0  ...     0   0   0    0   0  16\n",
            "0808.0084.txt   0    0        1    1     0     0  ...     1   0   0    0   0   0\n",
            "0811.1254.txt   1    1        0    0     0     0  ...     0   1   0    1   0   0\n",
            "0811.2853.txt   4    3        0    0     0     1  ...     0   0   0    0   0   0\n",
            "0812.2709.txt   0    0        0    0     0     0  ...     0   0   0    0   1   0\n",
            "\n",
            "[8 rows x 6069 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTZmF9XM5bKI",
        "outputId": "491e8ecd-b5a1-459b-c668-1c26292dce6a"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = df['contenido'].tolist()\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00', '000', '0010117', '003', '0044', '0045', '007', '0084v1', '01', '0156', '015848', '02', '025', '027', '03', '0301043', '0309430', '04', '0403263', '042312', '05', '0512258', '052307', '06', '0608426', '0610143', '0610143v2', '0610146', '0610146v2', '07', '0704', '0710', '0736v1', '08', '0803', '0808', '0811', '0812', '09', '0i', '0ih0', '0kwα', '10', '100', '101', '1010001', '1016', '102', '103', '104', '105', '106', '10623', '107', '108', '1085', '109', '1095', '1099', '11', '110', '1101000', '1108', '111', '1117', '112', '1123', '113', '114', '115', '116', '117', '117543', '118', '119', '12', '120', '1201', '121', '1212', '1216698', '122', '1229', '123', '123000', '1234', '1238', '124', '1245', '1248', '125', '1254v1', '126', '1261', '1267', '1268', '1269', '127', '1271766600', '1273', '128', '1288', '129', '12s0', '13', '130', '1302', '131', '132', '133', '134', '1343', '135', '1359', '136', '1365', '137', '138', '1382', '139', '1391', '1393', '1397', '14', '140', '141', '1416', '142', '1420', '1423', '1426', '143', '1435', '1438', '144', '1441', '145', '146', '147', '148', '149', '1496770', '1496833', '14th', '15', '150', '151', '152', '1523', '1527', '1554140', '1567', '157', '1584', '159', '16', '160', '161', '16242', '1632', '1641', '1650', '166', '167', '168', '1682', '169', '1694', '17', '172', '1737', '1744', '175', '1764', '1770', '179', '18', '180', '181', '182', '183', '1839', '184', '185', '1853', '1860', '1861', '1865', '187', '1873', '1886', '189', '19', '190', '190680', '1933', '1937', '1938', '194', '1940', '1948', '1949', '195', '1950', '1952', '1953', '1954', '1956', '1959', '196', '1960', '1961', '1963', '1964', '1965', '196560', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '199', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '1i', '1ih1', '1n', '1st', '1t', '1x', '20', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2009a', '2009b', '2010', '2011', '2013', '2014', '2015', '2017', '202204', '205', '206', '207', '209685', '21', '210', '211', '212', '214', '215', '216', '219', '22', '220', '221', '221229', '2231', '2235', '224', '225', '228', '229', '23', '232', '233', '237', '239', '239329029060', '24', '240', '241', '24140500956', '243', '2438973', '2477', '25', '2504', '251', '253', '256', '2561', '257', '2570v4', '2595', '26', '261', '262', '264', '265', '266', '267', '269', '27', '2702', '2709v4', '2714', '272', '275', '276', '277', '28', '283', '2853v2', '287', '288', '289', '2893', '29', '291', '295', '2957388', '296', '298', '2a', '2a0', '2a20', '2c1', '2d', '2e', '2e2m', '2e33', '2gn', '2iθj', '2k', '2m', '2n', '2nd', '2p', '2pe', '2pi', '2pn', '2pns', '2q', '2r', '2t', '2w', '2ǫ', '2β', '2γn1', '2δ', '2δj2', '2ε', '2η', '2ηn', '2π', '2πk', '2πψ', '2φ', '2ℓ', '30', '301', '305', '3060', '3063', '307', '3074', '3079', '3080', '309', '31', '310', '311', '313', '314', '315', '316', '318', '319', '32', '323', '325', '328', '33', '3329', '333', '3333', '334', '335', '336', '337', '3378', '339', '33rd', '34', '342', '34337160', '344', '348', '349', '35', '3504v1', '354', '36', '36176', '362', '363', '364', '366', '367', '37', '377', '3788', '379', '38', '381', '384', '387', '388', '39', '391', '396', '397', '39th', '3g1', '3k', '3kα', '3pprovided', '3qd', '3rd', '40', '400', '402', '406', '407', '408', '4096', '41', '417', '419', '41st', '42', '421', '422', '423', '425', '43', '435', '438', '44', '440', '449', '449820', '45', '450', '453', '454', '456', '45th', '46', '465', '468', '47', '476', '48', '487', '489', '49', '490', '491', '4978', '4a', '4a2', '4a20', '4anα', '4b', '4c2', '4htε', '4n', '4n1', '4pi', '4th', '4ǫ', '4λ2', '4νj2', '4φ', '4φ1', '4ℓ', '50', '50456', '506', '507', '508', '51', '518', '52', '522', '526', '53', '539', '5390', '54', '543', '55', '5577', '56', '566', '57', '575', '576', '577', '58', '581', '584', '59', '5963', '5a', '5b', '5th', '60', '600', '607', '61', '610814', '611', '613', '62', '623', '6279', '629', '63', '633', '639', '64', '640', '644', '647', '65', '656', '657', '66', '6648', '667', '67', '671', '677', '68', '682', '683', '685', '687', '689', '69', '690', '692', '6p', '70', '700', '701', '702', '707', '708', '71', '71307600', '714', '72', '729', '73', '733', '74', '747', '749', '749999640', '75', '753', '759', '76', '77', '771', '773', '78', '789', '78α1', '79', '80', '803', '807', '81', '819', '82', '824', '827', '828', '83', '831', '84', '845', '85', '8580', '86', '860', '864', '866876', '87', '873', '873450', '88', '880', '883', '889', '89', '8a20', '90', '91', '910', '91405', '916', '92', '928', '93', '94', '94305', '945', '949', '95', '9511026', '96', '963', '97', '98', '982', '99', '9rf', 'a0', 'a1', 'a11', 'a12', 'a15', 'a16', 'a2', 'a20', 'a21', 'a23', 'a2j', 'a3', 'a4', 'a5', 'a7', 'a8', 'aa05', 'aakv01', 'aaronson', 'aat', 'abelian', 'aber', 'abh', 'abilities', 'ability', 'able', 'abn', 'about', 'above', 'abovementioned', 'abs', 'absence', 'absent', 'absorbed', 'absorption', 'abstract', 'abundance', 'abuse', 'ac', 'acad', 'academic', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepts', 'access', 'accessed', 'accessible', 'accommodate', 'accomplished', 'according', 'account', 'accounts', 'accumulated', 'accuracy', 'achievability', 'achievable', 'achieve', 'achieved', 'achievements', 'achieves', 'achieving', 'acknowledge', 'acknowledged', 'acknowledges', 'acknowledgment', 'acknowledgments', 'acm', 'acquired', 'across', 'act', 'acta', 'acting', 'action', 'active', 'actively', 'actual', 'actually', 'ad', 'adapt', 'adaptation', 'adapted', 'adapting', 'adaptive', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adequate', 'adiposity', 'adjacency', 'adjacent', 'adjoin', 'adjusted', 'admit', 'admits', 'adolesc', 'adolescent', 'adopt', 'adriano', 'ads', 'adv', 'advance', 'advanced', 'advances', 'advancing', 'advantage', 'advantages', 'ae1', 'aegean', 'aep', 'affected', 'affiliations', 'affine', 'affirmative', 'afford', 'aforementioned', 'afosr', 'afresh', 'after', 'ag', 'again', 'against', 'agents', 'ages', 'agl', 'ago', 'agreed', 'aharonov', 'ahlswede', 'ai', 'aict', 'aigner', 'aij', 'ailath', 'aim', 'aimed', 'aims', 'ain', 'aj', 'ak', 'akj', 'akr05', 'al', 'alarm', 'alarms', 'albanese', 'albeit', 'albenese', 'albert', 'ale', 'alert', 'alessandro', 'algebr', 'algebra', 'algebraic', 'algebraically', 'algebras', 'algoqp', 'algorithm', 'algorithmic', 'algorithmica', 'algorithms', 'all', 'allen', 'allencahn', 'allerton', 'allgemeine', 'allocate', 'allocated', 'allocating', 'allocation', 'allow', 'allowable', 'allowed', 'allowing', 'allows', 'almost', 'alon', 'alone', 'along', 'alphabet', 'alphabets', 'already', 'also', 'altered', 'alternate', 'alternating', 'alternative', 'alternatively', 'although', 'always', 'am', 'amb03', 'amb04', 'ambainis', 'amer', 'american', 'amin', 'among', 'amongst', 'amount', 'amplification', 'amplified', 'amplifying', 'amplitude', 'amplitudes', 'amraoui', 'ams', 'amsterdam', 'an', 'anal', 'analog', 'analogies', 'analogon', 'analogons', 'analogous', 'analogs', 'analogue', 'analogues', 'analyses', 'analysis', 'analytical', 'analyzable', 'analyze', 'analyzed', 'analyzes', 'analyzing', 'anayak', 'and', 'andes', 'andrea', 'angew', 'angle', 'anisotropic', 'anita', 'anita2', 'ann', 'annotation', 'annotations', 'announc', 'annual', 'anomalies', 'anonymous', 'another', 'anr', 'answer', 'answered', 'answering', 'answers', 'anti', 'anticodes', 'antiphase', 'antonio', 'any', 'anytime', 'anyway', 'anα', 'apache', 'apacity', 'apart', 'apparent', 'appear', 'appeared', 'appearing', 'appears', 'appendix', 'appl', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approx', 'approximate', 'approximated', 'approximately', 'approximating', 'approximation', 'approximations', 'apr', 'april', 'arbitrarily', 'arbitrary', 'arch', 'architecture', 'architectures', 'architektur', 'are', 'area', 'areas', 'argue', 'argued', 'argument', 'arguments', 'arises', 'arising', 'arithmetic', 'aro', 'arose', 'around', 'arq', 'arrange', 'arranged', 'array', 'arrive', 'art', 'article', 'articles', 'artificially', 'arxiv', 'ary', 'as', 'aschbacher', 'ashwin', 'asiacrypt', 'aside', 'ask', 'asked', 'asks', 'aspect', 'aspects', 'assertion', 'assertions', 'assign', 'assigned', 'assigning', 'assignment', 'assigns', 'assmus', 'assmusmattson', 'assoc', 'associate', 'associated', 'associates', 'association', 'associations', 'associative', 'associe', 'assume', 'assumed', 'assumes', 'assuming', 'assumption', 'assumptions', 'assured', 'astronomy', 'asymptotic', 'asymptotically', 'at', 'att', 'attack', 'attained', 'attains', 'attempted', 'attempts', 'attend', 'attended', 'attention', 'attenuated', 'attenuation', 'attracted', 'attribute', 'aufgabe', 'aug', 'augmenting', 'august', 'aujol', 'aut', 'authentication', 'author', 'authoring', 'authors', 'automatic', 'automorphism', 'automorphisms', 'autonomously', 'aux', 'auxiliary', 'available', 'average', 'averaged', 'averages', 'averaging', 'avgustinovich', 'avoid', 'avoided', 'avoiding', 'avoids', 'awards', 'away', 'awgn', 'ax', 'axillary', 'aydinian', 'aθ', 'aθ0', 'aθj', 'b1', 'b2', 'b3', 'b4', 'bach', 'bachoc', 'back', 'background', 'backward', 'bad', 'baghdady', 'bajaj', 'balaji', 'balance', 'balanced', 'ball', 'balls', 'band', 'bandlimited', 'bandwidth', 'bannai', 'bar', 'bare', 'barely', 'barron', 'barıs', 'based', 'basel', 'basic', 'basically', 'basis', 'bassalygo', 'bayati', 'bayes', 'bb', 'be', 'bea', 'beautiful', 'became', 'because', 'becausepof', 'become', 'becomes', 'been', 'before', 'beginning', 'begun', 'behave', 'behaves', 'behavior', 'behind', 'being', 'believe', 'bell', 'belong', 'belonging', 'belongs', 'below', 'bence', 'bender', 'benefit', 'benefitted', 'benes', 'benjamin', 'ber', 'berkeley', 'berlekamp', 'berlin', 'bernoulli', 'berry', 'besides', 'best', 'bet', 'beth', 'betten', 'better', 'between', 'beutelspacher', 'beyond', 'bezdek', 'bi', 'bias', 'bias3', 'biased', 'biasπrg', 'biasπtf', 'big', 'bij', 'bijective', 'bility', 'billiard', 'bin', 'binary', 'biological', 'bipartite', 'biplane', 'biprandgraph', 'birkha', 'birkhauser', 'birkhoff', 'birth', 'bit', 'bits', 'bius', 'bizarre', 'bj', 'bjelakovic', 'bk', 'black', 'blake', 'blanc', 'blanchet', 'blind', 'blitzstein', 'block', 'blocklength', 'blocks', 'blomer', 'blowey', 'blowing', 'blows', 'blue', 'blur', 'bn', 'bn1', 'bob', 'boca', 'bohman', 'bold', 'bolloba', 'bonnecaze', 'book', 'boolean', 'boost', 'boosting', 'bootstrap', 'borade', 'bordered', 'boris', 'borrowing', 'bose', 'boston', 'both', 'bottle', 'boulders', 'bound', 'boundaries', 'boundary', 'bounded', 'bounding', 'bounds', 'box', 'boyarinov', 'brackets', 'bramoulle', 'branches', 'branching', 'braun', 'brazil', 'brazilian', 'brazilians', 'break', 'breaks', 'breiman', 'brest', 'brevity', 'brian', 'brief', 'briefly', 'brightness', 'brighton', 'britz', 'broad', 'broadband', 'broadcast', 'broader', 'broken', 'bross', 'brothers', 'brouwer', 'brown', 'browser', 'bruck', 'brust', 'bs', 'bsc', 'bu', 'buekenhout', 'buffalo', 'buhrman', 'building', 'builds', 'built', 'burlington', 'burnashev', 'business', 'but', 'bv', 'bx', 'by', 'by10', 'by13', 'by5', 'byers', 'bǫ', 'c0', 'c1', 'c2', 'c2r', 'c3', 'ca', 'cahn', 'calculate', 'calculated', 'calculates', 'calculating', 'calculation', 'calculations', 'calderbank', 'california', 'call', 'called', 'calls', 'calm', 'cam', 'cambridge', 'cameron', 'camion', 'can', 'canad', 'canada', 'cancel', 'canceled', 'candidate', 'candidates', 'canfield', 'cannot', 'canonical', 'capability', 'capable', 'capacities', 'capacity', 'capital', 'captured', 'captures', 'capturing', 'carathe', 'cardinality', 'care', 'carefully', 'carla', 'carlo', 'carmichael', 'carried', 'carus', 'case', 'caselles', 'cases', 'casual', 'catalogue', 'catalyst', 'category', 'causal', 'cause', 'caused', 'cavities', 'cavity', 'cb', 'cbt', 'ccf', 'ce', 'cea', 'cei', 'celebrated', 'cell', 'cemm98', 'census', 'center', 'centered', 'centers', 'central', 'centre', 'century', 'certain', 'certainly', 'certainty', 'cet', 'cf', 'cfm', 'cg04', 'ch', 'chain', 'chains', 'chalkwijk', 'challenge', 'challenges', 'challenging', 'chalupecky', 'chan', 'chance', 'chandrasekhar', 'change', 'changes', 'changing', 'channel', 'channels', 'chap', 'chapt', 'chapter', 'character', 'characteristic', 'characterizations', 'characterize', 'characterized', 'characterizing', 'chart', 'chebyshev', 'check', 'checking', 'checks', 'cheme', 'chen', 'cheong', 'chequered', 'chernoff', 'childs', 'chklovskii', 'choice', 'choose', 'chooses', 'choosing', 'chose', 'chosen', 'chou', 'chromaticity', 'chuang', 'chung', 'ci', 'cifar', 'cij', 'cijt', 'cinq', 'circles', 'circuit', 'circuits', 'circulant', 'citation', 'cj', 'ck', 'cker', 'claim', 'claims', 'clarendon', 'clarification', 'clarity', 'class', 'classes', 'classes_path', 'classic', 'classical', 'classification', 'classified', 'classify', 'classifying', 'classpara', 'classroom', 'clean', 'clear', 'clearly', 'cleve', 'clever', 'client', 'close', 'closely', 'closeness', 'closer', 'clouds', 'cluster', 'clusters', 'cmaia', 'cmmi', 'cn', 'cn2', 'cnrs', 'co', 'coached', 'coarse', 'coarsen', 'coarsening', 'coarsest', 'cobenge', 'code', 'codebook', 'coded', 'codes', 'codeword', 'codewords', 'coding', 'coe', 'coefficient', 'coefficients', 'coexist', 'cohen', 'cohn', 'coincides', 'coins', 'colbourn', 'coleman', 'collaborative', 'collected', 'collection', 'collectively', 'collinear', 'color', 'colour', 'colours', 'column', 'columns', 'com', 'com2', 'comb', 'combin', 'combination', 'combinatorial', 'combinatorially', 'combinatorica', 'combinatorics', 'combinatorische', 'combine', 'combined', 'combines', 'combining', 'combo', 'come', 'comes', 'comfortable', 'comlab', 'comm', 'commented', 'comments', 'commission', 'common', 'commun', 'communicate', 'communicated', 'communicating', 'communication', 'communications', 'community', 'commutativity', 'commute', 'comp', 'compact', 'comparable', 'compare', 'compared', 'comparing', 'comparison', 'complement', 'complementary', 'complete', 'completely', 'completeness', 'completes', 'complex', 'complexities', 'complexity', 'complicate', 'complicated', 'complication', 'component', 'components', 'composed', 'composite', 'composition', 'compressing', 'compression', 'compromised', 'comput', 'computable', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computerbased', 'computers', 'computes', 'computing', 'concatenation', 'concave', 'concavity', 'concede', 'concentrate', 'concentrated', 'concentration', 'concentrations', 'concentric', 'concept', 'concepts', 'conceptual', 'conceptually', 'concerned', 'concerning', 'concise', 'conclude', 'concluded', 'concludes', 'conclusion', 'conclusions', 'concrete', 'condition', 'conditional', 'conditioned', 'conditioning', 'conditions', 'conducted', 'conf', 'conference', 'confidence', 'configuration', 'configurations', 'confirmation', 'confirmed', 'confusion', 'congress', 'conj', 'conjecture', 'conjugate', 'conjugates', 'conjunction', 'connect', 'connected', 'connecting', 'connection', 'connections', 'connects', 'consequence', 'consequences', 'consequently', 'consider', 'considerable', 'considerably', 'consideration', 'considerations', 'considered', 'considering', 'considers', 'consisting', 'consists', 'consolidated', 'constant', 'constantly', 'constants', 'constellation', 'constituents', 'constrain', 'constrained', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'constructions', 'constructors', 'contain', 'contained', 'containing', 'contains', 'contemporary', 'content', 'context', 'contexts', 'contingency', 'continually', 'continue', 'continues', 'continuous', 'contour', 'contours', 'contradicting', 'contradiction', 'contradicts', 'contrary', 'contrast', 'contrasts', 'contributed', 'contribution', 'contributions', 'control', 'controlled', 'controlling', 'controversy', 'convenience', 'convenient', 'convention', 'conventional', 'conventionally', 'converge', 'convergence', 'converges', 'converse', 'conversely', 'converses', 'conversions', 'convex', 'convexity', 'convey', 'conveyed', 'convolve', 'convolving', 'conway', 'cooperative', 'coordinate', 'coordinates', 'coordination', 'copies', 'coprime', 'copy', 'copyright', 'core', 'corners', 'corollary', 'correct', 'corrected', 'correcting', 'correction', 'corrections', 'correctly', 'correlated', 'correlation', 'corresp', 'correspond', 'correspondence', 'corresponding', 'corresponds', 'corrollary', 'cos', 'cos2', 'cost', 'costly', 'costs', 'cot', 'cot2', 'could', 'count', 'countably', 'counted', 'counterexample', 'counting', 'counts', 'course', 'cover', 'covered', 'covering', 'covers', 'cq', 'cr', 'craig', 'crc', 'create', 'created', 'creates', 'creating', 'creation', 'criterion', 'critical', 'crucial', 'crude', 'cryptographic', 'cryptography', 'cs', 'csisza', 'cspecial', 'ct', 'cube', 'cuff', 'culture', 'cumbersome', 'curiously', 'current', 'curtis', 'curvature', 'curve', 'curves', 'customary', 'cv', 'cvetkovic', 'cw', 'cx', 'cx0', 'cycle', 'cycles', 'cycles2', 'cyclic', 'd0', 'd1', 'd2', 'd20', 'd21', 'd2i', 'd4', 'd8', 'damgn', 'danger', 'daniel', 'darpa', 'das', 'dass', 'data', 'database', 'david', 'days', 'db', 'de', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dec', 'decades', 'decay', 'decaying', 'decays', 'december', 'decide', 'decided', 'decides', 'decision', 'decisions', 'declare', 'declared', 'declares', 'declercq', 'decode', 'decoded', 'decoder', 'decodes', 'decoding', 'decomposes', 'decomposition', 'decrease', 'decreased', 'decreases', 'decreasing', 'dedicata', 'deduce', 'deep', 'deepened', 'deeper', 'defer', 'deferring', 'define', 'defined', 'defined6', 'defines', 'defining', 'definition', 'definitions', 'deg', 'degenerate', 'degree', 'degrees', 'degv', 'delay', 'delays', 'delete', 'deleting', 'delivering', 'delsarte', 'delving', 'demand', 'demanded', 'demands', 'dembowski', 'demonstrate', 'demonstrated', 'demonstrates', 'denoised', 'denoising', 'denominator', 'denote', 'denoted', 'denotes', 'densest', 'densities', 'density', 'department', 'departments', 'departure', 'depend', 'dependant', 'dependencies', 'dependent', 'depending', 'depends', 'depicting', 'dept', 'depth', 'der', 'derivation', 'derivative', 'derive', 'derived', 'deriving', 'des', 'descendant', 'descendants', 'descent', 'describe', 'described', 'describes', 'description', 'design', 'designed', 'designing', 'designs', 'desirable', 'desire', 'desired', 'despite', 'destroy', 'det', 'detail', 'detailed', 'details', 'detect', 'detected', 'detecting', 'detection', 'detections', 'determinant', 'determination', 'determine', 'determined', 'determines', 'determining', 'deterministic', 'deutsche', 'devedzic', 'develop', 'developed', 'developing', 'development', 'develops', 'deviation', 'deviations', 'device', 'devices', 'devised', 'devoted', 'df', 'dfg', 'di', 'diaconis', 'diag', 'diagonal', 'diagram', 'diam', 'diameter', 'did', 'die', 'differ', 'difference', 'differences', 'different', 'differential', 'differentiate', 'differently', 'differing', 'differs', 'difficult', 'difficulties', 'diffuse', 'diffusion', 'diggavi', 'digital', 'dij', 'dimension', 'dimensional', 'dimensions', 'dinitz', 'diploma', 'dirac', 'direct', 'directed', 'direction', 'directional', 'directions', 'directives', 'directly', 'directory', 'disadvantages', 'disappointing', 'discarded', 'discipline', 'disciplines', 'disconnect', 'discontinued', 'discontinuities', 'discontinuous', 'discover', 'discovered', 'discovery', 'discrete', 'discretisation', 'discretise', 'discriminant', 'discrimination', 'discuss', 'discussed', 'discusses', 'discussing', 'discussion', 'discussions', 'disjoint', 'diskretn', 'displays', 'diss', 'disseminate', 'dissemination', 'dissertation', 'distance', 'distances', 'distant', 'distinct', 'distinction', 'distinctness', 'distinguish', 'distinguishing', 'distortion', 'distributed', 'distribution', 'distributions', 'distributions12', 'divergence', 'diversity', 'divide', 'divided', 'divisible', 'dixon', 'dl', 'dmax', 'dmc', 'dmcs', 'dmv', 'dn', 'dn1', 'do', 'doc', 'dodecad', 'dodecads', 'dodecahedron', 'does', 'doesn', 'doi', 'doing', 'doklady', 'dom', 'domain', 'domains', 'dominate', 'dominated', 'dominating', 'done', 'doob', 'dordrecht', 'dot', 'double', 'doublecounting', 'doubles', 'doubly', 'dover', 'down', 'draft', 'draper', 'drawback', 'drawn', 'dreien', 'dreizehn', 'driven', 'driver', 'drives', 'driving', 'drop', 'drs', 'ds', 'dt', 'dtd', 'dual', 'due', 'dueck', 'duff', 'duration', 'durations', 'during', 'dwispc8', 'dx', 'dynamic', 'dynamically', 'dynamics', 'dynkin', 'dz', 'dθ', 'dσ', 'dℓ', 'e1', 'e2', 'e2iθj', 'e2t', 'e4', 'e8', 'ea', 'each', 'earlier', 'early', 'ease', 'easier', 'easiest', 'easily', 'easy', 'eb', 'ebenen', 'ebf', 'ebij', 'ebits', 'eccc', 'econometrics', 'economic', 'economics', 'ed', 'edge', 'edges', 'edited', 'edition', 'editor', 'editors', 'edmonds', 'eds', 'edu', 'education', 'edward', 'ee', 'eecs', 'eedback', 'efa', 'efaf', 'efal', 'efau', 'eferences', 'effect', 'effective', 'effectively', 'effects', 'efficiency', 'efficient', 'efficiently', 'effort', 'efron', 'eh', 'ei', 'ei1', 'eigenbasis', 'eigenphase', 'eigenphases', 'eigenschaft', 'eigenspace', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eigenvectors', 'eight', 'eighth', 'eindhoven', 'eine', 'einer', 'either', 'eiα', 'eiαj', 'eiθj', 'ek', 'ek00', 'ekert', 'el', 'elaborate', 'elaborates', 'electrical', 'electron', 'electronic', 'electronically', 'electronics', 'elegant', 'element', 'elementary', 'elemente', 'elementen', 'elements', 'elias', 'eliminate', 'eliminating', 'elkies', 'elliott', 'elliptic', 'else', 'embedding', 'emd', 'emergence', 'emergency', 'emerging', 'emphasis', 'emphasize', 'emphasizes', 'emphasizing', 'empirical', 'empirically', 'employed', 'employs', 'emprically', 'empty', 'emre', 'en', 'enabled', 'enables', 'enc', 'encapsulate', 'encoded', 'encoder', 'encoding', 'encourage', 'encouragement', 'encouraging', 'encyclopedia', 'encyclopedias', 'end', 'endlicher', 'endogenous', 'endpoint', 'ends', 'energy', 'enf', 'enforce', 'engineer', 'engineering', 'english', 'engr', 'ength', 'enh', 'enhance', 'enhancement', 'enhances', 'enough', 'enr', 'enrich', 'enrq', 'enseignement', 'ensemble', 'ensure', 'ensures', 'ensuring', 'entire', 'entries', 'entropy', 'entry', 'enumerator', 'enumerators', 'environment', 'environments', 'eo', 'eprint', 'eq', 'eqs', 'equal', 'equalities', 'equality', 'equally', 'equals', 'equation', 'equations', 'equi', 'equilibrium', 'equiprobable', 'equivalence', 'equivalent', 'equivalently', 'er', 'era', 'erasure', 'erasures', 'erdo', 'ergodic', 'ern', 'erroneous', 'erroneously', 'error', 'errorcorrecting', 'errors', 'erweiterungen', 'es', 'esedog', 'esp', 'especially', 'essence', 'essentially', 'establish', 'established', 'establishes', 'establishing', 'establishment', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estimations', 'esult', 'eswaran', 'et', 'etc', 'eth', 'ethz', 'etzion', 'eu', 'eua', 'euclidean', 'eugenics', 'euler', 'eur', 'eurasip', 'europ', 'european', 'ev', 'eva', 'evaluate', 'evaluated', 'evaluation', 'evaluations', 'even', 'event', 'events', 'eventually', 'ever', 'every', 'everything', 'evgeny', 'evidently', 'evolution', 'evolve', 'evolvement', 'evolvements', 'ex', 'exact', 'exactly', 'exam', 'examination', 'examine', 'examined', 'examining', 'example', 'examples', 'exams', 'exceed', 'exceeds', 'except', 'exception', 'exceptional', 'exclude', 'excluding', 'exclusion', 'exclusively', 'execute', 'execution', 'exemplary', 'exercise', 'exercises', 'exhibit', 'exist', 'existence', 'existent', 'existing', 'exists', 'exits', 'exp', 'expanding', 'expect', 'expectation', 'expected', 'expects', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'explicitly', 'exploiting', 'explore', 'exponent', 'exponential', 'exponential1', 'exponentially', 'exponentials', 'exponentiation', 'exponents', 'exposition', 'express', 'expressed', 'expressing', 'expression', 'expressions', 'extend', 'extended', 'extending', 'extends', 'extensibility', 'extension', 'extensions', 'extensive', 'extensively', 'extent', 'extra', 'extract', 'extracting', 'extraction', 'extraordinarily', 'extremal', 'extreme', 'extremely', 'extremum', 'ez', 'eθ', 'eπ', 'f1', 'f2', 'f24', 'f2n', 'f3', 'f4', 'fa', 'fa9550', 'face', 'faced', 'fach', 'facing', 'fact', 'factor', 'factors', 'facts', 'faculty', 'fahnenhomogene', 'fail', 'failed', 'failure', 'falls', 'falmer', 'faloutsos', 'false', 'falsified', 'familiar', 'family', 'famous', 'fano', 'far', 'fast', 'faster', 'favor', 'fe', 'feasible', 'feature', 'features', 'fec', 'fed', 'feedback', 'fekri', 'felt', 'few', 'fidelity', 'field', 'fields', 'fifteen', 'fifteenth', 'fifth', 'fifty', 'fig', 'figure', 'figures', 'fijalkow', 'file', 'fileclassloader', 'filepap', 'fill', 'filling', 'filter', 'final', 'finalizing', 'finallly', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finer', 'finest', 'finishes', 'finite', 'first', 'firstly', 'firststage', 'fisher', 'fit', 'fitting', 'five', 'fix', 'fixed', 'fixing', 'fl', 'flag', 'flaw', 'flexibility', 'flexible', 'flip', 'floors', 'flowers', 'fn', 'fnq', 'focus', 'focused', 'focuses', 'focusing', 'fois', 'fold', 'folds', 'follow', 'followed', 'following', 'follows', 'follows4', 'fonction', 'fonctions', 'for', 'forbidden', 'forced', 'forces', 'fore', 'forget', 'form', 'formal', 'formalizations', 'formally', 'format', 'formation', 'formed', 'former', 'forms', 'formula', 'formulas', 'formulation', 'formulations', 'forney', 'forschungsgemeinschaft', 'forty', 'forward', 'found', 'foundation', 'foundations', 'four', 'fourteen', 'fourth', 'fq', 'fr', 'fraction', 'fractional', 'fragments', 'frame', 'framework', 'france', 'francis', 'frasson', 'fre', 'free', 'freedom', 'freedom9', 'french', 'frequency', 'friends', 'friendships', 'fripertinger', 'from', 'fruitful', 'fu', 'fuer', 'fujimoto', 'full', 'fully', 'function', 'functional', 'functionalities', 'functionality', 'functions', 'fundamental', 'fundamentals', 'funded', 'further', 'furthermore', 'furtherpk', 'furtherǫ', 'fusco', 'future', 'g0', 'g2', 'gage', 'gain', 'galeotti', 'gallager', 'galois', 'game', 'games', 'gap', 'garcke', 'gas', 'gastpar', 'gates', 'gathered', 'gauss', 'gaussian', 'gaussiannoise', 'gauthier', 'gave', 'gegenbauer', 'general', 'generality', 'generalization', 'generalizations', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generator', 'generators', 'generic', 'geom', 'geometer', 'geometric', 'geometrical', 'geometrically', 'geometries', 'geometry', 'german', 'germany', 'get', 'getclass', 'getmethod', 'getnodename', 'gets', 'getting', 'gf', 'gi', 'gibbs', 'gils', 'ginn', 'girth', 'give', 'given', 'gives', 'giving', 'gk', 'gleason', 'global', 'globally', 'globalvalues', 'gm', 'gn', 'gn2', 'go', 'goal', 'goals', 'goes', 'goethals', 'going', 'golay', 'goldstone', 'good', 'gore', 'government', 'govindan', 'gr', 'grade', 'gradient', 'gradients', 'gradually', 'graduate', 'graham', 'grain', 'grands', 'grant', 'graph', 'graphical', 'graphs', 'grassman', 'gratefully', 'gray', 'great', 'greater', 'greatest', 'greedy', 'green', 'gregory', 'grid', 'grids', 'griffiths', 'gro', 'gro96', 'group', 'groupes', 'groups', 'grouptheoretical', 'grover', 'growing', 'grows', 'gruppen', 'gt', 'gtπ', 'gu', 'guarantee', 'guaranteed', 'guarantees', 'gui', 'gulliver', 'gθ', 'gπ', 'gπt', 'h0', 'h02ǫ', 'h0ǫ', 'h1', 'h2ǫ', 'had', 'hadamard', 'haemers', 'haifa', 'half', 'hall', 'halves', 'hamburg', 'hamming', 'han', 'hand', 'handbook', 'handheld', 'handle', 'hanging', 'hannel', 'happening', 'happens', 'harada', 'hard', 'harder', 'harlow', 'harmless', 'harmonic', 'harmonically', 'harper', 'has', 'hat', 'have', 'having', 'he', 'headers', 'health', 'healthcare', 'heavy', 'heden', 'heidelberg', 'helleseth', 'help', 'helpful', 'hence', 'henceforth', 'hensel', 'her', 'here', 'hering', 'hermitian', 'herrera', 'hessler', 'heterogeneity', 'heterogeneous', 'heuristic', 'hexacode', 'hf', 'hfalse', 'hierarchy', 'high', 'higher', 'highest', 'highlight', 'highly', 'hilbert', 'hill', 'him', 'hint', 'hints', 'hirschfeld', 'his', 'histogram', 'histograms', 'historical', 'history', 'hitting', 'hn', 'hoc', 'hoeffding', 'hold', 'holds', 'holland', 'holmes', 'home', 'homogeneous', 'homogeneously', 'hong', 'honkala', 'hope', 'hoped', 'host', 'hours', 'how', 'however', 'hp', 'hs', 'hsv', 'ht', 'html', 'http', 'https', 'htε', 'huber', 'hue', 'huffman', 'hughes', 'huht', 'human', 'hundred', 'hurting', 'hut', 'hutter', 'hw', 'hwα', 'hx', 'hxy', 'hy', 'hybrid', 'hypercube', 'hyperoval', 'hyperovals', 'hyperplane', 'hyperplanes', 'hypotheses', 'hypothesis', 'hypothetical', 'hyx', 'hz', 'hµ', 'hµz', 'hǫ', 'hǫ0', 'hǫα', 'hα', 'hαǫ', 'hβ', 'hε', 'hηi', 'hξ', 'hφ0', 'i0', 'i1', 'i12', 'i2', 'i3', 'i4', 'i6', 'icece', 'icm', 'icosahedron', 'id', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identification', 'identified', 'identify', 'identifying', 'identities', 'identity', 'ieee', 'if', 'ignored', 'ignoring', 'ihvi', 'ihwα', 'ihx', 'ihµz', 'ihθ', 'ihφx', 'ihφz', 'ii', 'iid', 'iii', 'ij', 'ijcv', 'ik', 'ik2', 'illinois', 'illness', 'illustrate', 'illustrated', 'illustrates', 'illustrating', 'illustration', 'illustrative', 'im', 'image', 'images', 'imagine', 'immaterial', 'immediate', 'immediately', 'implement', 'implementation', 'implemented', 'implementing', 'implications', 'implicitly', 'implied', 'implies', 'imply', 'importance', 'important', 'importantly', 'impose', 'imposed', 'impossible', 'impression', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'imre', 'in', 'inc', 'inceptions', 'incidence', 'include', 'includes', 'including', 'inclusion', 'incomplete', 'inconsequential', 'incorporating', 'incorrect', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'indebted', 'indeed', 'independent', 'independent4', 'independently', 'indepth', 'index', 'indexed', 'indicate', 'indicates', 'indicating', 'indication', 'indicator', 'indicators', 'indices', 'indirectly', 'individually', 'individuals', 'induced', 'inen', 'inequalities', 'inequality', 'inf', 'infeasibility', 'infeasible', 'infimum', 'infinite', 'infinity', 'influence', 'influencing', 'infocom', 'inform', 'information', 'informatsii', 'infrared', 'ingeniously', 'ingredients', 'inherent', 'initial', 'initialize', 'initialized', 'initially', 'initiated', 'inner', 'input', 'inputs', 'inquiring', 'inscribed', 'insensitive', 'inserting', 'inside', 'insights', 'insignificant', 'insignificantly', 'inspecting', 'inspired', 'inst', 'install', 'installation', 'installed', 'instance', 'instead', 'institut', 'institute', 'instruction', 'instructive', 'int', 'integer', 'integers', 'integral', 'integrated', 'integrates', 'integration', 'intelligence', 'intelligent', 'intelligente', 'intended', 'intense', 'intensity', 'intentionally', 'interact', 'interaction', 'interactions', 'interactive', 'interest', 'interested', 'interesting', 'interestingly', 'interface', 'interface1', 'interfaces', 'interference', 'intermittent', 'internal', 'internally', 'international', 'internet', 'interplay', 'interpolant', 'interpolants', 'interpolating', 'interpret', 'interpretation', 'interpreted', 'interrelations', 'interscience', 'intersect', 'intersecting', 'intersection', 'intersects', 'interval', 'interwoven', 'intial', 'into', 'introduce', 'introduced', 'introduces', 'introducing', 'introduction', 'introspection', 'intuition', 'intuitive', 'intuitively', 'invariant', 'invariants', 'inventiones', 'inverse', 'inversely', 'investigate', 'investigated', 'investigating', 'investigation', 'investigations', 'invoke', 'invokes', 'involes', 'involve', 'involved', 'involves', 'involving', 'io', 'ioannides', 'ionin', 'iq', 'iqc', 'ir', 'ire', 'irreducible', 'irregular', 'irrespective', 'is', 'is3', 'isaac', 'isit', 'isita', 'isometries', 'isomorphic', 'isomorphism', 'isotropic', 'iss', 'issled', 'issue', 'issues', 'ist', 'it', 'italy', 'iterate', 'iterated', 'iterates', 'iterating', 'iteration', 'iterations', 'iterative', 'ith', 'itmanet', 'ito', 'itoh', 'its', 'itself', 'itzkovitz', 'iu', 'iv', 'iwuc', 'ix', 'ixx', 'iyw', 'izvestiya', 'iα', 'iαj', 'iθj', 'j12', 'j4', 'j6', 'ja', 'jackknife', 'jackson', 'jacobi', 'jacqueline', 'jadohealth', 'jakob', 'jamin', 'jan', 'jansen', 'janson', 'java', 'jdbc', 'jdsk', 'je', 'jensen', 'jeong', 'ji', 'jilles', 'jk', 'jn', 'job', 'joel', 'john', 'johnson', 'join', 'joint', 'jointly', 'journal', 'journey', 'joy', 'jr', 'jsp', 'jul', 'july', 'jun', 'junctions', 'june', 'jung', 'jungnickel', 'juni', 'just', 'justified', 'justifies', 'k2', 'k22', 'kailath', 'kang', 'karush', 'kashtan', 'kaski', 'katsman', 'kay', 'keep', 'keeping', 'keevash', 'kempe', 'kennedy', 'kept', 'kerber', 'kernel', 'kernels', 'keshavan', 'keug1', 'key', 'keyes', 'keyword', 'keywords', 'khachatrian', 'kilgus', 'kim', 'kind', 'kinds', 'kirkman', 'kissing', 'kit95', 'kitaev', 'kkt', 'kl', 'kleinberg', 'kn', 'know', 'knowing', 'knowledge', 'known', 'knows', 'ko', 'koetter', 'kohnert', 'koric', 'kornhuber', 'kpy', 'kpz', 'kq', 'kr', 'kramer', 'krause', 'krawtchouk', 'krishnan', 'ksk', 'ksk2', 'kth', 'kudryashov', 'kugeln', 'kuhn', 'kullback', 'kumar', 'kw', 'kw1α', 'kwy', 'kwα', 'kwα1', 'kyoto', 'kyushu', 'kµ', 'kα', 'kφe0', 'kφx', 'kψk', 'kψk2', 'la', 'lab', 'label', 'labeled', 'labeling', 'labelled', 'labels', 'laboratories', 'laboratory', 'labs', 'lam', 'lamb', 'land', 'language', 'lapidoth', 'large', 'larger', 'largest', 'larms', 'las', 'lassen', 'last', 'lastly', 'later', 'latin', 'latter', 'lattice', 'lattices', 'launch', 'law', 'laws', 'layer', 'layered', 'layers', 'ldpc', 'lead', 'leading', 'leads', 'leaf', 'learn', 'learning', 'least', 'leave', 'leaves', 'leaving', 'lect', 'lectnotes', 'lecture', 'lectures', 'led', 'leech', 'left', 'legendresymbol', 'leibler', 'leiva', 'lemma', 'lemmas', 'lemmata', 'lena', 'lends', 'lenght', 'length', 'lengths', 'lenz', 'leont', 'les', 'less', 'let', 'lets', 'lett', 'letter', 'letters', 'letting', 'leung', 'level', 'levels', 'levenshtein', 'levensthein', 'lewis', 'lexicographic', 'lexicographically', 'lias', 'lie', 'lies', 'lifted', 'light', 'like', 'likelihood', 'likely', 'lim', 'limit', 'limited', 'limits', 'limk', 'limx', 'limǫ', 'limβ', 'lin', 'line', 'linear', 'linearly', 'linearly14', 'liner', 'lines', 'link', 'links', 'lint', 'lipschitz', 'list', 'listed', 'literature', 'litsyn', 'little', 'liu', 'lizhong', 'lloyd', 'ln', 'lnc2', 'lncs', 'lne', 'loadclass', 'loader', 'lobstein', 'local', 'locate', 'locations', 'lock', 'locked', 'log', 'log2', 'logarithmic', 'logic', 'login', 'lond', 'london', 'long', 'longer', 'longman', 'look', 'looking', 'looks', 'looped', 'looping', 'loops', 'loseness', 'losing', 'loss', 'lost', 'lot', 'lova', 'low', 'lower', 'lowest', 'lri', 'lu', 'luby', 'luczak', 'luminosity', 'lumped', 'luxembourg', 'luxury', 'm0', 'm1', 'm11', 'm12', 'm2', 'm22', 'm23', 'm24', 'm3', 'm4', 'mac', 'macchiavello', 'machine', 'macro', 'macroscopic', 'macwilliams', 'made', 'mag', 'magniez', 'magnitude', 'main', 'maindetect', 'mainly', 'maintained', 'maintaining', 'maintains', 'major', 'majority', 'make', 'makers', 'makes', 'making', 'malik', 'management', 'manipulations', 'manner', 'manuscript', 'many', 'map', 'mapped', 'mapping', 'mappings', 'maps', 'mar', 'marcellin', 'march', 'margin', 'marked', 'market', 'markov', 'marshall', 'martingale', 'masiero', 'masnick', 'mass', 'massachusetts', 'master', 'mat', 'match', 'matched', 'material', 'materials', 'math', 'mathematicae', 'mathematical', 'mathematics', 'mathematik', 'mathieu', 'matrices', 'matrix', 'matta', 'matter', 'matthias', 'mattson', 'mauritius', 'max', 'max1', 'maxi', 'maximal', 'maximization', 'maximize', 'maximizers', 'maximizing', 'maximum', 'maxz', 'maxzn', 'may', 'mbo', 'mcgraw', 'mcmillan', 'me', 'mean', 'meaning', 'means', 'meantime', 'meanwhile', 'measurable', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'mechanical', 'mechanism', 'mechanisms', 'medina', 'medt', 'meet', 'meeting', 'meets', 'member', 'members', 'memorial', 'memory', 'memoryless', 'mention', 'mentioned', 'mentioning', 'merely', 'merriman', 'mes', 'mesh', 'mesner', 'message', 'messages', 'messagewise', 'messina', 'metal', 'meter', 'method', 'methodology', 'methods', 'metz', 'meyer', 'mg', 'mhuber', 'mi', 'michael', 'mid', 'might', 'mij', 'miklos', 'mikula', 'milan', 'milestones', 'milo', 'min', 'mini', 'minima', 'minimal', 'minimisation', 'minimise', 'minimised', 'minimises', 'minimising', 'minimize', 'minimized', 'minimizes', 'minimizing', 'minimum', 'ministry', 'ministère', 'minj', 'minqy', 'minute', 'miracle', 'misconceptions', 'misleading', 'missed', 'misseddetection', 'mistaken', 'mit', 'mitter', 'mitzenmacher', 'mixing', 'mj', 'mk', 'ml', 'mm', 'mmse', 'mn', 'mn07', 'mnrs07', 'mo', 'mobile', 'mod', 'mode', 'model', 'modeling', 'models', 'moderately', 'modern', 'modest', 'modica', 'modification', 'modifications', 'modified', 'modify', 'modifying', 'modulation', 'mog', 'mohsen', 'moire', 'moment', 'monograph', 'monotone', 'monotonically', 'monotonicity', 'montanar', 'montanari', 'monte', 'monthly', 'montreal', 'more', 'morelos', 'moreover', 'morris', 'mortimer', 'mortola', 'mosca', 'most', 'mostly', 'motifs', 'motion', 'motivate', 'motivated', 'motivates', 'motivation', 'mountain', 'move', 'moves', 'moving', 'mr', 'mrt', 'ms', 'mss07', 'mt', 'much', 'muller', 'multi', 'multiclassloader', 'multigrid', 'multilevel', 'multiphase', 'multiple', 'multiplechoice', 'multiples', 'multiplexing', 'multiplication', 'multiplications', 'multiplicative', 'multiplied', 'multiply', 'multiplying', 'multrigrid', 'mumford', 'musin', 'must', 'mutual', 'mutually', 'mx', 'myopic', 'mβ', 'n0', 'n00014', 'n0q', 'n1', 'n2', 'n2k', 'n2kα', 'n2l', 'n2rα', 'n3', 'nakib', 'nakibog', 'name', 'named', 'namely', 'national', 'nato', 'nats', 'natural', 'naturally', 'nature', 'naval', 'navigation', 'nayak', 'nd', 'ndu', 'ne', 'nearest', 'nearly', 'neb', 'nebe', 'neburg', 'neces', 'necessarily', 'necessary', 'necessitate', 'necessity', 'necked', 'need', 'needed', 'needless', 'needs', 'negative', 'negativity', 'negligible', 'neighbor', 'neighborhood', 'neighboring', 'neighbors', 'neighbouring', 'neighbours', 'neither', 'nemd', 'neo', 'neq', 'nested', 'nestler', 'netherlands', 'netw', 'network', 'networking', 'networks', 'neumaier', 'neumann', 'neuss', 'never', 'nevertheless', 'new', 'newinstance', 'newly', 'newman', 'newton', 'next', 'nh', 'ni', 'nielsen', 'nig', 'nighttime', 'nik', 'nine', 'nineteenth', 'njas', 'nk', 'nkα', 'nl', 'nn', 'no', 'node', 'nodes', 'noise', 'noiseless', 'noisy', 'non', 'nonadjacent', 'nonbinary', 'nonblock', 'nonempty', 'nonetheless', 'nonexistence', 'nonlinear', 'nonn', 'nonnegative', 'nonnormalized', 'nonuniform', 'nonzero', 'nor', 'norm', 'normal', 'normalization', 'normalized', 'normalizing', 'north', 'northholland', 'nos', 'not', 'notably', 'notation', 'notational', 'notations', 'note', 'noted', 'notes', 'notes5', 'nothing', 'notice', 'notices', 'noting', 'notion', 'notionpof', 'notions', 'nov', 'novel', 'now', 'nowadays', 'np', 'np2', 'nr', 'nr1', 'ns', 'nsa', 'nserc', 'nsα', 'nth', 'ntroduction', 'null', 'number', 'numbered', 'numbers', 'numerator', 'numerical', 'numerically', 'numerically12', 'numerics', 'numerische', 'numerous', 'nur', 'nv00', 'nx', 'ny', 'nyi', 'nyquist', 'nǫ', 'nα', 'nν', 'nνn', 'nφ', 'obesity', 'object', 'objection', 'objective', 'objects', 'observation', 'observations', 'observe', 'observed', 'observes', 'observing', 'obstacle', 'obstacles', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'occasional', 'occupy', 'occur', 'occurring', 'occurs', 'oct', 'octad', 'octads', 'octahedron', 'october', 'odbc', 'odd', 'odel', 'odes', 'odlyzko', 'odory', 'of', 'off', 'offer', 'offers', 'office', 'official', 'offs', 'often', 'oh', 'oi', 'old', 'older', 'olof', 'omitted', 'omitting', 'on', 'once', 'onclusions', 'one', 'ones', 'onion', 'onlinelearning', 'only', 'ontario', 'onto', 'open', 'oper', 'operating', 'operation', 'operations', 'operator', 'operators', 'opinion', 'opinions', 'opportunity', 'opposed', 'opposite', 'optimal', 'optimalities', 'optimality', 'optimisation', 'optimization', 'optimizations', 'optimize', 'optimized', 'optimizing', 'optimum', 'option', 'optional', 'options', 'or', 'oral', 'order', 'ordered', 'ordering', 'orderings', 'orders', 'ordinary', 'ordnen', 'org', 'organization', 'organized', 'origin', 'original', 'originally', 'origins', 'orlando', 'orsay', 'orthogonal', 'orthonormal', 'osaka', 'osc', 'oscillating', 'oscillation', 'osher', 'osthus', 'otation', 'other', 'others', 'otherwise', 'otions', 'our', 'ours', 'ourself', 'ourselves', 'out', 'outcome', 'outcomes', 'outer', 'outline', 'outlined', 'outperforms', 'output', 'outputs', 'outside', 'oval', 'ovals', 'over', 'overall', 'overlap', 'overlapping', 'overrelaxation', 'overview', 'overweight', 'own', 'ox', 'ox1', 'oxford', 'ozarow', 'ozbudak', 'p0', 'p00', 'p0gt', 'p1', 'p19', 'p2', 'p2k', 'p2r', 'p3', 'p481', 'p4c', 'p5', 'p6', 'pack', 'packed', 'packet', 'packing', 'packings', 'page', 'pages', 'paid', 'paige', 'pair', 'pairs', 'palek', 'paley', 'palmtop', 'pam', 'pand', 'papadimitriou', 'paper', 'papers', 'paradigm', 'paradigms', 'parallel', 'parallelisms', 'parameter', 'parameters', 'parametric', 'parametrisation', 'parent', 'paris', 'parity', 'park', 'parks', 'parser', 'part', 'partial', 'partially', 'participants', 'particles', 'particular', 'particularly', 'partition', 'partitioned', 'partitioning', 'partitions', 'partly', 'parts', 'pasch', 'pass', 'passed', 'passes', 'path', 'paths', 'pattern', 'patterns', 'paulo', 'payload', 'pde', 'pdes', 'pdf', 'pe', 'pecial', 'peculiar', 'pedagogical', 'peeling', 'pei', 'pem', 'penalizing', 'per', 'perasure', 'perdachi', 'peredaci', 'perfect', 'perform', 'performance', 'performances', 'performed', 'performing', 'perhaps', 'peridachi', 'perimeter', 'periods', 'permutation', 'permutations', 'permutationsgruppen', 'perona', 'personalized', 'persons', 'perspective', 'pet', 'peter', 'peterson', 'petitot', 'pfender', 'pg', 'pgt', 'ph', 'phase', 'phases', 'phd', 'phelps', 'phenomena', 'phenomenon', 'philadelphia', 'philips', 'philos', 'phone', 'photograph', 'photographed', 'phys', 'physica', 'physical', 'physician', 'physicians', 'physics', 'physiology', 'pi', 'pick', 'pickert', 'pictures', 'piece', 'pieces', 'piecewise', 'pij', 'pim', 'pinsker', 'piper', 'piracicaba', 'pishro', 'pixel', 'pixels', 'pk', 'pkij', 'pkji', 'place', 'placement', 'places', 'plain', 'plane', 'planes', 'plants', 'platforms', 'plausible', 'play', 'plays', 'plength', 'pless', 'plet', 'plu', 'plus', 'plusieurs', 'pm', 'pn', 'pn1', 'pn2', 'pny', 'pohang', 'point', 'pointed', 'pointing', 'points', 'pointwise', 'poisson', 'policy', 'poltyrev', 'polylog', 'polynomial', 'polynomials', 'pontiveros', 'poor', 'poorly', 'popular', 'populous', 'porto', 'portugal', 'pose', 'posed', 'positions', 'positive', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'posteriori', 'postmodern', 'postponed', 'postponement', 'potential', 'potentially', 'potenza', 'pottonen', 'poulliat', 'power', 'powers', 'pp', 'ppendix', 'ppn', 'pr', 'pr1', 'prabhakar', 'practical', 'practice', 'practitioners', 'prange', 'precise', 'precisely', 'precision', 'predecessor', 'predetermined', 'preece', 'prefer', 'prefix', 'preliminaries', 'preliminary', 'prepare', 'prepared', 'preprint', 'prescribed', 'presence', 'present', 'presentation', 'presented', 'presently', 'presents', 'preserve', 'preserved', 'preserves', 'press', 'prevailing', 'previous', 'previously', 'prg', 'prg0', 'primary', 'prime', 'primitive', 'principal', 'principle', 'principles', 'print', 'prior', 'priori', 'priority', 'privacy', 'prob', 'proba', 'probab', 'probabilistic', 'probabilities', 'probability', 'probability1', 'probable', 'probably', 'probl', 'problem', 'problematic', 'problems', 'problemy', 'proc', 'procedure', 'procedures', 'proceed', 'proceedings', 'proceeds', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'product', 'products', 'professor', 'professors', 'program', 'programming', 'programs', 'progress', 'proietti', 'project', 'projected', 'projecting', 'projection', 'projective', 'projector', 'projektive', 'promising', 'promotes', 'proof', 'proofs', 'propagation', 'properly', 'properties', 'property', 'proportional', 'proposals', 'propose', 'proposed', 'proposing', 'proposition', 'protect', 'protected', 'protecting', 'protection', 'protects', 'protocol', 'protocols', 'prove', 'proved', 'proven', 'proves', 'provide', 'provided', 'providence', 'provides', 'providing', 'province', 'proving', 'provision', 'ps', 'pseudo', 'pseudocode', 'pt', 'ptf', 'ptij', 'ptrue', 'pu', 'pub', 'public', 'publication', 'publications', 'published', 'publishers', 'publishing', 'pulkit', 'pulse', 'punctured', 'pure', 'pures', 'purpose', 'purposes', 'px', 'px1', 'px2', 'pxi', 'pxk', 'pxn', 'pxy', 'py', 'py1', 'pyx', 'pz', 'pθj', 'pλδ', 'pλδe', 'pπ', 'pτ', 'q0', 'q1', 'q3', 'qap', 'qd', 'qh', 'qht', 'qhtε', 'qi', 'qij', 'qk', 'qm', 'qn', 'qnpr', 'qq', 'qr', 'qt', 'qtr', 'quadrangle', 'quadratic', 'quadratically', 'quadric', 'quadrupelsysteme', 'quadruple', 'quality', 'quant', 'quantite', 'quantitie', 'quantities', 'quantity', 'quantization', 'quantizes', 'quantum', 'quantumly', 'quantumworks', 'quarterly', 'quartic', 'quaternary', 'qubit', 'question', 'questionaires', 'questionnaire', 'questionnaires', 'questions', 'quickly', 'quite', 'qy', 'r0', 'r1', 'r2', 'r24', 'r3', 'r4', 'r5', 'radius', 'radovic', 'rahnavard', 'raises', 'ramsey', 'randgraph', 'randgraph0', 'random', 'randomization', 'randomized', 'randomly', 'randomness', 'range', 'rank', 'rapid', 'rapidly', 'rare', 'rarely', 'rate', 'rates', 'rather', 'ratio', 'rational', 'raton', 'ray', 'rc', 'rc2', 'rd', 're', 'reach', 'reachable', 'reached', 'reaches', 'reaching', 'reachpan', 'readable', 'readapt', 'reader', 'readily', 'reading', 'ready', 'real', 'realization', 'realizations', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reals', 'realworld', 'reason', 'reasonable', 'reasonably', 'reasons', 'reboot', 'recall', 'recalling', 'recapitulate', 'receive', 'received', 'receiver', 'receivers', 'recent', 'recently', 'recherche', 'recognize', 'recognized', 'recognizing', 'recommend', 'reconciliation', 'reconstruction', 'recovered', 'recursion', 'recursive', 'recursively', 'red', 'redefine', 'redesign', 'redesigned', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'reed', 'ref', 'refer', 'referees', 'reference', 'references', 'referral', 'referrals', 'referred', 'referring', 'refers', 'refinability', 'refinable', 'refinement', 'refinements', 'refining', 'reflection', 'reflections', 'reflects', 'refrain', 'regarded', 'regarding', 'regardless', 'regime', 'region', 'regions', 'register', 'regular', 'regularity', 'reidel', 'reine', 'reinterpreted', 'reject', 'rejected', 'rejects', 'rel', 'relabeled', 'relate', 'related', 'relates', 'relating', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relaxed', 'relay', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'relies', 'reliminaries', 'rely', 'relying', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remark', 'remarkable', 'remarks', 'remedied', 'remembering', 'reminds', 'remmers', 'remote', 'removal', 'removed', 'removing', 'renamed', 'rend', 'renders', 'rendic', 'renner', 'reordering', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetition', 'repetitions', 'rephrased', 'replace', 'replaced', 'replaces', 'replacing', 'replication', 'report', 'reported', 'reports', 'represent', 'representation', 'representatives', 'represented', 'representing', 'represents', 'reprint', 'reproducible', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'res', 'research', 'researchers', 'reserved', 'residue', 'residues', 'resilient', 'resolution', 'resolved', 'resource', 'resources', 'resp', 'respect', 'respective', 'respectively', 'rest', 'restoration', 'restrict', 'restricted', 'restriction', 'restrictions', 'result', 'resulted', 'resulting', 'results', 'results13', 'retain', 'retransmission', 'retransmissions', 'retransmitted', 'return', 'returns', 'reuse', 'reused', 'reusing', 'rev', 'revealed', 'reveals', 'reversal', 'reverses', 'reversibility', 'reversible', 'review', 'reviews', 'revised', 'revisit', 'revisited', 'rewarding', 'rewrite', 'rewriting', 'rewritten', 'rg', 'rgb', 'ri', 'ric', 'rich', 'richardson', 'richter', 'richterp', 'rifa', 'right', 'rigorous', 'rij', 'rimoldi', 'ring', 'riordan', 'rise', 'rivosh', 'rj', 'rk', 'rl', 'rm', 'rn', 'rn1', 'rn3', 'rner', 'road', 'robert', 'robot', 'robust', 'robustness', 'rocky', 'rodney', 'rogers', 'roland', 'role', 'roles', 'romanov', 'roofs', 'room', 'root', 'rooted', 'roots', 'rotate', 'rotation', 'roth', 'rothkugel', 'roughly', 'round', 'rounded', 'row', 'rows', 'roy', 'royal', 'rq', 'rs', 'rth', 'rucinski', 'rudolf', 'rule', 'rummel', 'run', 'running', 'runs', 'runtime', 'rw', 'ryser', 'rθ', 'rω', 's0', 's1', 's12', 's2', 's2k', 's3', 'saberi', 'sahai', 'said', 'sake', 'same', 'sample', 'samples', 'sampling', 'san', 'san08', 'sand', 'sanghavi', 'sankaranarayanan', 'santha', 'sao', 'sapiro', 'sarily', 'satellite', 'satisfactory', 'satisfied', 'satisfies', 'satisfy', 'satisfying', 'saturation', 'say', 'saying', 'says', 'scalar', 'scale', 'scaled', 'scales', 'scaling', 'scan', 'scarcely', 'scenario', 'scenarios', 'schalkwijk', 'schalkwijk6', 'scheduling', 'scheme', 'schemes', 'schoenmakers', 'school', 'schu', 'schwartz', 'schwarz', 'sci', 'science', 'sciences', 'scope', 'scratch', 'se', 'search', 'searches', 'sec', 'second', 'secondly', 'secondmoment', 'seconds', 'secret', 'section', 'sections', 'secure', 'security', 'see', 'seek', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'segment', 'segmentation', 'segmentations', 'segmented', 'segments', 'seidel', 'selected', 'selecting', 'selection', 'selects', 'self', 'selfdual', 'sem', 'semakov', 'semidefinite', 'seminar', 'send', 'sending', 'sends', 'sense', 'sensitive', 'sensitivity', 'sent', 'sep', 'separate', 'separated', 'separately', 'separates', 'separation', 'separator', 'sept', 'september', 'sequel', 'sequence', 'sequences', 'sequential', 'sequentially', 'ser', 'series', 'serious', 'serve', 'server', 'services', 'servlet', 'seshadri', 'session', 'set', 'sets', 'settest', 'setting', 'settings', 'setup', 'seven', 'seventeenth', 'several', 'sh', 'shaded', 'shah', 'shalkwijk', 'shall', 'shannon', 'sharing', 'sharp', 'sharper', 'shashi', 'shed', 'shell', 'shells', 'shen', 'shenker', 'shenorr', 'shenvi', 'shift', 'shifted', 'shimamoto', 'shin', 'shiriaev', 'shiromoto', 'shokrollahi', 'short', 'shortened', 'shorter', 'shortest', 'shorthand', 'shortly', 'should', 'show', 'showed', 'showing', 'shown', 'shows', 'shrikhande', 'shutdowns', 'si', 'siam', 'sich', 'side', 'sides', 'sigcomm', 'sign', 'signal', 'signaling', 'signals', 'significance', 'significant', 'significantly', 'similar', 'similarly', 'simonis', 'simple', 'simpler', 'simplest', 'simplex', 'simplicity', 'simplifications', 'simplified', 'simplifies', 'simplify', 'simplifying', 'simply', 'simulating', 'simulation', 'simultaneous', 'simultaneously', 'sin', 'sin2', 'since', 'sinclair', 'singapore', 'singe', 'single', 'singleerror', 'singular', 'situation', 'situations', 'six', 'sixteenth', 'sixty', 'size', 'sizes', 'sk', 'skills', 'skip', 'skw03', 'sl', 'slides', 'slight', 'slightly', 'slim', 'sloane', 'slowly', 'small', 'smaller', 'smallest', 'smooth', 'smoothers', 'smoothing', 'smoothly', 'smoothness', 'sn', 'snapshots', 'snover', 'snr', 'so', 'soc', 'social', 'society', 'socrates', 'soda', 'software', 'sole', 'solids', 'solov', 'solution', 'solutions', 'solve', 'solved', 'solver', 'solvers', 'solving', 'some', 'something', 'sometimes', 'somewhat', 'sons', 'soon', 'sought', 'sounds', 'source', 'sources', 'sov', 'space', 'spacecraft', 'spaced', 'spaces', 'spacing', 'span', 'spanned', 'sparse', 'sparsity', 'spatial', 'spb', 'speak', 'special', 'specialty', 'specific', 'specifically', 'specification', 'specified', 'specifies', 'specify', 'specifying', 'spectral', 'spectrum', 'speed', 'speedup', 'spencer', 'spend', 'sphere', 'spherepacking', 'spheres', 'spherical', 'spielman', 'spiral', 'spite', 'split', 'splitting', 'spoil', 'sporadic', 'springer', 'spruijt', 'spurious', 'sqs', 'square', 'squared', 'squares', 'sr', 'ssc', 'st', 'stabilizer', 'stable', 'stage', 'stages', 'staircasing', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanford', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'static', 'stating', 'stationary', 'statist', 'statistic', 'statistical', 'statistically', 'statistics', 'steal', 'steer', 'stefanos', 'steffen', 'steger', 'stein', 'steiner', 'steinersche', 'stemann', 'step', 'stepping', 'steps', 'sterga', 'sth', 'stichtenoth', 'stick', 'stiffness', 'still', 'stinson', 'stoc', 'stochastic', 'stop', 'stopping', 'stops', 'storage', 'store', 'stored', 'storme', 'stoth', 'straightforward', 'strange', 'strategies', 'strategy', 'straße', 'strengthen', 'strengthening', 'strict', 'strictly', 'string', 'strings', 'strong', 'stronger', 'struct', 'structural', 'structure', 'structures', 'student', 'students', 'studied', 'studies', 'study', 'studying', 'stylized', 'sub', 'subclass', 'subcode', 'subdomains', 'subgradient', 'subgraph', 'subgraphs', 'subgroup', 'subject', 'sublinear', 'submessage', 'submessages', 'submitted', 'subpart', 'subscript', 'subscripts', 'subsection', 'subsections', 'subsections1', 'subsequent', 'subsequently', 'subset', 'subsets', 'subspace', 'subspaces', 'substantial', 'substantially', 'substitute', 'substituting', 'substitution', 'substructures', 'subtler', 'subtracting', 'subword', 'subwords', 'succesive', 'success', 'successful', 'successfully', 'successive', 'successively', 'such', 'sud', 'sudan', 'suen', 'suffices', 'sufficiency', 'sufficient', 'sufficiently', 'suggest', 'suggested', 'suggesting', 'suggests', 'suitable', 'sum', 'summarise', 'summarize', 'summarized', 'summarizes', 'summarizing', 'summary', 'summation', 'summing', 'sup', 'super', 'superimposed', 'superior', 'supn', 'supp', 'suppl', 'support', 'supported', 'supports', 'suppose', 'supposition', 'supq', 'supremum', 'supérieur', 'supσ', 'sur', 'sure', 'surprising', 'surprisingly', 'surrogate', 'survey', 'surveys', 'sussex', 'swap', 'swapped', 'swiercz', 'swiftness', 'swing', 'switch', 'switches', 'switching', 'swoh', 'sym2', 'sym36', 'symbol', 'symbols', 'symmetric', 'symmetrical', 'symmetrized', 'symmetry', 'symp', 'symposium', 'synchronization', 'syst', 'syste', 'system', 'systematic', 'systeme', 'systemindependent', 'systems', 'sz', 'sze04', 'szegedy', 'szkola', 'são', 'sǫα', 'sα', 'sαǫ', 'sβ', 'sν', 'sσ11u1', 'sσ22u2', 'sτ', 't1', 'ta', 'table', 'tables', 'tackle', 'tackles', 'tactical', 'tai', 'tail', 'take', 'taken', 'takes', 'taking', 'talking', 'tan', 'tan2', 'tanabe', 'tang', 'tangent', 'tangmunarunkit', 'tanner', 'taormina', 'taraz', 'target', 'task', 'tcc', 'teach', 'teaching', 'tech', 'technical', 'technicality', 'technically', 'technique', 'techniques', 'technische', 'technologies', 'technology', 'teirlinck', 'tel', 'telatar', 'telecomm', 'telecommunications', 'telephone', 'teletar', 'tell', 'tells', 'temporarily', 'tended', 'tentative', 'term', 'term4', 'terminates', 'terminating', 'terminologies', 'terminology', 'terms', 'ternary', 'test', 'testclass', 'testing', 'testinhtml', 'testmethod', 'testobject', 'tests', 'tetrahedron', 'texas', 'text', 'texts', 'texture', 'tf', 'th', 'than', 'thank', 'thanks', 'thas', 'that', 'the', 'theblank', 'their', 'them', 'theme', 'themselves', 'then', 'theorem', 'theorems', 'theoretic', 'theoretical', 'theories', 'theorists', 'theory', 'there', 'thereby', 'therefore', 'therein', 'these', 'thesis', 'they', 'thick', 'thickness', 'thiel', 'thing', 'think', 'third', 'thirteenth', 'this', 'thomas', 'thompson', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'thoughts', 'three', 'threshold', 'thresholding', 'through', 'throughout', 'throw', 'thus', 'tieta', 'tight', 'tighter', 'tilborg', 'till', 'time', 'times', 'tits', 'tjoelker', 'tl', 'to', 'todd', 'together', 'togonidze', 'tolerance', 'tomasi', 'tomecat', 'tonchev', 'too', 'took', 'tool', 'tools', 'top', 'topic', 'topics', 'topological', 'topologies', 'topology', 'torus', 'total', 'touch', 'towards', 'towsley', 'tpx', 'tpy', 'tr', 'trace', 'track', 'tracked', 'trade', 'tradeoffs', 'tradition', 'traditional', 'training', 'trans', 'transactions', 'transferred', 'transformations', 'transition', 'transitions', 'transitive', 'transitively', 'transitiven', 'transitivity', 'translated', 'translates', 'translating', 'translation', 'transmission', 'transmissions', 'transmit', 'transmits', 'transmitted', 'transmitter', 'transmitting', 'transparent', 'treat', 'treated', 'treating', 'treatment', 'treats', 'tree', 'trial', 'triangle', 'trianglefree', 'triangles', 'triangulation', 'trick', 'trier', 'tries', 'triple', 'trivial', 'trivially', 'trois', 'trott', 'troubles', 'true', 'truncated', 'try', 'trying', 'tsai', 'tschel', 'tse', 'tte', 'tu', 'tucker', 'tude', 'tue', 'tul08', 'tulsi', 'tuned', 'tuple', 'tuples', 'turbo', 'turn', 'turns', 'tutor', 'tutorial', 'tutorielle', 'tutoring', 'tuy', 'tuyls', 'tv', 'tvi', 'tw', 'twelve', 'twentieth', 'twenty', 'twentyeighth', 'twice', 'twin', 'twisted', 'two', 'tx', 'type', 'types', 'typical', 'typically', 'tǫ', 'tδ', 'tσn', 'u0', 'u1', 'u12', 'u1θ', 'u2', 'u2θ', 'u3', 'ubiquitous', 'uc', 'ucla', 'uep', 'ugly', 'uh', 'uhi', 'ui', 'ui2', 'uk', 'ultimately', 'un', 'un2', 'unable', 'unacceptable', 'unaffected', 'unam', 'unambiguous', 'unavoidable', 'unbiased', 'unbounded', 'uncanceled', 'unchanged', 'unclear', 'unconditional', 'unconstrained', 'uncoordinated', 'under', 'underline', 'underlying', 'understand', 'understanding', 'understood', 'undesirable', 'undetected', 'undirected', 'undo', 'unequal', 'unexpected', 'unfortunately', 'uniform', 'uniformity', 'uniformly', 'unify', 'unimodular', 'unimportant', 'union', 'unique', 'uniquely', 'uniqueness', 'unit', 'unitaries', 'unitary', 'units', 'univ', 'universal', 'universally', 'universita', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unlimited', 'unnecessary', 'unnormalized', 'unnoticed', 'unpublished', 'unrealistic', 'unreliable', 'unresolved', 'unrestricted', 'unsatisfactory', 'unsolved', 'unstable', 'unsymmetric', 'until', 'unused', 'unusually', 'up', 'update', 'updated', 'updating', 'upon', 'upper', 'urbanke', 'url', 'us', 'usa', 'use', 'used', 'useful', 'usefulness', 'user', 'uses', 'using', 'ussr', 'usual', 'usually', 'ut', 'utility', 'uture', 'uwaterloo', 'uy', 'v0', 'v1', 'v2', 'va', 'valente', 'valid', 'validate', 'vallentin', 'valuable', 'value', 'valued', 'values', 'van', 'vanish', 'vanishes', 'vanishing', 'vanishingly', 'vanlehn', 'vanstone', 'var', 'variable', 'variables', 'variance', 'variant', 'variation', 'variational', 'variations', 'varies', 'varieties', 'variety', 'various', 'vary', 'varying', 'vasic', 'vazirani', 'vector', 'vectorial', 'vectors', 'vegas', 'verbindung', 'verbitskiy', 'verification', 'verified', 'verify', 'verlag', 'version', 'versions', 'versus', 'vertex', 'vertices', 'very', 'vese', 'vh', 'vi', 'via', 'viable', 'view', 'viewed', 'vii', 'viii', 'villanueva', 'violate', 'virtual', 'vishwanath', 'visible', 'vision', 'visiting', 'visual', 'visualization', 'visualize', 'visually', 'vivid', 'vj', 'vn', 'vol', 'volume', 'von', 'vontobel', 'vorkommen', 'voyager', 'vr', 'vs', 'vt', 'vu', 'vub', 'vy', 'vτ', 'w0', 'w1', 'w3c', 'waerden', 'walk', 'walker', 'walks', 'want', 'wanted', 'ward', 'warnke', 'was', 'wassermann', 'wasteful', 'waterloo', 'watrous', 'watson', 'watts', 'wavelenghts', 'wavelength', 'wavelengths', 'way', 'ways', 'we', 'weak', 'weakening', 'weaker', 'weakly', 'web', 'webapplication', 'week', 'weeks', 'weight', 'weighted', 'weights', 'weissman', 'welche', 'weldon', 'well', 'were', 'weyl', 'whaley', 'what', 'whatever', 'when', 'whenever', 'where', 'whereas', 'whereby', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whose', 'why', 'wi', 'wide', 'widely', 'widen', 'wider', 'widlund', 'width', 'wielandt', 'wigger', 'wiley', 'will', 'willinger', 'wilson', 'win', 'winkler', 'wirel', 'wireless', 'wise', 'wish', 'with', 'within', 'without', 'witt', 'wj', 'wk', 'wn', 'wolf', 'wolfovitz', 'wolfson', 'wonder', 'woolhouse', 'word', 'words', 'work', 'working', 'works', 'workshop', 'world', 'wormald', 'worst', 'worth', 'worthwhile', 'would', 'wp', 'write', 'writing', 'written', 'wrong', 'www', 'wy', 'wyy', 'wα', 'wαz', 'x0', 'x1', 'x12', 'x2', 'x22', 'x6', 'xa', 'xd', 'xfl', 'xfu', 'xh', 'xi', 'xi2', 'xihx', 'xj', 'xk', 'xml', 'xmlbasierte', 'xn', 'xn1', 'xn2', 'xp', 'xpy', 'xr', 'xt', 'xu', 'xv', 'xx', 'xxx', 'xyi', 'y0', 'y0n', 'y1', 'y1i', 'y1n', 'y1n1', 'y2', 'y6', 'yamamoto', 'yamamotoitoh', 'yan', 'yann', 'years', 'yes', 'yet', 'yi', 'yield', 'yielding', 'yields', 'yihy', 'yis', 'yj', 'yk', 'yn', 'york', 'ypr', 'ypy', 'yt', 'yu', 'yx', 'z0', 'z0n', 'z1', 'z4', 'zahl', 'zaitsev', 'zaragoza', 'zenios', 'zero', 'zhao', 'zheng', 'zi', 'ziegler', 'zigangirov', 'zihz', 'zinov', 'zinoviev', 'zn', 'zn1', 'zu', 'zwei', 'zyablov', 'µ1', 'µ2', 'µi', 'µihµ', 'µm', 'µz', 'ǫa', 'ǫauk', 'ǫcloseness', 'ǫh', 'ǫn', 'ǫsmooth', 'ǫtypical', 'ǫua', 'ǫδta', 'α1', 'α2', 'α21', 'αd', 'αi', 'αil', 'αj', 'αk', 'αtv', 'αv', 'αx', 'αz', 'αβxx', 'αη', 'αηt', 'αθ', 'αθ1', 'βx', 'γdmax', 'γk', 'γn', 'γn1', 'γp', 'γq', 'γr', 'γℓn', 'δ0', 'δ02', 'δ2', 'δj', 'δj2', 'δk', 'δkk', 'δn', 'δt', 'δtt', 'δλ', 'δℓ', 'ζj', 'ζn', 'ηi', 'ηil', 'ηn', 'ηt', 'θ0', 'θ1', 'θ12', 'θ2', 'θ2k2', 'θa', 'θi', 'θihθ', 'θj', 'θj2', 'θjj', 'θm', 'θn', 'θp', 'θr', 'θs', 'κ1', 'κk', 'κn', 'λ1', 'λ2', 'λf', 'λi', 'λj', 'λm', 'λmax', 'λn', 'λs', 'λu', 'λw', 'λµ', 'λǫ', 'λγk', 'λδ', 'λδe', 'ν1', 'ν2', 'νez', 'νj', 'νj2', 'νn', 'νδk', 'ξj', 'π6', 'πi', 'πk', 'πp', 'πt', 'πx', 'πy', 'πψ', 'πψi', 'ρ0', 'ρα', 'σ0', 'σ02', 'σ1', 'σ12', 'σ2', 'σ22', 'σi', 'σi2', 'σn', 'σn1', 'σn2', 'σz2', 'τ1', 'τ2', 'τ24', 'τ3', 'τ4', 'τ8', 'τi', 'τj', 'τn', 'τδ', 'φ0', 'φ1', 'φ2', 'φe0', 'φi', 'φj', 'φm', 'φmax', 'φq', 'φs', 'φx', 'φy', 'φz', 'ψi', 'ψihψ', 'ψik2', 'ω2', 'ωi', 'ωm2', 'ℓ2', 'ℓn']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 1 0 ... 1 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iztwu9377YsS",
        "outputId": "596467b0-534c-4435-957b-f234e71c3c30"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = df['contenido'].tolist()\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00', '000', '0010117', '003', '0044', '0045', '007', '0084v1', '01', '0156', '015848', '02', '025', '027', '03', '0301043', '0309430', '04', '0403263', '042312', '05', '0512258', '052307', '06', '0608426', '0610143', '0610143v2', '0610146', '0610146v2', '07', '0704', '0710', '0736v1', '08', '0803', '0808', '0811', '0812', '09', '0i', '0ih0', '0kwα', '10', '100', '101', '1010001', '1016', '102', '103', '104', '105', '106', '10623', '107', '108', '1085', '109', '1095', '1099', '11', '110', '1101000', '1108', '111', '1117', '112', '1123', '113', '114', '115', '116', '117', '117543', '118', '119', '12', '120', '1201', '121', '1212', '1216698', '122', '1229', '123', '123000', '1234', '1238', '124', '1245', '1248', '125', '1254v1', '126', '1261', '1267', '1268', '1269', '127', '1271766600', '1273', '128', '1288', '129', '12s0', '13', '130', '1302', '131', '132', '133', '134', '1343', '135', '1359', '136', '1365', '137', '138', '1382', '139', '1391', '1393', '1397', '14', '140', '141', '1416', '142', '1420', '1423', '1426', '143', '1435', '1438', '144', '1441', '145', '146', '147', '148', '149', '1496770', '1496833', '14th', '15', '150', '151', '152', '1523', '1527', '1554140', '1567', '157', '1584', '159', '16', '160', '161', '16242', '1632', '1641', '1650', '166', '167', '168', '1682', '169', '1694', '17', '172', '1737', '1744', '175', '1764', '1770', '179', '18', '180', '181', '182', '183', '1839', '184', '185', '1853', '1860', '1861', '1865', '187', '1873', '1886', '189', '19', '190', '190680', '1933', '1937', '1938', '194', '1940', '1948', '1949', '195', '1950', '1952', '1953', '1954', '1956', '1959', '196', '1960', '1961', '1963', '1964', '1965', '196560', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '199', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '1i', '1ih1', '1n', '1st', '1t', '1x', '20', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2009a', '2009b', '2010', '2011', '2013', '2014', '2015', '2017', '202204', '205', '206', '207', '209685', '21', '210', '211', '212', '214', '215', '216', '219', '22', '220', '221', '221229', '2231', '2235', '224', '225', '228', '229', '23', '232', '233', '237', '239', '239329029060', '24', '240', '241', '24140500956', '243', '2438973', '2477', '25', '2504', '251', '253', '256', '2561', '257', '2570v4', '2595', '26', '261', '262', '264', '265', '266', '267', '269', '27', '2702', '2709v4', '2714', '272', '275', '276', '277', '28', '283', '2853v2', '287', '288', '289', '2893', '29', '291', '295', '2957388', '296', '298', '2a', '2a0', '2a20', '2c1', '2d', '2e', '2e2m', '2e33', '2gn', '2iθj', '2k', '2m', '2n', '2nd', '2p', '2pe', '2pi', '2pn', '2pns', '2q', '2r', '2t', '2w', '2ǫ', '2β', '2γn1', '2δ', '2δj2', '2ε', '2η', '2ηn', '2π', '2πk', '2πψ', '2φ', '2ℓ', '30', '301', '305', '3060', '3063', '307', '3074', '3079', '3080', '309', '31', '310', '311', '313', '314', '315', '316', '318', '319', '32', '323', '325', '328', '33', '3329', '333', '3333', '334', '335', '336', '337', '3378', '339', '33rd', '34', '342', '34337160', '344', '348', '349', '35', '3504v1', '354', '36', '36176', '362', '363', '364', '366', '367', '37', '377', '3788', '379', '38', '381', '384', '387', '388', '39', '391', '396', '397', '39th', '3g1', '3k', '3kα', '3pprovided', '3qd', '3rd', '40', '400', '402', '406', '407', '408', '4096', '41', '417', '419', '41st', '42', '421', '422', '423', '425', '43', '435', '438', '44', '440', '449', '449820', '45', '450', '453', '454', '456', '45th', '46', '465', '468', '47', '476', '48', '487', '489', '49', '490', '491', '4978', '4a', '4a2', '4a20', '4anα', '4b', '4c2', '4htε', '4n', '4n1', '4pi', '4th', '4ǫ', '4λ2', '4νj2', '4φ', '4φ1', '4ℓ', '50', '50456', '506', '507', '508', '51', '518', '52', '522', '526', '53', '539', '5390', '54', '543', '55', '5577', '56', '566', '57', '575', '576', '577', '58', '581', '584', '59', '5963', '5a', '5b', '5th', '60', '600', '607', '61', '610814', '611', '613', '62', '623', '6279', '629', '63', '633', '639', '64', '640', '644', '647', '65', '656', '657', '66', '6648', '667', '67', '671', '677', '68', '682', '683', '685', '687', '689', '69', '690', '692', '6p', '70', '700', '701', '702', '707', '708', '71', '71307600', '714', '72', '729', '73', '733', '74', '747', '749', '749999640', '75', '753', '759', '76', '77', '771', '773', '78', '789', '78α1', '79', '80', '803', '807', '81', '819', '82', '824', '827', '828', '83', '831', '84', '845', '85', '8580', '86', '860', '864', '866876', '87', '873', '873450', '88', '880', '883', '889', '89', '8a20', '90', '91', '910', '91405', '916', '92', '928', '93', '94', '94305', '945', '949', '95', '9511026', '96', '963', '97', '98', '982', '99', '9rf', 'a0', 'a1', 'a11', 'a12', 'a15', 'a16', 'a2', 'a20', 'a21', 'a23', 'a2j', 'a3', 'a4', 'a5', 'a7', 'a8', 'aa05', 'aakv01', 'aaronson', 'aat', 'abelian', 'aber', 'abh', 'abilities', 'ability', 'able', 'abn', 'about', 'above', 'abovementioned', 'abs', 'absence', 'absent', 'absorbed', 'absorption', 'abstract', 'abundance', 'abuse', 'ac', 'acad', 'academic', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepts', 'access', 'accessed', 'accessible', 'accommodate', 'accomplished', 'according', 'account', 'accounts', 'accumulated', 'accuracy', 'achievability', 'achievable', 'achieve', 'achieved', 'achievements', 'achieves', 'achieving', 'acknowledge', 'acknowledged', 'acknowledges', 'acknowledgment', 'acknowledgments', 'acm', 'acquired', 'across', 'act', 'acta', 'acting', 'action', 'active', 'actively', 'actual', 'actually', 'ad', 'adapt', 'adaptation', 'adapted', 'adapting', 'adaptive', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adequate', 'adiposity', 'adjacency', 'adjacent', 'adjoin', 'adjusted', 'admit', 'admits', 'adolesc', 'adolescent', 'adopt', 'adriano', 'ads', 'adv', 'advance', 'advanced', 'advances', 'advancing', 'advantage', 'advantages', 'ae1', 'aegean', 'aep', 'affected', 'affiliations', 'affine', 'affirmative', 'afford', 'aforementioned', 'afosr', 'afresh', 'after', 'ag', 'again', 'against', 'agents', 'ages', 'agl', 'ago', 'agreed', 'aharonov', 'ahlswede', 'ai', 'aict', 'aigner', 'aij', 'ailath', 'aim', 'aimed', 'aims', 'ain', 'aj', 'ak', 'akj', 'akr05', 'al', 'alarm', 'alarms', 'albanese', 'albeit', 'albenese', 'albert', 'ale', 'alert', 'alessandro', 'algebr', 'algebra', 'algebraic', 'algebraically', 'algebras', 'algoqp', 'algorithm', 'algorithmic', 'algorithmica', 'algorithms', 'all', 'allen', 'allencahn', 'allerton', 'allgemeine', 'allocate', 'allocated', 'allocating', 'allocation', 'allow', 'allowable', 'allowed', 'allowing', 'allows', 'almost', 'alon', 'alone', 'along', 'alphabet', 'alphabets', 'already', 'also', 'altered', 'alternate', 'alternating', 'alternative', 'alternatively', 'although', 'always', 'am', 'amb03', 'amb04', 'ambainis', 'amer', 'american', 'amin', 'among', 'amongst', 'amount', 'amplification', 'amplified', 'amplifying', 'amplitude', 'amplitudes', 'amraoui', 'ams', 'amsterdam', 'an', 'anal', 'analog', 'analogies', 'analogon', 'analogons', 'analogous', 'analogs', 'analogue', 'analogues', 'analyses', 'analysis', 'analytical', 'analyzable', 'analyze', 'analyzed', 'analyzes', 'analyzing', 'anayak', 'and', 'andes', 'andrea', 'angew', 'angle', 'anisotropic', 'anita', 'anita2', 'ann', 'annotation', 'annotations', 'announc', 'annual', 'anomalies', 'anonymous', 'another', 'anr', 'answer', 'answered', 'answering', 'answers', 'anti', 'anticodes', 'antiphase', 'antonio', 'any', 'anytime', 'anyway', 'anα', 'apache', 'apacity', 'apart', 'apparent', 'appear', 'appeared', 'appearing', 'appears', 'appendix', 'appl', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriately', 'approx', 'approximate', 'approximated', 'approximately', 'approximating', 'approximation', 'approximations', 'apr', 'april', 'arbitrarily', 'arbitrary', 'arch', 'architecture', 'architectures', 'architektur', 'are', 'area', 'areas', 'argue', 'argued', 'argument', 'arguments', 'arises', 'arising', 'arithmetic', 'aro', 'arose', 'around', 'arq', 'arrange', 'arranged', 'array', 'arrive', 'art', 'article', 'articles', 'artificially', 'arxiv', 'ary', 'as', 'aschbacher', 'ashwin', 'asiacrypt', 'aside', 'ask', 'asked', 'asks', 'aspect', 'aspects', 'assertion', 'assertions', 'assign', 'assigned', 'assigning', 'assignment', 'assigns', 'assmus', 'assmusmattson', 'assoc', 'associate', 'associated', 'associates', 'association', 'associations', 'associative', 'associe', 'assume', 'assumed', 'assumes', 'assuming', 'assumption', 'assumptions', 'assured', 'astronomy', 'asymptotic', 'asymptotically', 'at', 'att', 'attack', 'attained', 'attains', 'attempted', 'attempts', 'attend', 'attended', 'attention', 'attenuated', 'attenuation', 'attracted', 'attribute', 'aufgabe', 'aug', 'augmenting', 'august', 'aujol', 'aut', 'authentication', 'author', 'authoring', 'authors', 'automatic', 'automorphism', 'automorphisms', 'autonomously', 'aux', 'auxiliary', 'available', 'average', 'averaged', 'averages', 'averaging', 'avgustinovich', 'avoid', 'avoided', 'avoiding', 'avoids', 'awards', 'away', 'awgn', 'ax', 'axillary', 'aydinian', 'aθ', 'aθ0', 'aθj', 'b1', 'b2', 'b3', 'b4', 'bach', 'bachoc', 'back', 'background', 'backward', 'bad', 'baghdady', 'bajaj', 'balaji', 'balance', 'balanced', 'ball', 'balls', 'band', 'bandlimited', 'bandwidth', 'bannai', 'bar', 'bare', 'barely', 'barron', 'barıs', 'based', 'basel', 'basic', 'basically', 'basis', 'bassalygo', 'bayati', 'bayes', 'bb', 'be', 'bea', 'beautiful', 'became', 'because', 'becausepof', 'become', 'becomes', 'been', 'before', 'beginning', 'begun', 'behave', 'behaves', 'behavior', 'behind', 'being', 'believe', 'bell', 'belong', 'belonging', 'belongs', 'below', 'bence', 'bender', 'benefit', 'benefitted', 'benes', 'benjamin', 'ber', 'berkeley', 'berlekamp', 'berlin', 'bernoulli', 'berry', 'besides', 'best', 'bet', 'beth', 'betten', 'better', 'between', 'beutelspacher', 'beyond', 'bezdek', 'bi', 'bias', 'bias3', 'biased', 'biasπrg', 'biasπtf', 'big', 'bij', 'bijective', 'bility', 'billiard', 'bin', 'binary', 'biological', 'bipartite', 'biplane', 'biprandgraph', 'birkha', 'birkhauser', 'birkhoff', 'birth', 'bit', 'bits', 'bius', 'bizarre', 'bj', 'bjelakovic', 'bk', 'black', 'blake', 'blanc', 'blanchet', 'blind', 'blitzstein', 'block', 'blocklength', 'blocks', 'blomer', 'blowey', 'blowing', 'blows', 'blue', 'blur', 'bn', 'bn1', 'bob', 'boca', 'bohman', 'bold', 'bolloba', 'bonnecaze', 'book', 'boolean', 'boost', 'boosting', 'bootstrap', 'borade', 'bordered', 'boris', 'borrowing', 'bose', 'boston', 'both', 'bottle', 'boulders', 'bound', 'boundaries', 'boundary', 'bounded', 'bounding', 'bounds', 'box', 'boyarinov', 'brackets', 'bramoulle', 'branches', 'branching', 'braun', 'brazil', 'brazilian', 'brazilians', 'break', 'breaks', 'breiman', 'brest', 'brevity', 'brian', 'brief', 'briefly', 'brightness', 'brighton', 'britz', 'broad', 'broadband', 'broadcast', 'broader', 'broken', 'bross', 'brothers', 'brouwer', 'brown', 'browser', 'bruck', 'brust', 'bs', 'bsc', 'bu', 'buekenhout', 'buffalo', 'buhrman', 'building', 'builds', 'built', 'burlington', 'burnashev', 'business', 'but', 'bv', 'bx', 'by', 'by10', 'by13', 'by5', 'byers', 'bǫ', 'c0', 'c1', 'c2', 'c2r', 'c3', 'ca', 'cahn', 'calculate', 'calculated', 'calculates', 'calculating', 'calculation', 'calculations', 'calderbank', 'california', 'call', 'called', 'calls', 'calm', 'cam', 'cambridge', 'cameron', 'camion', 'can', 'canad', 'canada', 'cancel', 'canceled', 'candidate', 'candidates', 'canfield', 'cannot', 'canonical', 'capability', 'capable', 'capacities', 'capacity', 'capital', 'captured', 'captures', 'capturing', 'carathe', 'cardinality', 'care', 'carefully', 'carla', 'carlo', 'carmichael', 'carried', 'carus', 'case', 'caselles', 'cases', 'casual', 'catalogue', 'catalyst', 'category', 'causal', 'cause', 'caused', 'cavities', 'cavity', 'cb', 'cbt', 'ccf', 'ce', 'cea', 'cei', 'celebrated', 'cell', 'cemm98', 'census', 'center', 'centered', 'centers', 'central', 'centre', 'century', 'certain', 'certainly', 'certainty', 'cet', 'cf', 'cfm', 'cg04', 'ch', 'chain', 'chains', 'chalkwijk', 'challenge', 'challenges', 'challenging', 'chalupecky', 'chan', 'chance', 'chandrasekhar', 'change', 'changes', 'changing', 'channel', 'channels', 'chap', 'chapt', 'chapter', 'character', 'characteristic', 'characterizations', 'characterize', 'characterized', 'characterizing', 'chart', 'chebyshev', 'check', 'checking', 'checks', 'cheme', 'chen', 'cheong', 'chequered', 'chernoff', 'childs', 'chklovskii', 'choice', 'choose', 'chooses', 'choosing', 'chose', 'chosen', 'chou', 'chromaticity', 'chuang', 'chung', 'ci', 'cifar', 'cij', 'cijt', 'cinq', 'circles', 'circuit', 'circuits', 'circulant', 'citation', 'cj', 'ck', 'cker', 'claim', 'claims', 'clarendon', 'clarification', 'clarity', 'class', 'classes', 'classes_path', 'classic', 'classical', 'classification', 'classified', 'classify', 'classifying', 'classpara', 'classroom', 'clean', 'clear', 'clearly', 'cleve', 'clever', 'client', 'close', 'closely', 'closeness', 'closer', 'clouds', 'cluster', 'clusters', 'cmaia', 'cmmi', 'cn', 'cn2', 'cnrs', 'co', 'coached', 'coarse', 'coarsen', 'coarsening', 'coarsest', 'cobenge', 'code', 'codebook', 'coded', 'codes', 'codeword', 'codewords', 'coding', 'coe', 'coefficient', 'coefficients', 'coexist', 'cohen', 'cohn', 'coincides', 'coins', 'colbourn', 'coleman', 'collaborative', 'collected', 'collection', 'collectively', 'collinear', 'color', 'colour', 'colours', 'column', 'columns', 'com', 'com2', 'comb', 'combin', 'combination', 'combinatorial', 'combinatorially', 'combinatorica', 'combinatorics', 'combinatorische', 'combine', 'combined', 'combines', 'combining', 'combo', 'come', 'comes', 'comfortable', 'comlab', 'comm', 'commented', 'comments', 'commission', 'common', 'commun', 'communicate', 'communicated', 'communicating', 'communication', 'communications', 'community', 'commutativity', 'commute', 'comp', 'compact', 'comparable', 'compare', 'compared', 'comparing', 'comparison', 'complement', 'complementary', 'complete', 'completely', 'completeness', 'completes', 'complex', 'complexities', 'complexity', 'complicate', 'complicated', 'complication', 'component', 'components', 'composed', 'composite', 'composition', 'compressing', 'compression', 'compromised', 'comput', 'computable', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computerbased', 'computers', 'computes', 'computing', 'concatenation', 'concave', 'concavity', 'concede', 'concentrate', 'concentrated', 'concentration', 'concentrations', 'concentric', 'concept', 'concepts', 'conceptual', 'conceptually', 'concerned', 'concerning', 'concise', 'conclude', 'concluded', 'concludes', 'conclusion', 'conclusions', 'concrete', 'condition', 'conditional', 'conditioned', 'conditioning', 'conditions', 'conducted', 'conf', 'conference', 'confidence', 'configuration', 'configurations', 'confirmation', 'confirmed', 'confusion', 'congress', 'conj', 'conjecture', 'conjugate', 'conjugates', 'conjunction', 'connect', 'connected', 'connecting', 'connection', 'connections', 'connects', 'consequence', 'consequences', 'consequently', 'consider', 'considerable', 'considerably', 'consideration', 'considerations', 'considered', 'considering', 'considers', 'consisting', 'consists', 'consolidated', 'constant', 'constantly', 'constants', 'constellation', 'constituents', 'constrain', 'constrained', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'constructions', 'constructors', 'contain', 'contained', 'containing', 'contains', 'contemporary', 'content', 'context', 'contexts', 'contingency', 'continually', 'continue', 'continues', 'continuous', 'contour', 'contours', 'contradicting', 'contradiction', 'contradicts', 'contrary', 'contrast', 'contrasts', 'contributed', 'contribution', 'contributions', 'control', 'controlled', 'controlling', 'controversy', 'convenience', 'convenient', 'convention', 'conventional', 'conventionally', 'converge', 'convergence', 'converges', 'converse', 'conversely', 'converses', 'conversions', 'convex', 'convexity', 'convey', 'conveyed', 'convolve', 'convolving', 'conway', 'cooperative', 'coordinate', 'coordinates', 'coordination', 'copies', 'coprime', 'copy', 'copyright', 'core', 'corners', 'corollary', 'correct', 'corrected', 'correcting', 'correction', 'corrections', 'correctly', 'correlated', 'correlation', 'corresp', 'correspond', 'correspondence', 'corresponding', 'corresponds', 'corrollary', 'cos', 'cos2', 'cost', 'costly', 'costs', 'cot', 'cot2', 'could', 'count', 'countably', 'counted', 'counterexample', 'counting', 'counts', 'course', 'cover', 'covered', 'covering', 'covers', 'cq', 'cr', 'craig', 'crc', 'create', 'created', 'creates', 'creating', 'creation', 'criterion', 'critical', 'crucial', 'crude', 'cryptographic', 'cryptography', 'cs', 'csisza', 'cspecial', 'ct', 'cube', 'cuff', 'culture', 'cumbersome', 'curiously', 'current', 'curtis', 'curvature', 'curve', 'curves', 'customary', 'cv', 'cvetkovic', 'cw', 'cx', 'cx0', 'cycle', 'cycles', 'cycles2', 'cyclic', 'd0', 'd1', 'd2', 'd20', 'd21', 'd2i', 'd4', 'd8', 'damgn', 'danger', 'daniel', 'darpa', 'das', 'dass', 'data', 'database', 'david', 'days', 'db', 'de', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dec', 'decades', 'decay', 'decaying', 'decays', 'december', 'decide', 'decided', 'decides', 'decision', 'decisions', 'declare', 'declared', 'declares', 'declercq', 'decode', 'decoded', 'decoder', 'decodes', 'decoding', 'decomposes', 'decomposition', 'decrease', 'decreased', 'decreases', 'decreasing', 'dedicata', 'deduce', 'deep', 'deepened', 'deeper', 'defer', 'deferring', 'define', 'defined', 'defined6', 'defines', 'defining', 'definition', 'definitions', 'deg', 'degenerate', 'degree', 'degrees', 'degv', 'delay', 'delays', 'delete', 'deleting', 'delivering', 'delsarte', 'delving', 'demand', 'demanded', 'demands', 'dembowski', 'demonstrate', 'demonstrated', 'demonstrates', 'denoised', 'denoising', 'denominator', 'denote', 'denoted', 'denotes', 'densest', 'densities', 'density', 'department', 'departments', 'departure', 'depend', 'dependant', 'dependencies', 'dependent', 'depending', 'depends', 'depicting', 'dept', 'depth', 'der', 'derivation', 'derivative', 'derive', 'derived', 'deriving', 'des', 'descendant', 'descendants', 'descent', 'describe', 'described', 'describes', 'description', 'design', 'designed', 'designing', 'designs', 'desirable', 'desire', 'desired', 'despite', 'destroy', 'det', 'detail', 'detailed', 'details', 'detect', 'detected', 'detecting', 'detection', 'detections', 'determinant', 'determination', 'determine', 'determined', 'determines', 'determining', 'deterministic', 'deutsche', 'devedzic', 'develop', 'developed', 'developing', 'development', 'develops', 'deviation', 'deviations', 'device', 'devices', 'devised', 'devoted', 'df', 'dfg', 'di', 'diaconis', 'diag', 'diagonal', 'diagram', 'diam', 'diameter', 'did', 'die', 'differ', 'difference', 'differences', 'different', 'differential', 'differentiate', 'differently', 'differing', 'differs', 'difficult', 'difficulties', 'diffuse', 'diffusion', 'diggavi', 'digital', 'dij', 'dimension', 'dimensional', 'dimensions', 'dinitz', 'diploma', 'dirac', 'direct', 'directed', 'direction', 'directional', 'directions', 'directives', 'directly', 'directory', 'disadvantages', 'disappointing', 'discarded', 'discipline', 'disciplines', 'disconnect', 'discontinued', 'discontinuities', 'discontinuous', 'discover', 'discovered', 'discovery', 'discrete', 'discretisation', 'discretise', 'discriminant', 'discrimination', 'discuss', 'discussed', 'discusses', 'discussing', 'discussion', 'discussions', 'disjoint', 'diskretn', 'displays', 'diss', 'disseminate', 'dissemination', 'dissertation', 'distance', 'distances', 'distant', 'distinct', 'distinction', 'distinctness', 'distinguish', 'distinguishing', 'distortion', 'distributed', 'distribution', 'distributions', 'distributions12', 'divergence', 'diversity', 'divide', 'divided', 'divisible', 'dixon', 'dl', 'dmax', 'dmc', 'dmcs', 'dmv', 'dn', 'dn1', 'do', 'doc', 'dodecad', 'dodecads', 'dodecahedron', 'does', 'doesn', 'doi', 'doing', 'doklady', 'dom', 'domain', 'domains', 'dominate', 'dominated', 'dominating', 'done', 'doob', 'dordrecht', 'dot', 'double', 'doublecounting', 'doubles', 'doubly', 'dover', 'down', 'draft', 'draper', 'drawback', 'drawn', 'dreien', 'dreizehn', 'driven', 'driver', 'drives', 'driving', 'drop', 'drs', 'ds', 'dt', 'dtd', 'dual', 'due', 'dueck', 'duff', 'duration', 'durations', 'during', 'dwispc8', 'dx', 'dynamic', 'dynamically', 'dynamics', 'dynkin', 'dz', 'dθ', 'dσ', 'dℓ', 'e1', 'e2', 'e2iθj', 'e2t', 'e4', 'e8', 'ea', 'each', 'earlier', 'early', 'ease', 'easier', 'easiest', 'easily', 'easy', 'eb', 'ebenen', 'ebf', 'ebij', 'ebits', 'eccc', 'econometrics', 'economic', 'economics', 'ed', 'edge', 'edges', 'edited', 'edition', 'editor', 'editors', 'edmonds', 'eds', 'edu', 'education', 'edward', 'ee', 'eecs', 'eedback', 'efa', 'efaf', 'efal', 'efau', 'eferences', 'effect', 'effective', 'effectively', 'effects', 'efficiency', 'efficient', 'efficiently', 'effort', 'efron', 'eh', 'ei', 'ei1', 'eigenbasis', 'eigenphase', 'eigenphases', 'eigenschaft', 'eigenspace', 'eigenvalue', 'eigenvalues', 'eigenvector', 'eigenvectors', 'eight', 'eighth', 'eindhoven', 'eine', 'einer', 'either', 'eiα', 'eiαj', 'eiθj', 'ek', 'ek00', 'ekert', 'el', 'elaborate', 'elaborates', 'electrical', 'electron', 'electronic', 'electronically', 'electronics', 'elegant', 'element', 'elementary', 'elemente', 'elementen', 'elements', 'elias', 'eliminate', 'eliminating', 'elkies', 'elliott', 'elliptic', 'else', 'embedding', 'emd', 'emergence', 'emergency', 'emerging', 'emphasis', 'emphasize', 'emphasizes', 'emphasizing', 'empirical', 'empirically', 'employed', 'employs', 'emprically', 'empty', 'emre', 'en', 'enabled', 'enables', 'enc', 'encapsulate', 'encoded', 'encoder', 'encoding', 'encourage', 'encouragement', 'encouraging', 'encyclopedia', 'encyclopedias', 'end', 'endlicher', 'endogenous', 'endpoint', 'ends', 'energy', 'enf', 'enforce', 'engineer', 'engineering', 'english', 'engr', 'ength', 'enh', 'enhance', 'enhancement', 'enhances', 'enough', 'enr', 'enrich', 'enrq', 'enseignement', 'ensemble', 'ensure', 'ensures', 'ensuring', 'entire', 'entries', 'entropy', 'entry', 'enumerator', 'enumerators', 'environment', 'environments', 'eo', 'eprint', 'eq', 'eqs', 'equal', 'equalities', 'equality', 'equally', 'equals', 'equation', 'equations', 'equi', 'equilibrium', 'equiprobable', 'equivalence', 'equivalent', 'equivalently', 'er', 'era', 'erasure', 'erasures', 'erdo', 'ergodic', 'ern', 'erroneous', 'erroneously', 'error', 'errorcorrecting', 'errors', 'erweiterungen', 'es', 'esedog', 'esp', 'especially', 'essence', 'essentially', 'establish', 'established', 'establishes', 'establishing', 'establishment', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estimations', 'esult', 'eswaran', 'et', 'etc', 'eth', 'ethz', 'etzion', 'eu', 'eua', 'euclidean', 'eugenics', 'euler', 'eur', 'eurasip', 'europ', 'european', 'ev', 'eva', 'evaluate', 'evaluated', 'evaluation', 'evaluations', 'even', 'event', 'events', 'eventually', 'ever', 'every', 'everything', 'evgeny', 'evidently', 'evolution', 'evolve', 'evolvement', 'evolvements', 'ex', 'exact', 'exactly', 'exam', 'examination', 'examine', 'examined', 'examining', 'example', 'examples', 'exams', 'exceed', 'exceeds', 'except', 'exception', 'exceptional', 'exclude', 'excluding', 'exclusion', 'exclusively', 'execute', 'execution', 'exemplary', 'exercise', 'exercises', 'exhibit', 'exist', 'existence', 'existent', 'existing', 'exists', 'exits', 'exp', 'expanding', 'expect', 'expectation', 'expected', 'expects', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'explicitly', 'exploiting', 'explore', 'exponent', 'exponential', 'exponential1', 'exponentially', 'exponentials', 'exponentiation', 'exponents', 'exposition', 'express', 'expressed', 'expressing', 'expression', 'expressions', 'extend', 'extended', 'extending', 'extends', 'extensibility', 'extension', 'extensions', 'extensive', 'extensively', 'extent', 'extra', 'extract', 'extracting', 'extraction', 'extraordinarily', 'extremal', 'extreme', 'extremely', 'extremum', 'ez', 'eθ', 'eπ', 'f1', 'f2', 'f24', 'f2n', 'f3', 'f4', 'fa', 'fa9550', 'face', 'faced', 'fach', 'facing', 'fact', 'factor', 'factors', 'facts', 'faculty', 'fahnenhomogene', 'fail', 'failed', 'failure', 'falls', 'falmer', 'faloutsos', 'false', 'falsified', 'familiar', 'family', 'famous', 'fano', 'far', 'fast', 'faster', 'favor', 'fe', 'feasible', 'feature', 'features', 'fec', 'fed', 'feedback', 'fekri', 'felt', 'few', 'fidelity', 'field', 'fields', 'fifteen', 'fifteenth', 'fifth', 'fifty', 'fig', 'figure', 'figures', 'fijalkow', 'file', 'fileclassloader', 'filepap', 'fill', 'filling', 'filter', 'final', 'finalizing', 'finallly', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finer', 'finest', 'finishes', 'finite', 'first', 'firstly', 'firststage', 'fisher', 'fit', 'fitting', 'five', 'fix', 'fixed', 'fixing', 'fl', 'flag', 'flaw', 'flexibility', 'flexible', 'flip', 'floors', 'flowers', 'fn', 'fnq', 'focus', 'focused', 'focuses', 'focusing', 'fois', 'fold', 'folds', 'follow', 'followed', 'following', 'follows', 'follows4', 'fonction', 'fonctions', 'for', 'forbidden', 'forced', 'forces', 'fore', 'forget', 'form', 'formal', 'formalizations', 'formally', 'format', 'formation', 'formed', 'former', 'forms', 'formula', 'formulas', 'formulation', 'formulations', 'forney', 'forschungsgemeinschaft', 'forty', 'forward', 'found', 'foundation', 'foundations', 'four', 'fourteen', 'fourth', 'fq', 'fr', 'fraction', 'fractional', 'fragments', 'frame', 'framework', 'france', 'francis', 'frasson', 'fre', 'free', 'freedom', 'freedom9', 'french', 'frequency', 'friends', 'friendships', 'fripertinger', 'from', 'fruitful', 'fu', 'fuer', 'fujimoto', 'full', 'fully', 'function', 'functional', 'functionalities', 'functionality', 'functions', 'fundamental', 'fundamentals', 'funded', 'further', 'furthermore', 'furtherpk', 'furtherǫ', 'fusco', 'future', 'g0', 'g2', 'gage', 'gain', 'galeotti', 'gallager', 'galois', 'game', 'games', 'gap', 'garcke', 'gas', 'gastpar', 'gates', 'gathered', 'gauss', 'gaussian', 'gaussiannoise', 'gauthier', 'gave', 'gegenbauer', 'general', 'generality', 'generalization', 'generalizations', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generator', 'generators', 'generic', 'geom', 'geometer', 'geometric', 'geometrical', 'geometrically', 'geometries', 'geometry', 'german', 'germany', 'get', 'getclass', 'getmethod', 'getnodename', 'gets', 'getting', 'gf', 'gi', 'gibbs', 'gils', 'ginn', 'girth', 'give', 'given', 'gives', 'giving', 'gk', 'gleason', 'global', 'globally', 'globalvalues', 'gm', 'gn', 'gn2', 'go', 'goal', 'goals', 'goes', 'goethals', 'going', 'golay', 'goldstone', 'good', 'gore', 'government', 'govindan', 'gr', 'grade', 'gradient', 'gradients', 'gradually', 'graduate', 'graham', 'grain', 'grands', 'grant', 'graph', 'graphical', 'graphs', 'grassman', 'gratefully', 'gray', 'great', 'greater', 'greatest', 'greedy', 'green', 'gregory', 'grid', 'grids', 'griffiths', 'gro', 'gro96', 'group', 'groupes', 'groups', 'grouptheoretical', 'grover', 'growing', 'grows', 'gruppen', 'gt', 'gtπ', 'gu', 'guarantee', 'guaranteed', 'guarantees', 'gui', 'gulliver', 'gθ', 'gπ', 'gπt', 'h0', 'h02ǫ', 'h0ǫ', 'h1', 'h2ǫ', 'had', 'hadamard', 'haemers', 'haifa', 'half', 'hall', 'halves', 'hamburg', 'hamming', 'han', 'hand', 'handbook', 'handheld', 'handle', 'hanging', 'hannel', 'happening', 'happens', 'harada', 'hard', 'harder', 'harlow', 'harmless', 'harmonic', 'harmonically', 'harper', 'has', 'hat', 'have', 'having', 'he', 'headers', 'health', 'healthcare', 'heavy', 'heden', 'heidelberg', 'helleseth', 'help', 'helpful', 'hence', 'henceforth', 'hensel', 'her', 'here', 'hering', 'hermitian', 'herrera', 'hessler', 'heterogeneity', 'heterogeneous', 'heuristic', 'hexacode', 'hf', 'hfalse', 'hierarchy', 'high', 'higher', 'highest', 'highlight', 'highly', 'hilbert', 'hill', 'him', 'hint', 'hints', 'hirschfeld', 'his', 'histogram', 'histograms', 'historical', 'history', 'hitting', 'hn', 'hoc', 'hoeffding', 'hold', 'holds', 'holland', 'holmes', 'home', 'homogeneous', 'homogeneously', 'hong', 'honkala', 'hope', 'hoped', 'host', 'hours', 'how', 'however', 'hp', 'hs', 'hsv', 'ht', 'html', 'http', 'https', 'htε', 'huber', 'hue', 'huffman', 'hughes', 'huht', 'human', 'hundred', 'hurting', 'hut', 'hutter', 'hw', 'hwα', 'hx', 'hxy', 'hy', 'hybrid', 'hypercube', 'hyperoval', 'hyperovals', 'hyperplane', 'hyperplanes', 'hypotheses', 'hypothesis', 'hypothetical', 'hyx', 'hz', 'hµ', 'hµz', 'hǫ', 'hǫ0', 'hǫα', 'hα', 'hαǫ', 'hβ', 'hε', 'hηi', 'hξ', 'hφ0', 'i0', 'i1', 'i12', 'i2', 'i3', 'i4', 'i6', 'icece', 'icm', 'icosahedron', 'id', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identification', 'identified', 'identify', 'identifying', 'identities', 'identity', 'ieee', 'if', 'ignored', 'ignoring', 'ihvi', 'ihwα', 'ihx', 'ihµz', 'ihθ', 'ihφx', 'ihφz', 'ii', 'iid', 'iii', 'ij', 'ijcv', 'ik', 'ik2', 'illinois', 'illness', 'illustrate', 'illustrated', 'illustrates', 'illustrating', 'illustration', 'illustrative', 'im', 'image', 'images', 'imagine', 'immaterial', 'immediate', 'immediately', 'implement', 'implementation', 'implemented', 'implementing', 'implications', 'implicitly', 'implied', 'implies', 'imply', 'importance', 'important', 'importantly', 'impose', 'imposed', 'impossible', 'impression', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'imre', 'in', 'inc', 'inceptions', 'incidence', 'include', 'includes', 'including', 'inclusion', 'incomplete', 'inconsequential', 'incorporating', 'incorrect', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'indebted', 'indeed', 'independent', 'independent4', 'independently', 'indepth', 'index', 'indexed', 'indicate', 'indicates', 'indicating', 'indication', 'indicator', 'indicators', 'indices', 'indirectly', 'individually', 'individuals', 'induced', 'inen', 'inequalities', 'inequality', 'inf', 'infeasibility', 'infeasible', 'infimum', 'infinite', 'infinity', 'influence', 'influencing', 'infocom', 'inform', 'information', 'informatsii', 'infrared', 'ingeniously', 'ingredients', 'inherent', 'initial', 'initialize', 'initialized', 'initially', 'initiated', 'inner', 'input', 'inputs', 'inquiring', 'inscribed', 'insensitive', 'inserting', 'inside', 'insights', 'insignificant', 'insignificantly', 'inspecting', 'inspired', 'inst', 'install', 'installation', 'installed', 'instance', 'instead', 'institut', 'institute', 'instruction', 'instructive', 'int', 'integer', 'integers', 'integral', 'integrated', 'integrates', 'integration', 'intelligence', 'intelligent', 'intelligente', 'intended', 'intense', 'intensity', 'intentionally', 'interact', 'interaction', 'interactions', 'interactive', 'interest', 'interested', 'interesting', 'interestingly', 'interface', 'interface1', 'interfaces', 'interference', 'intermittent', 'internal', 'internally', 'international', 'internet', 'interplay', 'interpolant', 'interpolants', 'interpolating', 'interpret', 'interpretation', 'interpreted', 'interrelations', 'interscience', 'intersect', 'intersecting', 'intersection', 'intersects', 'interval', 'interwoven', 'intial', 'into', 'introduce', 'introduced', 'introduces', 'introducing', 'introduction', 'introspection', 'intuition', 'intuitive', 'intuitively', 'invariant', 'invariants', 'inventiones', 'inverse', 'inversely', 'investigate', 'investigated', 'investigating', 'investigation', 'investigations', 'invoke', 'invokes', 'involes', 'involve', 'involved', 'involves', 'involving', 'io', 'ioannides', 'ionin', 'iq', 'iqc', 'ir', 'ire', 'irreducible', 'irregular', 'irrespective', 'is', 'is3', 'isaac', 'isit', 'isita', 'isometries', 'isomorphic', 'isomorphism', 'isotropic', 'iss', 'issled', 'issue', 'issues', 'ist', 'it', 'italy', 'iterate', 'iterated', 'iterates', 'iterating', 'iteration', 'iterations', 'iterative', 'ith', 'itmanet', 'ito', 'itoh', 'its', 'itself', 'itzkovitz', 'iu', 'iv', 'iwuc', 'ix', 'ixx', 'iyw', 'izvestiya', 'iα', 'iαj', 'iθj', 'j12', 'j4', 'j6', 'ja', 'jackknife', 'jackson', 'jacobi', 'jacqueline', 'jadohealth', 'jakob', 'jamin', 'jan', 'jansen', 'janson', 'java', 'jdbc', 'jdsk', 'je', 'jensen', 'jeong', 'ji', 'jilles', 'jk', 'jn', 'job', 'joel', 'john', 'johnson', 'join', 'joint', 'jointly', 'journal', 'journey', 'joy', 'jr', 'jsp', 'jul', 'july', 'jun', 'junctions', 'june', 'jung', 'jungnickel', 'juni', 'just', 'justified', 'justifies', 'k2', 'k22', 'kailath', 'kang', 'karush', 'kashtan', 'kaski', 'katsman', 'kay', 'keep', 'keeping', 'keevash', 'kempe', 'kennedy', 'kept', 'kerber', 'kernel', 'kernels', 'keshavan', 'keug1', 'key', 'keyes', 'keyword', 'keywords', 'khachatrian', 'kilgus', 'kim', 'kind', 'kinds', 'kirkman', 'kissing', 'kit95', 'kitaev', 'kkt', 'kl', 'kleinberg', 'kn', 'know', 'knowing', 'knowledge', 'known', 'knows', 'ko', 'koetter', 'kohnert', 'koric', 'kornhuber', 'kpy', 'kpz', 'kq', 'kr', 'kramer', 'krause', 'krawtchouk', 'krishnan', 'ksk', 'ksk2', 'kth', 'kudryashov', 'kugeln', 'kuhn', 'kullback', 'kumar', 'kw', 'kw1α', 'kwy', 'kwα', 'kwα1', 'kyoto', 'kyushu', 'kµ', 'kα', 'kφe0', 'kφx', 'kψk', 'kψk2', 'la', 'lab', 'label', 'labeled', 'labeling', 'labelled', 'labels', 'laboratories', 'laboratory', 'labs', 'lam', 'lamb', 'land', 'language', 'lapidoth', 'large', 'larger', 'largest', 'larms', 'las', 'lassen', 'last', 'lastly', 'later', 'latin', 'latter', 'lattice', 'lattices', 'launch', 'law', 'laws', 'layer', 'layered', 'layers', 'ldpc', 'lead', 'leading', 'leads', 'leaf', 'learn', 'learning', 'least', 'leave', 'leaves', 'leaving', 'lect', 'lectnotes', 'lecture', 'lectures', 'led', 'leech', 'left', 'legendresymbol', 'leibler', 'leiva', 'lemma', 'lemmas', 'lemmata', 'lena', 'lends', 'lenght', 'length', 'lengths', 'lenz', 'leont', 'les', 'less', 'let', 'lets', 'lett', 'letter', 'letters', 'letting', 'leung', 'level', 'levels', 'levenshtein', 'levensthein', 'lewis', 'lexicographic', 'lexicographically', 'lias', 'lie', 'lies', 'lifted', 'light', 'like', 'likelihood', 'likely', 'lim', 'limit', 'limited', 'limits', 'limk', 'limx', 'limǫ', 'limβ', 'lin', 'line', 'linear', 'linearly', 'linearly14', 'liner', 'lines', 'link', 'links', 'lint', 'lipschitz', 'list', 'listed', 'literature', 'litsyn', 'little', 'liu', 'lizhong', 'lloyd', 'ln', 'lnc2', 'lncs', 'lne', 'loadclass', 'loader', 'lobstein', 'local', 'locate', 'locations', 'lock', 'locked', 'log', 'log2', 'logarithmic', 'logic', 'login', 'lond', 'london', 'long', 'longer', 'longman', 'look', 'looking', 'looks', 'looped', 'looping', 'loops', 'loseness', 'losing', 'loss', 'lost', 'lot', 'lova', 'low', 'lower', 'lowest', 'lri', 'lu', 'luby', 'luczak', 'luminosity', 'lumped', 'luxembourg', 'luxury', 'm0', 'm1', 'm11', 'm12', 'm2', 'm22', 'm23', 'm24', 'm3', 'm4', 'mac', 'macchiavello', 'machine', 'macro', 'macroscopic', 'macwilliams', 'made', 'mag', 'magniez', 'magnitude', 'main', 'maindetect', 'mainly', 'maintained', 'maintaining', 'maintains', 'major', 'majority', 'make', 'makers', 'makes', 'making', 'malik', 'management', 'manipulations', 'manner', 'manuscript', 'many', 'map', 'mapped', 'mapping', 'mappings', 'maps', 'mar', 'marcellin', 'march', 'margin', 'marked', 'market', 'markov', 'marshall', 'martingale', 'masiero', 'masnick', 'mass', 'massachusetts', 'master', 'mat', 'match', 'matched', 'material', 'materials', 'math', 'mathematicae', 'mathematical', 'mathematics', 'mathematik', 'mathieu', 'matrices', 'matrix', 'matta', 'matter', 'matthias', 'mattson', 'mauritius', 'max', 'max1', 'maxi', 'maximal', 'maximization', 'maximize', 'maximizers', 'maximizing', 'maximum', 'maxz', 'maxzn', 'may', 'mbo', 'mcgraw', 'mcmillan', 'me', 'mean', 'meaning', 'means', 'meantime', 'meanwhile', 'measurable', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'mechanical', 'mechanism', 'mechanisms', 'medina', 'medt', 'meet', 'meeting', 'meets', 'member', 'members', 'memorial', 'memory', 'memoryless', 'mention', 'mentioned', 'mentioning', 'merely', 'merriman', 'mes', 'mesh', 'mesner', 'message', 'messages', 'messagewise', 'messina', 'metal', 'meter', 'method', 'methodology', 'methods', 'metz', 'meyer', 'mg', 'mhuber', 'mi', 'michael', 'mid', 'might', 'mij', 'miklos', 'mikula', 'milan', 'milestones', 'milo', 'min', 'mini', 'minima', 'minimal', 'minimisation', 'minimise', 'minimised', 'minimises', 'minimising', 'minimize', 'minimized', 'minimizes', 'minimizing', 'minimum', 'ministry', 'ministère', 'minj', 'minqy', 'minute', 'miracle', 'misconceptions', 'misleading', 'missed', 'misseddetection', 'mistaken', 'mit', 'mitter', 'mitzenmacher', 'mixing', 'mj', 'mk', 'ml', 'mm', 'mmse', 'mn', 'mn07', 'mnrs07', 'mo', 'mobile', 'mod', 'mode', 'model', 'modeling', 'models', 'moderately', 'modern', 'modest', 'modica', 'modification', 'modifications', 'modified', 'modify', 'modifying', 'modulation', 'mog', 'mohsen', 'moire', 'moment', 'monograph', 'monotone', 'monotonically', 'monotonicity', 'montanar', 'montanari', 'monte', 'monthly', 'montreal', 'more', 'morelos', 'moreover', 'morris', 'mortimer', 'mortola', 'mosca', 'most', 'mostly', 'motifs', 'motion', 'motivate', 'motivated', 'motivates', 'motivation', 'mountain', 'move', 'moves', 'moving', 'mr', 'mrt', 'ms', 'mss07', 'mt', 'much', 'muller', 'multi', 'multiclassloader', 'multigrid', 'multilevel', 'multiphase', 'multiple', 'multiplechoice', 'multiples', 'multiplexing', 'multiplication', 'multiplications', 'multiplicative', 'multiplied', 'multiply', 'multiplying', 'multrigrid', 'mumford', 'musin', 'must', 'mutual', 'mutually', 'mx', 'myopic', 'mβ', 'n0', 'n00014', 'n0q', 'n1', 'n2', 'n2k', 'n2kα', 'n2l', 'n2rα', 'n3', 'nakib', 'nakibog', 'name', 'named', 'namely', 'national', 'nato', 'nats', 'natural', 'naturally', 'nature', 'naval', 'navigation', 'nayak', 'nd', 'ndu', 'ne', 'nearest', 'nearly', 'neb', 'nebe', 'neburg', 'neces', 'necessarily', 'necessary', 'necessitate', 'necessity', 'necked', 'need', 'needed', 'needless', 'needs', 'negative', 'negativity', 'negligible', 'neighbor', 'neighborhood', 'neighboring', 'neighbors', 'neighbouring', 'neighbours', 'neither', 'nemd', 'neo', 'neq', 'nested', 'nestler', 'netherlands', 'netw', 'network', 'networking', 'networks', 'neumaier', 'neumann', 'neuss', 'never', 'nevertheless', 'new', 'newinstance', 'newly', 'newman', 'newton', 'next', 'nh', 'ni', 'nielsen', 'nig', 'nighttime', 'nik', 'nine', 'nineteenth', 'njas', 'nk', 'nkα', 'nl', 'nn', 'no', 'node', 'nodes', 'noise', 'noiseless', 'noisy', 'non', 'nonadjacent', 'nonbinary', 'nonblock', 'nonempty', 'nonetheless', 'nonexistence', 'nonlinear', 'nonn', 'nonnegative', 'nonnormalized', 'nonuniform', 'nonzero', 'nor', 'norm', 'normal', 'normalization', 'normalized', 'normalizing', 'north', 'northholland', 'nos', 'not', 'notably', 'notation', 'notational', 'notations', 'note', 'noted', 'notes', 'notes5', 'nothing', 'notice', 'notices', 'noting', 'notion', 'notionpof', 'notions', 'nov', 'novel', 'now', 'nowadays', 'np', 'np2', 'nr', 'nr1', 'ns', 'nsa', 'nserc', 'nsα', 'nth', 'ntroduction', 'null', 'number', 'numbered', 'numbers', 'numerator', 'numerical', 'numerically', 'numerically12', 'numerics', 'numerische', 'numerous', 'nur', 'nv00', 'nx', 'ny', 'nyi', 'nyquist', 'nǫ', 'nα', 'nν', 'nνn', 'nφ', 'obesity', 'object', 'objection', 'objective', 'objects', 'observation', 'observations', 'observe', 'observed', 'observes', 'observing', 'obstacle', 'obstacles', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'occasional', 'occupy', 'occur', 'occurring', 'occurs', 'oct', 'octad', 'octads', 'octahedron', 'october', 'odbc', 'odd', 'odel', 'odes', 'odlyzko', 'odory', 'of', 'off', 'offer', 'offers', 'office', 'official', 'offs', 'often', 'oh', 'oi', 'old', 'older', 'olof', 'omitted', 'omitting', 'on', 'once', 'onclusions', 'one', 'ones', 'onion', 'onlinelearning', 'only', 'ontario', 'onto', 'open', 'oper', 'operating', 'operation', 'operations', 'operator', 'operators', 'opinion', 'opinions', 'opportunity', 'opposed', 'opposite', 'optimal', 'optimalities', 'optimality', 'optimisation', 'optimization', 'optimizations', 'optimize', 'optimized', 'optimizing', 'optimum', 'option', 'optional', 'options', 'or', 'oral', 'order', 'ordered', 'ordering', 'orderings', 'orders', 'ordinary', 'ordnen', 'org', 'organization', 'organized', 'origin', 'original', 'originally', 'origins', 'orlando', 'orsay', 'orthogonal', 'orthonormal', 'osaka', 'osc', 'oscillating', 'oscillation', 'osher', 'osthus', 'otation', 'other', 'others', 'otherwise', 'otions', 'our', 'ours', 'ourself', 'ourselves', 'out', 'outcome', 'outcomes', 'outer', 'outline', 'outlined', 'outperforms', 'output', 'outputs', 'outside', 'oval', 'ovals', 'over', 'overall', 'overlap', 'overlapping', 'overrelaxation', 'overview', 'overweight', 'own', 'ox', 'ox1', 'oxford', 'ozarow', 'ozbudak', 'p0', 'p00', 'p0gt', 'p1', 'p19', 'p2', 'p2k', 'p2r', 'p3', 'p481', 'p4c', 'p5', 'p6', 'pack', 'packed', 'packet', 'packing', 'packings', 'page', 'pages', 'paid', 'paige', 'pair', 'pairs', 'palek', 'paley', 'palmtop', 'pam', 'pand', 'papadimitriou', 'paper', 'papers', 'paradigm', 'paradigms', 'parallel', 'parallelisms', 'parameter', 'parameters', 'parametric', 'parametrisation', 'parent', 'paris', 'parity', 'park', 'parks', 'parser', 'part', 'partial', 'partially', 'participants', 'particles', 'particular', 'particularly', 'partition', 'partitioned', 'partitioning', 'partitions', 'partly', 'parts', 'pasch', 'pass', 'passed', 'passes', 'path', 'paths', 'pattern', 'patterns', 'paulo', 'payload', 'pde', 'pdes', 'pdf', 'pe', 'pecial', 'peculiar', 'pedagogical', 'peeling', 'pei', 'pem', 'penalizing', 'per', 'perasure', 'perdachi', 'peredaci', 'perfect', 'perform', 'performance', 'performances', 'performed', 'performing', 'perhaps', 'peridachi', 'perimeter', 'periods', 'permutation', 'permutations', 'permutationsgruppen', 'perona', 'personalized', 'persons', 'perspective', 'pet', 'peter', 'peterson', 'petitot', 'pfender', 'pg', 'pgt', 'ph', 'phase', 'phases', 'phd', 'phelps', 'phenomena', 'phenomenon', 'philadelphia', 'philips', 'philos', 'phone', 'photograph', 'photographed', 'phys', 'physica', 'physical', 'physician', 'physicians', 'physics', 'physiology', 'pi', 'pick', 'pickert', 'pictures', 'piece', 'pieces', 'piecewise', 'pij', 'pim', 'pinsker', 'piper', 'piracicaba', 'pishro', 'pixel', 'pixels', 'pk', 'pkij', 'pkji', 'place', 'placement', 'places', 'plain', 'plane', 'planes', 'plants', 'platforms', 'plausible', 'play', 'plays', 'plength', 'pless', 'plet', 'plu', 'plus', 'plusieurs', 'pm', 'pn', 'pn1', 'pn2', 'pny', 'pohang', 'point', 'pointed', 'pointing', 'points', 'pointwise', 'poisson', 'policy', 'poltyrev', 'polylog', 'polynomial', 'polynomials', 'pontiveros', 'poor', 'poorly', 'popular', 'populous', 'porto', 'portugal', 'pose', 'posed', 'positions', 'positive', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'posteriori', 'postmodern', 'postponed', 'postponement', 'potential', 'potentially', 'potenza', 'pottonen', 'poulliat', 'power', 'powers', 'pp', 'ppendix', 'ppn', 'pr', 'pr1', 'prabhakar', 'practical', 'practice', 'practitioners', 'prange', 'precise', 'precisely', 'precision', 'predecessor', 'predetermined', 'preece', 'prefer', 'prefix', 'preliminaries', 'preliminary', 'prepare', 'prepared', 'preprint', 'prescribed', 'presence', 'present', 'presentation', 'presented', 'presently', 'presents', 'preserve', 'preserved', 'preserves', 'press', 'prevailing', 'previous', 'previously', 'prg', 'prg0', 'primary', 'prime', 'primitive', 'principal', 'principle', 'principles', 'print', 'prior', 'priori', 'priority', 'privacy', 'prob', 'proba', 'probab', 'probabilistic', 'probabilities', 'probability', 'probability1', 'probable', 'probably', 'probl', 'problem', 'problematic', 'problems', 'problemy', 'proc', 'procedure', 'procedures', 'proceed', 'proceedings', 'proceeds', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'product', 'products', 'professor', 'professors', 'program', 'programming', 'programs', 'progress', 'proietti', 'project', 'projected', 'projecting', 'projection', 'projective', 'projector', 'projektive', 'promising', 'promotes', 'proof', 'proofs', 'propagation', 'properly', 'properties', 'property', 'proportional', 'proposals', 'propose', 'proposed', 'proposing', 'proposition', 'protect', 'protected', 'protecting', 'protection', 'protects', 'protocol', 'protocols', 'prove', 'proved', 'proven', 'proves', 'provide', 'provided', 'providence', 'provides', 'providing', 'province', 'proving', 'provision', 'ps', 'pseudo', 'pseudocode', 'pt', 'ptf', 'ptij', 'ptrue', 'pu', 'pub', 'public', 'publication', 'publications', 'published', 'publishers', 'publishing', 'pulkit', 'pulse', 'punctured', 'pure', 'pures', 'purpose', 'purposes', 'px', 'px1', 'px2', 'pxi', 'pxk', 'pxn', 'pxy', 'py', 'py1', 'pyx', 'pz', 'pθj', 'pλδ', 'pλδe', 'pπ', 'pτ', 'q0', 'q1', 'q3', 'qap', 'qd', 'qh', 'qht', 'qhtε', 'qi', 'qij', 'qk', 'qm', 'qn', 'qnpr', 'qq', 'qr', 'qt', 'qtr', 'quadrangle', 'quadratic', 'quadratically', 'quadric', 'quadrupelsysteme', 'quadruple', 'quality', 'quant', 'quantite', 'quantitie', 'quantities', 'quantity', 'quantization', 'quantizes', 'quantum', 'quantumly', 'quantumworks', 'quarterly', 'quartic', 'quaternary', 'qubit', 'question', 'questionaires', 'questionnaire', 'questionnaires', 'questions', 'quickly', 'quite', 'qy', 'r0', 'r1', 'r2', 'r24', 'r3', 'r4', 'r5', 'radius', 'radovic', 'rahnavard', 'raises', 'ramsey', 'randgraph', 'randgraph0', 'random', 'randomization', 'randomized', 'randomly', 'randomness', 'range', 'rank', 'rapid', 'rapidly', 'rare', 'rarely', 'rate', 'rates', 'rather', 'ratio', 'rational', 'raton', 'ray', 'rc', 'rc2', 'rd', 're', 'reach', 'reachable', 'reached', 'reaches', 'reaching', 'reachpan', 'readable', 'readapt', 'reader', 'readily', 'reading', 'ready', 'real', 'realization', 'realizations', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reals', 'realworld', 'reason', 'reasonable', 'reasonably', 'reasons', 'reboot', 'recall', 'recalling', 'recapitulate', 'receive', 'received', 'receiver', 'receivers', 'recent', 'recently', 'recherche', 'recognize', 'recognized', 'recognizing', 'recommend', 'reconciliation', 'reconstruction', 'recovered', 'recursion', 'recursive', 'recursively', 'red', 'redefine', 'redesign', 'redesigned', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'reed', 'ref', 'refer', 'referees', 'reference', 'references', 'referral', 'referrals', 'referred', 'referring', 'refers', 'refinability', 'refinable', 'refinement', 'refinements', 'refining', 'reflection', 'reflections', 'reflects', 'refrain', 'regarded', 'regarding', 'regardless', 'regime', 'region', 'regions', 'register', 'regular', 'regularity', 'reidel', 'reine', 'reinterpreted', 'reject', 'rejected', 'rejects', 'rel', 'relabeled', 'relate', 'related', 'relates', 'relating', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relaxed', 'relay', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'relies', 'reliminaries', 'rely', 'relying', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remark', 'remarkable', 'remarks', 'remedied', 'remembering', 'reminds', 'remmers', 'remote', 'removal', 'removed', 'removing', 'renamed', 'rend', 'renders', 'rendic', 'renner', 'reordering', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetition', 'repetitions', 'rephrased', 'replace', 'replaced', 'replaces', 'replacing', 'replication', 'report', 'reported', 'reports', 'represent', 'representation', 'representatives', 'represented', 'representing', 'represents', 'reprint', 'reproducible', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'res', 'research', 'researchers', 'reserved', 'residue', 'residues', 'resilient', 'resolution', 'resolved', 'resource', 'resources', 'resp', 'respect', 'respective', 'respectively', 'rest', 'restoration', 'restrict', 'restricted', 'restriction', 'restrictions', 'result', 'resulted', 'resulting', 'results', 'results13', 'retain', 'retransmission', 'retransmissions', 'retransmitted', 'return', 'returns', 'reuse', 'reused', 'reusing', 'rev', 'revealed', 'reveals', 'reversal', 'reverses', 'reversibility', 'reversible', 'review', 'reviews', 'revised', 'revisit', 'revisited', 'rewarding', 'rewrite', 'rewriting', 'rewritten', 'rg', 'rgb', 'ri', 'ric', 'rich', 'richardson', 'richter', 'richterp', 'rifa', 'right', 'rigorous', 'rij', 'rimoldi', 'ring', 'riordan', 'rise', 'rivosh', 'rj', 'rk', 'rl', 'rm', 'rn', 'rn1', 'rn3', 'rner', 'road', 'robert', 'robot', 'robust', 'robustness', 'rocky', 'rodney', 'rogers', 'roland', 'role', 'roles', 'romanov', 'roofs', 'room', 'root', 'rooted', 'roots', 'rotate', 'rotation', 'roth', 'rothkugel', 'roughly', 'round', 'rounded', 'row', 'rows', 'roy', 'royal', 'rq', 'rs', 'rth', 'rucinski', 'rudolf', 'rule', 'rummel', 'run', 'running', 'runs', 'runtime', 'rw', 'ryser', 'rθ', 'rω', 's0', 's1', 's12', 's2', 's2k', 's3', 'saberi', 'sahai', 'said', 'sake', 'same', 'sample', 'samples', 'sampling', 'san', 'san08', 'sand', 'sanghavi', 'sankaranarayanan', 'santha', 'sao', 'sapiro', 'sarily', 'satellite', 'satisfactory', 'satisfied', 'satisfies', 'satisfy', 'satisfying', 'saturation', 'say', 'saying', 'says', 'scalar', 'scale', 'scaled', 'scales', 'scaling', 'scan', 'scarcely', 'scenario', 'scenarios', 'schalkwijk', 'schalkwijk6', 'scheduling', 'scheme', 'schemes', 'schoenmakers', 'school', 'schu', 'schwartz', 'schwarz', 'sci', 'science', 'sciences', 'scope', 'scratch', 'se', 'search', 'searches', 'sec', 'second', 'secondly', 'secondmoment', 'seconds', 'secret', 'section', 'sections', 'secure', 'security', 'see', 'seek', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'segment', 'segmentation', 'segmentations', 'segmented', 'segments', 'seidel', 'selected', 'selecting', 'selection', 'selects', 'self', 'selfdual', 'sem', 'semakov', 'semidefinite', 'seminar', 'send', 'sending', 'sends', 'sense', 'sensitive', 'sensitivity', 'sent', 'sep', 'separate', 'separated', 'separately', 'separates', 'separation', 'separator', 'sept', 'september', 'sequel', 'sequence', 'sequences', 'sequential', 'sequentially', 'ser', 'series', 'serious', 'serve', 'server', 'services', 'servlet', 'seshadri', 'session', 'set', 'sets', 'settest', 'setting', 'settings', 'setup', 'seven', 'seventeenth', 'several', 'sh', 'shaded', 'shah', 'shalkwijk', 'shall', 'shannon', 'sharing', 'sharp', 'sharper', 'shashi', 'shed', 'shell', 'shells', 'shen', 'shenker', 'shenorr', 'shenvi', 'shift', 'shifted', 'shimamoto', 'shin', 'shiriaev', 'shiromoto', 'shokrollahi', 'short', 'shortened', 'shorter', 'shortest', 'shorthand', 'shortly', 'should', 'show', 'showed', 'showing', 'shown', 'shows', 'shrikhande', 'shutdowns', 'si', 'siam', 'sich', 'side', 'sides', 'sigcomm', 'sign', 'signal', 'signaling', 'signals', 'significance', 'significant', 'significantly', 'similar', 'similarly', 'simonis', 'simple', 'simpler', 'simplest', 'simplex', 'simplicity', 'simplifications', 'simplified', 'simplifies', 'simplify', 'simplifying', 'simply', 'simulating', 'simulation', 'simultaneous', 'simultaneously', 'sin', 'sin2', 'since', 'sinclair', 'singapore', 'singe', 'single', 'singleerror', 'singular', 'situation', 'situations', 'six', 'sixteenth', 'sixty', 'size', 'sizes', 'sk', 'skills', 'skip', 'skw03', 'sl', 'slides', 'slight', 'slightly', 'slim', 'sloane', 'slowly', 'small', 'smaller', 'smallest', 'smooth', 'smoothers', 'smoothing', 'smoothly', 'smoothness', 'sn', 'snapshots', 'snover', 'snr', 'so', 'soc', 'social', 'society', 'socrates', 'soda', 'software', 'sole', 'solids', 'solov', 'solution', 'solutions', 'solve', 'solved', 'solver', 'solvers', 'solving', 'some', 'something', 'sometimes', 'somewhat', 'sons', 'soon', 'sought', 'sounds', 'source', 'sources', 'sov', 'space', 'spacecraft', 'spaced', 'spaces', 'spacing', 'span', 'spanned', 'sparse', 'sparsity', 'spatial', 'spb', 'speak', 'special', 'specialty', 'specific', 'specifically', 'specification', 'specified', 'specifies', 'specify', 'specifying', 'spectral', 'spectrum', 'speed', 'speedup', 'spencer', 'spend', 'sphere', 'spherepacking', 'spheres', 'spherical', 'spielman', 'spiral', 'spite', 'split', 'splitting', 'spoil', 'sporadic', 'springer', 'spruijt', 'spurious', 'sqs', 'square', 'squared', 'squares', 'sr', 'ssc', 'st', 'stabilizer', 'stable', 'stage', 'stages', 'staircasing', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanford', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'static', 'stating', 'stationary', 'statist', 'statistic', 'statistical', 'statistically', 'statistics', 'steal', 'steer', 'stefanos', 'steffen', 'steger', 'stein', 'steiner', 'steinersche', 'stemann', 'step', 'stepping', 'steps', 'sterga', 'sth', 'stichtenoth', 'stick', 'stiffness', 'still', 'stinson', 'stoc', 'stochastic', 'stop', 'stopping', 'stops', 'storage', 'store', 'stored', 'storme', 'stoth', 'straightforward', 'strange', 'strategies', 'strategy', 'straße', 'strengthen', 'strengthening', 'strict', 'strictly', 'string', 'strings', 'strong', 'stronger', 'struct', 'structural', 'structure', 'structures', 'student', 'students', 'studied', 'studies', 'study', 'studying', 'stylized', 'sub', 'subclass', 'subcode', 'subdomains', 'subgradient', 'subgraph', 'subgraphs', 'subgroup', 'subject', 'sublinear', 'submessage', 'submessages', 'submitted', 'subpart', 'subscript', 'subscripts', 'subsection', 'subsections', 'subsections1', 'subsequent', 'subsequently', 'subset', 'subsets', 'subspace', 'subspaces', 'substantial', 'substantially', 'substitute', 'substituting', 'substitution', 'substructures', 'subtler', 'subtracting', 'subword', 'subwords', 'succesive', 'success', 'successful', 'successfully', 'successive', 'successively', 'such', 'sud', 'sudan', 'suen', 'suffices', 'sufficiency', 'sufficient', 'sufficiently', 'suggest', 'suggested', 'suggesting', 'suggests', 'suitable', 'sum', 'summarise', 'summarize', 'summarized', 'summarizes', 'summarizing', 'summary', 'summation', 'summing', 'sup', 'super', 'superimposed', 'superior', 'supn', 'supp', 'suppl', 'support', 'supported', 'supports', 'suppose', 'supposition', 'supq', 'supremum', 'supérieur', 'supσ', 'sur', 'sure', 'surprising', 'surprisingly', 'surrogate', 'survey', 'surveys', 'sussex', 'swap', 'swapped', 'swiercz', 'swiftness', 'swing', 'switch', 'switches', 'switching', 'swoh', 'sym2', 'sym36', 'symbol', 'symbols', 'symmetric', 'symmetrical', 'symmetrized', 'symmetry', 'symp', 'symposium', 'synchronization', 'syst', 'syste', 'system', 'systematic', 'systeme', 'systemindependent', 'systems', 'sz', 'sze04', 'szegedy', 'szkola', 'são', 'sǫα', 'sα', 'sαǫ', 'sβ', 'sν', 'sσ11u1', 'sσ22u2', 'sτ', 't1', 'ta', 'table', 'tables', 'tackle', 'tackles', 'tactical', 'tai', 'tail', 'take', 'taken', 'takes', 'taking', 'talking', 'tan', 'tan2', 'tanabe', 'tang', 'tangent', 'tangmunarunkit', 'tanner', 'taormina', 'taraz', 'target', 'task', 'tcc', 'teach', 'teaching', 'tech', 'technical', 'technicality', 'technically', 'technique', 'techniques', 'technische', 'technologies', 'technology', 'teirlinck', 'tel', 'telatar', 'telecomm', 'telecommunications', 'telephone', 'teletar', 'tell', 'tells', 'temporarily', 'tended', 'tentative', 'term', 'term4', 'terminates', 'terminating', 'terminologies', 'terminology', 'terms', 'ternary', 'test', 'testclass', 'testing', 'testinhtml', 'testmethod', 'testobject', 'tests', 'tetrahedron', 'texas', 'text', 'texts', 'texture', 'tf', 'th', 'than', 'thank', 'thanks', 'thas', 'that', 'the', 'theblank', 'their', 'them', 'theme', 'themselves', 'then', 'theorem', 'theorems', 'theoretic', 'theoretical', 'theories', 'theorists', 'theory', 'there', 'thereby', 'therefore', 'therein', 'these', 'thesis', 'they', 'thick', 'thickness', 'thiel', 'thing', 'think', 'third', 'thirteenth', 'this', 'thomas', 'thompson', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'thoughts', 'three', 'threshold', 'thresholding', 'through', 'throughout', 'throw', 'thus', 'tieta', 'tight', 'tighter', 'tilborg', 'till', 'time', 'times', 'tits', 'tjoelker', 'tl', 'to', 'todd', 'together', 'togonidze', 'tolerance', 'tomasi', 'tomecat', 'tonchev', 'too', 'took', 'tool', 'tools', 'top', 'topic', 'topics', 'topological', 'topologies', 'topology', 'torus', 'total', 'touch', 'towards', 'towsley', 'tpx', 'tpy', 'tr', 'trace', 'track', 'tracked', 'trade', 'tradeoffs', 'tradition', 'traditional', 'training', 'trans', 'transactions', 'transferred', 'transformations', 'transition', 'transitions', 'transitive', 'transitively', 'transitiven', 'transitivity', 'translated', 'translates', 'translating', 'translation', 'transmission', 'transmissions', 'transmit', 'transmits', 'transmitted', 'transmitter', 'transmitting', 'transparent', 'treat', 'treated', 'treating', 'treatment', 'treats', 'tree', 'trial', 'triangle', 'trianglefree', 'triangles', 'triangulation', 'trick', 'trier', 'tries', 'triple', 'trivial', 'trivially', 'trois', 'trott', 'troubles', 'true', 'truncated', 'try', 'trying', 'tsai', 'tschel', 'tse', 'tte', 'tu', 'tucker', 'tude', 'tue', 'tul08', 'tulsi', 'tuned', 'tuple', 'tuples', 'turbo', 'turn', 'turns', 'tutor', 'tutorial', 'tutorielle', 'tutoring', 'tuy', 'tuyls', 'tv', 'tvi', 'tw', 'twelve', 'twentieth', 'twenty', 'twentyeighth', 'twice', 'twin', 'twisted', 'two', 'tx', 'type', 'types', 'typical', 'typically', 'tǫ', 'tδ', 'tσn', 'u0', 'u1', 'u12', 'u1θ', 'u2', 'u2θ', 'u3', 'ubiquitous', 'uc', 'ucla', 'uep', 'ugly', 'uh', 'uhi', 'ui', 'ui2', 'uk', 'ultimately', 'un', 'un2', 'unable', 'unacceptable', 'unaffected', 'unam', 'unambiguous', 'unavoidable', 'unbiased', 'unbounded', 'uncanceled', 'unchanged', 'unclear', 'unconditional', 'unconstrained', 'uncoordinated', 'under', 'underline', 'underlying', 'understand', 'understanding', 'understood', 'undesirable', 'undetected', 'undirected', 'undo', 'unequal', 'unexpected', 'unfortunately', 'uniform', 'uniformity', 'uniformly', 'unify', 'unimodular', 'unimportant', 'union', 'unique', 'uniquely', 'uniqueness', 'unit', 'unitaries', 'unitary', 'units', 'univ', 'universal', 'universally', 'universita', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unlimited', 'unnecessary', 'unnormalized', 'unnoticed', 'unpublished', 'unrealistic', 'unreliable', 'unresolved', 'unrestricted', 'unsatisfactory', 'unsolved', 'unstable', 'unsymmetric', 'until', 'unused', 'unusually', 'up', 'update', 'updated', 'updating', 'upon', 'upper', 'urbanke', 'url', 'us', 'usa', 'use', 'used', 'useful', 'usefulness', 'user', 'uses', 'using', 'ussr', 'usual', 'usually', 'ut', 'utility', 'uture', 'uwaterloo', 'uy', 'v0', 'v1', 'v2', 'va', 'valente', 'valid', 'validate', 'vallentin', 'valuable', 'value', 'valued', 'values', 'van', 'vanish', 'vanishes', 'vanishing', 'vanishingly', 'vanlehn', 'vanstone', 'var', 'variable', 'variables', 'variance', 'variant', 'variation', 'variational', 'variations', 'varies', 'varieties', 'variety', 'various', 'vary', 'varying', 'vasic', 'vazirani', 'vector', 'vectorial', 'vectors', 'vegas', 'verbindung', 'verbitskiy', 'verification', 'verified', 'verify', 'verlag', 'version', 'versions', 'versus', 'vertex', 'vertices', 'very', 'vese', 'vh', 'vi', 'via', 'viable', 'view', 'viewed', 'vii', 'viii', 'villanueva', 'violate', 'virtual', 'vishwanath', 'visible', 'vision', 'visiting', 'visual', 'visualization', 'visualize', 'visually', 'vivid', 'vj', 'vn', 'vol', 'volume', 'von', 'vontobel', 'vorkommen', 'voyager', 'vr', 'vs', 'vt', 'vu', 'vub', 'vy', 'vτ', 'w0', 'w1', 'w3c', 'waerden', 'walk', 'walker', 'walks', 'want', 'wanted', 'ward', 'warnke', 'was', 'wassermann', 'wasteful', 'waterloo', 'watrous', 'watson', 'watts', 'wavelenghts', 'wavelength', 'wavelengths', 'way', 'ways', 'we', 'weak', 'weakening', 'weaker', 'weakly', 'web', 'webapplication', 'week', 'weeks', 'weight', 'weighted', 'weights', 'weissman', 'welche', 'weldon', 'well', 'were', 'weyl', 'whaley', 'what', 'whatever', 'when', 'whenever', 'where', 'whereas', 'whereby', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whose', 'why', 'wi', 'wide', 'widely', 'widen', 'wider', 'widlund', 'width', 'wielandt', 'wigger', 'wiley', 'will', 'willinger', 'wilson', 'win', 'winkler', 'wirel', 'wireless', 'wise', 'wish', 'with', 'within', 'without', 'witt', 'wj', 'wk', 'wn', 'wolf', 'wolfovitz', 'wolfson', 'wonder', 'woolhouse', 'word', 'words', 'work', 'working', 'works', 'workshop', 'world', 'wormald', 'worst', 'worth', 'worthwhile', 'would', 'wp', 'write', 'writing', 'written', 'wrong', 'www', 'wy', 'wyy', 'wα', 'wαz', 'x0', 'x1', 'x12', 'x2', 'x22', 'x6', 'xa', 'xd', 'xfl', 'xfu', 'xh', 'xi', 'xi2', 'xihx', 'xj', 'xk', 'xml', 'xmlbasierte', 'xn', 'xn1', 'xn2', 'xp', 'xpy', 'xr', 'xt', 'xu', 'xv', 'xx', 'xxx', 'xyi', 'y0', 'y0n', 'y1', 'y1i', 'y1n', 'y1n1', 'y2', 'y6', 'yamamoto', 'yamamotoitoh', 'yan', 'yann', 'years', 'yes', 'yet', 'yi', 'yield', 'yielding', 'yields', 'yihy', 'yis', 'yj', 'yk', 'yn', 'york', 'ypr', 'ypy', 'yt', 'yu', 'yx', 'z0', 'z0n', 'z1', 'z4', 'zahl', 'zaitsev', 'zaragoza', 'zenios', 'zero', 'zhao', 'zheng', 'zi', 'ziegler', 'zigangirov', 'zihz', 'zinov', 'zinoviev', 'zn', 'zn1', 'zu', 'zwei', 'zyablov', 'µ1', 'µ2', 'µi', 'µihµ', 'µm', 'µz', 'ǫa', 'ǫauk', 'ǫcloseness', 'ǫh', 'ǫn', 'ǫsmooth', 'ǫtypical', 'ǫua', 'ǫδta', 'α1', 'α2', 'α21', 'αd', 'αi', 'αil', 'αj', 'αk', 'αtv', 'αv', 'αx', 'αz', 'αβxx', 'αη', 'αηt', 'αθ', 'αθ1', 'βx', 'γdmax', 'γk', 'γn', 'γn1', 'γp', 'γq', 'γr', 'γℓn', 'δ0', 'δ02', 'δ2', 'δj', 'δj2', 'δk', 'δkk', 'δn', 'δt', 'δtt', 'δλ', 'δℓ', 'ζj', 'ζn', 'ηi', 'ηil', 'ηn', 'ηt', 'θ0', 'θ1', 'θ12', 'θ2', 'θ2k2', 'θa', 'θi', 'θihθ', 'θj', 'θj2', 'θjj', 'θm', 'θn', 'θp', 'θr', 'θs', 'κ1', 'κk', 'κn', 'λ1', 'λ2', 'λf', 'λi', 'λj', 'λm', 'λmax', 'λn', 'λs', 'λu', 'λw', 'λµ', 'λǫ', 'λγk', 'λδ', 'λδe', 'ν1', 'ν2', 'νez', 'νj', 'νj2', 'νn', 'νδk', 'ξj', 'π6', 'πi', 'πk', 'πp', 'πt', 'πx', 'πy', 'πψ', 'πψi', 'ρ0', 'ρα', 'σ0', 'σ02', 'σ1', 'σ12', 'σ2', 'σ22', 'σi', 'σi2', 'σn', 'σn1', 'σn2', 'σz2', 'τ1', 'τ2', 'τ24', 'τ3', 'τ4', 'τ8', 'τi', 'τj', 'τn', 'τδ', 'φ0', 'φ1', 'φ2', 'φe0', 'φi', 'φj', 'φm', 'φmax', 'φq', 'φs', 'φx', 'φy', 'φz', 'ψi', 'ψihψ', 'ψik2', 'ω2', 'ωi', 'ωm2', 'ℓ2', 'ℓn']\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.00536387 0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.0013934  0.00161475 0.         ... 0.00192673 0.         0.        ]\n",
            " [0.00547562 0.0047591  0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.00240244 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5E1llEfa0XK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}